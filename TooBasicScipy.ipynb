{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGyW1F5SZHAWDwrxesItTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddh-webd-ml/Machine-Learning/blob/main/TooBasicScipy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slpxqhJSnP-s"
      },
      "outputs": [],
      "source": [
        "# Scipy ---> Scientific Python."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helps us to provide more utility functions for Optimization of stats and signal processing\n",
        "\n",
        "import scipy"
      ],
      "metadata": {
        "id": "jiAYPa_SqQoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---->constants\n",
        "# ----> stats"
      ],
      "metadata": {
        "id": "pdtIa9ZGspIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "IeVUA59EtK4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "help(stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x9LIIOAwtLXQ",
        "outputId": "348be444-6dc9-43c6-992f-334877fdbb81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package scipy.stats in scipy:\n",
            "\n",
            "NAME\n",
            "    scipy.stats - .. _statsrefmanual:\n",
            "\n",
            "DESCRIPTION\n",
            "    ==========================================\n",
            "    Statistical functions (:mod:`scipy.stats`)\n",
            "    ==========================================\n",
            "    \n",
            "    .. currentmodule:: scipy.stats\n",
            "    \n",
            "    This module contains a large number of probability distributions,\n",
            "    summary and frequency statistics, correlation functions and statistical\n",
            "    tests, masked statistics, kernel density estimation, quasi-Monte Carlo\n",
            "    functionality, and more.\n",
            "    \n",
            "    Statistics is a very large area, and there are topics that are out of scope\n",
            "    for SciPy and are covered by other packages. Some of the most important ones\n",
            "    are:\n",
            "    \n",
            "    - `statsmodels <https://www.statsmodels.org/stable/index.html>`__:\n",
            "      regression, linear models, time series analysis, extensions to topics\n",
            "      also covered by ``scipy.stats``.\n",
            "    - `Pandas <https://pandas.pydata.org/>`__: tabular data, time series\n",
            "      functionality, interfaces to other statistical languages.\n",
            "    - `PyMC <https://docs.pymc.io/>`__: Bayesian statistical\n",
            "      modeling, probabilistic machine learning.\n",
            "    - `scikit-learn <https://scikit-learn.org/>`__: classification, regression,\n",
            "      model selection.\n",
            "    - `Seaborn <https://seaborn.pydata.org/>`__: statistical data visualization.\n",
            "    - `rpy2 <https://rpy2.github.io/>`__: Python to R bridge.\n",
            "    \n",
            "    \n",
            "    Probability distributions\n",
            "    =========================\n",
            "    \n",
            "    Each univariate distribution is an instance of a subclass of `rv_continuous`\n",
            "    (`rv_discrete` for discrete distributions):\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       rv_continuous\n",
            "       rv_discrete\n",
            "       rv_histogram\n",
            "    \n",
            "    Continuous distributions\n",
            "    ------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       alpha             -- Alpha\n",
            "       anglit            -- Anglit\n",
            "       arcsine           -- Arcsine\n",
            "       argus             -- Argus\n",
            "       beta              -- Beta\n",
            "       betaprime         -- Beta Prime\n",
            "       bradford          -- Bradford\n",
            "       burr              -- Burr (Type III)\n",
            "       burr12            -- Burr (Type XII)\n",
            "       cauchy            -- Cauchy\n",
            "       chi               -- Chi\n",
            "       chi2              -- Chi-squared\n",
            "       cosine            -- Cosine\n",
            "       crystalball       -- Crystalball\n",
            "       dgamma            -- Double Gamma\n",
            "       dweibull          -- Double Weibull\n",
            "       erlang            -- Erlang\n",
            "       expon             -- Exponential\n",
            "       exponnorm         -- Exponentially Modified Normal\n",
            "       exponweib         -- Exponentiated Weibull\n",
            "       exponpow          -- Exponential Power\n",
            "       f                 -- F (Snecdor F)\n",
            "       fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n",
            "       fisk              -- Fisk\n",
            "       foldcauchy        -- Folded Cauchy\n",
            "       foldnorm          -- Folded Normal\n",
            "       genlogistic       -- Generalized Logistic\n",
            "       gennorm           -- Generalized normal\n",
            "       genpareto         -- Generalized Pareto\n",
            "       genexpon          -- Generalized Exponential\n",
            "       genextreme        -- Generalized Extreme Value\n",
            "       gausshyper        -- Gauss Hypergeometric\n",
            "       gamma             -- Gamma\n",
            "       gengamma          -- Generalized gamma\n",
            "       genhalflogistic   -- Generalized Half Logistic\n",
            "       genhyperbolic     -- Generalized Hyperbolic\n",
            "       geninvgauss       -- Generalized Inverse Gaussian\n",
            "       gibrat            -- Gibrat\n",
            "       gompertz          -- Gompertz (Truncated Gumbel)\n",
            "       gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n",
            "       gumbel_l          -- Left Sided Gumbel, etc.\n",
            "       halfcauchy        -- Half Cauchy\n",
            "       halflogistic      -- Half Logistic\n",
            "       halfnorm          -- Half Normal\n",
            "       halfgennorm       -- Generalized Half Normal\n",
            "       hypsecant         -- Hyperbolic Secant\n",
            "       invgamma          -- Inverse Gamma\n",
            "       invgauss          -- Inverse Gaussian\n",
            "       invweibull        -- Inverse Weibull\n",
            "       jf_skew_t         -- Jones and Faddy Skew-T\n",
            "       johnsonsb         -- Johnson SB\n",
            "       johnsonsu         -- Johnson SU\n",
            "       kappa4            -- Kappa 4 parameter\n",
            "       kappa3            -- Kappa 3 parameter\n",
            "       ksone             -- Distribution of Kolmogorov-Smirnov one-sided test statistic\n",
            "       kstwo             -- Distribution of Kolmogorov-Smirnov two-sided test statistic\n",
            "       kstwobign         -- Limiting Distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n",
            "       laplace           -- Laplace\n",
            "       laplace_asymmetric    -- Asymmetric Laplace\n",
            "       levy              -- Levy\n",
            "       levy_l\n",
            "       levy_stable\n",
            "       logistic          -- Logistic\n",
            "       loggamma          -- Log-Gamma\n",
            "       loglaplace        -- Log-Laplace (Log Double Exponential)\n",
            "       lognorm           -- Log-Normal\n",
            "       loguniform        -- Log-Uniform\n",
            "       lomax             -- Lomax (Pareto of the second kind)\n",
            "       maxwell           -- Maxwell\n",
            "       mielke            -- Mielke's Beta-Kappa\n",
            "       moyal             -- Moyal\n",
            "       nakagami          -- Nakagami\n",
            "       ncx2              -- Non-central chi-squared\n",
            "       ncf               -- Non-central F\n",
            "       nct               -- Non-central Student's T\n",
            "       norm              -- Normal (Gaussian)\n",
            "       norminvgauss      -- Normal Inverse Gaussian\n",
            "       pareto            -- Pareto\n",
            "       pearson3          -- Pearson type III\n",
            "       powerlaw          -- Power-function\n",
            "       powerlognorm      -- Power log normal\n",
            "       powernorm         -- Power normal\n",
            "       rdist             -- R-distribution\n",
            "       rayleigh          -- Rayleigh\n",
            "       rel_breitwigner   -- Relativistic Breit-Wigner\n",
            "       rice              -- Rice\n",
            "       recipinvgauss     -- Reciprocal Inverse Gaussian\n",
            "       semicircular      -- Semicircular\n",
            "       skewcauchy        -- Skew Cauchy\n",
            "       skewnorm          -- Skew normal\n",
            "       studentized_range    -- Studentized Range\n",
            "       t                 -- Student's T\n",
            "       trapezoid         -- Trapezoidal\n",
            "       triang            -- Triangular\n",
            "       truncexpon        -- Truncated Exponential\n",
            "       truncnorm         -- Truncated Normal\n",
            "       truncpareto       -- Truncated Pareto\n",
            "       truncweibull_min  -- Truncated minimum Weibull distribution\n",
            "       tukeylambda       -- Tukey-Lambda\n",
            "       uniform           -- Uniform\n",
            "       vonmises          -- Von-Mises (Circular)\n",
            "       vonmises_line     -- Von-Mises (Line)\n",
            "       wald              -- Wald\n",
            "       weibull_min       -- Minimum Weibull (see Frechet)\n",
            "       weibull_max       -- Maximum Weibull (see Frechet)\n",
            "       wrapcauchy        -- Wrapped Cauchy\n",
            "    \n",
            "    The ``fit`` method of the univariate continuous distributions uses\n",
            "    maximum likelihood estimation to fit the distribution to a data set.\n",
            "    The ``fit`` method can accept regular data or *censored data*.\n",
            "    Censored data is represented with instances of the `CensoredData`\n",
            "    class.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       CensoredData\n",
            "    \n",
            "    \n",
            "    Multivariate distributions\n",
            "    --------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       multivariate_normal    -- Multivariate normal distribution\n",
            "       matrix_normal          -- Matrix normal distribution\n",
            "       dirichlet              -- Dirichlet\n",
            "       dirichlet_multinomial  -- Dirichlet multinomial distribution\n",
            "       wishart                -- Wishart\n",
            "       invwishart             -- Inverse Wishart\n",
            "       multinomial            -- Multinomial distribution\n",
            "       special_ortho_group    -- SO(N) group\n",
            "       ortho_group            -- O(N) group\n",
            "       unitary_group          -- U(N) group\n",
            "       random_correlation     -- random correlation matrices\n",
            "       multivariate_t         -- Multivariate t-distribution\n",
            "       multivariate_hypergeom -- Multivariate hypergeometric distribution\n",
            "       random_table           -- Distribution of random tables with given marginals\n",
            "       uniform_direction      -- Uniform distribution on S(N-1)\n",
            "       vonmises_fisher        -- Von Mises-Fisher distribution\n",
            "    \n",
            "    `scipy.stats.multivariate_normal` methods accept instances\n",
            "    of the following class to represent the covariance.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       Covariance             -- Representation of a covariance matrix\n",
            "    \n",
            "    \n",
            "    Discrete distributions\n",
            "    ----------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       bernoulli                -- Bernoulli\n",
            "       betabinom                -- Beta-Binomial\n",
            "       betanbinom               -- Beta-Negative Binomial\n",
            "       binom                    -- Binomial\n",
            "       boltzmann                -- Boltzmann (Truncated Discrete Exponential)\n",
            "       dlaplace                 -- Discrete Laplacian\n",
            "       geom                     -- Geometric\n",
            "       hypergeom                -- Hypergeometric\n",
            "       logser                   -- Logarithmic (Log-Series, Series)\n",
            "       nbinom                   -- Negative Binomial\n",
            "       nchypergeom_fisher       -- Fisher's Noncentral Hypergeometric\n",
            "       nchypergeom_wallenius    -- Wallenius's Noncentral Hypergeometric\n",
            "       nhypergeom               -- Negative Hypergeometric\n",
            "       planck                   -- Planck (Discrete Exponential)\n",
            "       poisson                  -- Poisson\n",
            "       randint                  -- Discrete Uniform\n",
            "       skellam                  -- Skellam\n",
            "       yulesimon                -- Yule-Simon\n",
            "       zipf                     -- Zipf (Zeta)\n",
            "       zipfian                  -- Zipfian\n",
            "    \n",
            "    \n",
            "    An overview of statistical functions is given below.  Many of these functions\n",
            "    have a similar version in `scipy.stats.mstats` which work for masked arrays.\n",
            "    \n",
            "    Summary statistics\n",
            "    ==================\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       describe          -- Descriptive statistics\n",
            "       gmean             -- Geometric mean\n",
            "       hmean             -- Harmonic mean\n",
            "       pmean             -- Power mean\n",
            "       kurtosis          -- Fisher or Pearson kurtosis\n",
            "       mode              -- Modal value\n",
            "       moment            -- Central moment\n",
            "       expectile         -- Expectile\n",
            "       skew              -- Skewness\n",
            "       kstat             --\n",
            "       kstatvar          --\n",
            "       tmean             -- Truncated arithmetic mean\n",
            "       tvar              -- Truncated variance\n",
            "       tmin              --\n",
            "       tmax              --\n",
            "       tstd              --\n",
            "       tsem              --\n",
            "       variation         -- Coefficient of variation\n",
            "       find_repeats\n",
            "       rankdata\n",
            "       tiecorrect\n",
            "       trim_mean\n",
            "       gstd              -- Geometric Standard Deviation\n",
            "       iqr\n",
            "       sem\n",
            "       bayes_mvs\n",
            "       mvsdist\n",
            "       entropy\n",
            "       differential_entropy\n",
            "       median_abs_deviation\n",
            "    \n",
            "    Frequency statistics\n",
            "    ====================\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       cumfreq\n",
            "       percentileofscore\n",
            "       scoreatpercentile\n",
            "       relfreq\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       binned_statistic     -- Compute a binned statistic for a set of data.\n",
            "       binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n",
            "       binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n",
            "    \n",
            "    Hypothesis Tests and related functions\n",
            "    ======================================\n",
            "    SciPy has many functions for performing hypothesis tests that return a\n",
            "    test statistic and a p-value, and several of them return confidence intervals\n",
            "    and/or other related information.\n",
            "    \n",
            "    The headings below are based on common uses of the functions within, but due to\n",
            "    the wide variety of statistical procedures, any attempt at coarse-grained\n",
            "    categorization will be imperfect. Also, note that tests within the same heading\n",
            "    are not interchangeable in general (e.g. many have different distributional\n",
            "    assumptions).\n",
            "    \n",
            "    One Sample Tests / Paired Sample Tests\n",
            "    --------------------------------------\n",
            "    One sample tests are typically used to assess whether a single sample was\n",
            "    drawn from a specified distribution or a distribution with specified properties\n",
            "    (e.g. zero mean).\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       ttest_1samp\n",
            "       binomtest\n",
            "       quantile_test\n",
            "       skewtest\n",
            "       kurtosistest\n",
            "       normaltest\n",
            "       jarque_bera\n",
            "       shapiro\n",
            "       anderson\n",
            "       cramervonmises\n",
            "       ks_1samp\n",
            "       goodness_of_fit\n",
            "       chisquare\n",
            "       power_divergence\n",
            "    \n",
            "    Paired sample tests are often used to assess whether two samples were drawn\n",
            "    from the same distribution; they differ from the independent sample tests below\n",
            "    in that each observation in one sample is treated as paired with a\n",
            "    closely-related observation in the other sample (e.g. when environmental\n",
            "    factors are controlled between observations within a pair but not among pairs).\n",
            "    They can also be interpreted or used as one-sample tests (e.g. tests on the\n",
            "    mean or median of *differences* between paired observations).\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       ttest_rel\n",
            "       wilcoxon\n",
            "    \n",
            "    Association/Correlation Tests\n",
            "    -----------------------------\n",
            "    \n",
            "    These tests are often used to assess whether there is a relationship (e.g.\n",
            "    linear) between paired observations in multiple samples or among the\n",
            "    coordinates of multivariate observations.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       linregress\n",
            "       pearsonr\n",
            "       spearmanr\n",
            "       pointbiserialr\n",
            "       kendalltau\n",
            "       weightedtau\n",
            "       somersd\n",
            "       siegelslopes\n",
            "       theilslopes\n",
            "       page_trend_test\n",
            "       multiscale_graphcorr\n",
            "    \n",
            "    These association tests and are to work with samples in the form of contingency\n",
            "    tables. Supporting functions are available in `scipy.stats.contingency`.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       chi2_contingency\n",
            "       fisher_exact\n",
            "       barnard_exact\n",
            "       boschloo_exact\n",
            "    \n",
            "    Independent Sample Tests\n",
            "    ------------------------\n",
            "    Independent sample tests are typically used to assess whether multiple samples\n",
            "    were independently drawn from the same distribution or different distributions\n",
            "    with a shared property (e.g. equal means).\n",
            "    \n",
            "    Some tests are specifically for comparing two samples.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       ttest_ind_from_stats\n",
            "       poisson_means_test\n",
            "       ttest_ind\n",
            "       mannwhitneyu\n",
            "       bws_test\n",
            "       ranksums\n",
            "       brunnermunzel\n",
            "       mood\n",
            "       ansari\n",
            "       cramervonmises_2samp\n",
            "       epps_singleton_2samp\n",
            "       ks_2samp\n",
            "       kstest\n",
            "    \n",
            "    Others are generalized to multiple samples.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       f_oneway\n",
            "       tukey_hsd\n",
            "       dunnett\n",
            "       kruskal\n",
            "       alexandergovern\n",
            "       fligner\n",
            "       levene\n",
            "       bartlett\n",
            "       median_test\n",
            "       friedmanchisquare\n",
            "       anderson_ksamp\n",
            "    \n",
            "    Resampling and Monte Carlo Methods\n",
            "    ----------------------------------\n",
            "    The following functions can reproduce the p-value and confidence interval\n",
            "    results of most of the functions above, and often produce accurate results in a\n",
            "    wider variety of conditions. They can also be used to perform hypothesis tests\n",
            "    and generate confidence intervals for custom statistics. This flexibility comes\n",
            "    at the cost of greater computational requirements and stochastic results.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       monte_carlo_test\n",
            "       permutation_test\n",
            "       bootstrap\n",
            "    \n",
            "    Instances of the following object can be passed into some hypothesis test\n",
            "    functions to perform a resampling or Monte Carlo version of the hypothesis\n",
            "    test.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       MonteCarloMethod\n",
            "       PermutationMethod\n",
            "       BootstrapMethod\n",
            "    \n",
            "    Multiple Hypothesis Testing and Meta-Analysis\n",
            "    ---------------------------------------------\n",
            "    These functions are for assessing the results of individual tests as a whole.\n",
            "    Functions for performing specific multiple hypothesis tests (e.g. post hoc\n",
            "    tests) are listed above.\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       combine_pvalues\n",
            "       false_discovery_control\n",
            "    \n",
            "    \n",
            "    The following functions are related to the tests above but do not belong in the\n",
            "    above categories.\n",
            "    \n",
            "    Quasi-Monte Carlo\n",
            "    =================\n",
            "    \n",
            "    .. toctree::\n",
            "       :maxdepth: 4\n",
            "    \n",
            "       stats.qmc\n",
            "    \n",
            "    Contingency Tables\n",
            "    ==================\n",
            "    \n",
            "    .. toctree::\n",
            "       :maxdepth: 4\n",
            "    \n",
            "       stats.contingency\n",
            "    \n",
            "    Masked statistics functions\n",
            "    ===========================\n",
            "    \n",
            "    .. toctree::\n",
            "    \n",
            "       stats.mstats\n",
            "    \n",
            "    \n",
            "    Other statistical functionality\n",
            "    ===============================\n",
            "    \n",
            "    Transformations\n",
            "    ---------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       boxcox\n",
            "       boxcox_normmax\n",
            "       boxcox_llf\n",
            "       yeojohnson\n",
            "       yeojohnson_normmax\n",
            "       yeojohnson_llf\n",
            "       obrientransform\n",
            "       sigmaclip\n",
            "       trimboth\n",
            "       trim1\n",
            "       zmap\n",
            "       zscore\n",
            "       gzscore\n",
            "    \n",
            "    Statistical distances\n",
            "    ---------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       wasserstein_distance\n",
            "       wasserstein_distance_nd\n",
            "       energy_distance\n",
            "    \n",
            "    Sampling\n",
            "    --------\n",
            "    \n",
            "    .. toctree::\n",
            "       :maxdepth: 4\n",
            "    \n",
            "       stats.sampling\n",
            "    \n",
            "    Random variate generation / CDF Inversion\n",
            "    -----------------------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       rvs_ratio_uniforms\n",
            "    \n",
            "    Fitting / Survival Analysis\n",
            "    ---------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       fit\n",
            "       ecdf\n",
            "       logrank\n",
            "    \n",
            "    Directional statistical functions\n",
            "    ---------------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       directional_stats\n",
            "       circmean\n",
            "       circvar\n",
            "       circstd\n",
            "    \n",
            "    Sensitivity Analysis\n",
            "    --------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       sobol_indices\n",
            "    \n",
            "    Plot-tests\n",
            "    ----------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       ppcc_max\n",
            "       ppcc_plot\n",
            "       probplot\n",
            "       boxcox_normplot\n",
            "       yeojohnson_normplot\n",
            "    \n",
            "    Univariate and multivariate kernel density estimation\n",
            "    -----------------------------------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       gaussian_kde\n",
            "    \n",
            "    Warnings / Errors used in :mod:`scipy.stats`\n",
            "    --------------------------------------------\n",
            "    \n",
            "    .. autosummary::\n",
            "       :toctree: generated/\n",
            "    \n",
            "       DegenerateDataWarning\n",
            "       ConstantInputWarning\n",
            "       NearConstantInputWarning\n",
            "       FitError\n",
            "    \n",
            "    Result classes used in :mod:`scipy.stats`\n",
            "    -----------------------------------------\n",
            "    \n",
            "    .. warning::\n",
            "    \n",
            "        These classes are private, but they are included here because instances\n",
            "        of them are returned by other statistical functions. User import and\n",
            "        instantiation is not supported.\n",
            "    \n",
            "    .. toctree::\n",
            "       :maxdepth: 2\n",
            "    \n",
            "       stats._result_classes\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    _ansari_swilk_statistics\n",
            "    _axis_nan_policy\n",
            "    _biasedurn\n",
            "    _binned_statistic\n",
            "    _binomtest\n",
            "    _boost (package)\n",
            "    _bws_test\n",
            "    _censored_data\n",
            "    _common\n",
            "    _constants\n",
            "    _continuous_distns\n",
            "    _covariance\n",
            "    _crosstab\n",
            "    _discrete_distns\n",
            "    _distn_infrastructure\n",
            "    _distr_params\n",
            "    _entropy\n",
            "    _fit\n",
            "    _generate_pyx\n",
            "    _hypotests\n",
            "    _kde\n",
            "    _ksstats\n",
            "    _levy_stable (package)\n",
            "    _mannwhitneyu\n",
            "    _morestats\n",
            "    _mstats_basic\n",
            "    _mstats_extras\n",
            "    _multicomp\n",
            "    _multivariate\n",
            "    _mvn\n",
            "    _odds_ratio\n",
            "    _page_trend_test\n",
            "    _qmc\n",
            "    _qmc_cy\n",
            "    _qmvnt\n",
            "    _rcont (package)\n",
            "    _relative_risk\n",
            "    _resampling\n",
            "    _result_classes\n",
            "    _rvs_sampling\n",
            "    _sampling\n",
            "    _sensitivity_analysis\n",
            "    _sobol\n",
            "    _stats\n",
            "    _stats_mstats_common\n",
            "    _stats_py\n",
            "    _stats_pythran\n",
            "    _survival\n",
            "    _tukeylambda_stats\n",
            "    _unuran (package)\n",
            "    _variation\n",
            "    _warnings_errors\n",
            "    _wilcoxon\n",
            "    biasedurn\n",
            "    contingency\n",
            "    distributions\n",
            "    kde\n",
            "    morestats\n",
            "    mstats\n",
            "    mstats_basic\n",
            "    mstats_extras\n",
            "    mvn\n",
            "    qmc\n",
            "    sampling\n",
            "    stats\n",
            "    tests (package)\n",
            "\n",
            "CLASSES\n",
            "    builtins.RuntimeError(builtins.Exception)\n",
            "        scipy.stats._warnings_errors.FitError\n",
            "    builtins.RuntimeWarning(builtins.Warning)\n",
            "        scipy.stats._warnings_errors.DegenerateDataWarning\n",
            "            scipy.stats._warnings_errors.ConstantInputWarning\n",
            "            scipy.stats._warnings_errors.NearConstantInputWarning\n",
            "    builtins.object\n",
            "        scipy.stats._censored_data.CensoredData\n",
            "        scipy.stats._covariance.Covariance\n",
            "        scipy.stats._kde.gaussian_kde\n",
            "    scipy.stats._distn_infrastructure.rv_generic(builtins.object)\n",
            "        scipy.stats._distn_infrastructure.rv_continuous\n",
            "            scipy.stats._continuous_distns.rv_histogram\n",
            "        scipy.stats._distn_infrastructure.rv_discrete\n",
            "    scipy.stats._resampling.ResamplingMethod(builtins.object)\n",
            "        scipy.stats._resampling.BootstrapMethod\n",
            "        scipy.stats._resampling.MonteCarloMethod\n",
            "        scipy.stats._resampling.PermutationMethod\n",
            "    \n",
            "    class BootstrapMethod(ResamplingMethod)\n",
            "     |  BootstrapMethod(n_resamples: 'int' = 9999, batch: 'int' = None, random_state: 'object' = None, method: 'str' = 'BCa') -> None\n",
            "     |  \n",
            "     |  Configuration information for a bootstrap confidence interval.\n",
            "     |  \n",
            "     |  Instances of this class can be passed into the `method` parameter of some\n",
            "     |  confidence interval methods to generate a bootstrap confidence interval.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  n_resamples : int, optional\n",
            "     |      The number of resamples to perform. Default is 9999.\n",
            "     |  batch : int, optional\n",
            "     |      The number of resamples to process in each vectorized call to\n",
            "     |      the statistic. Batch sizes >>1 tend to be faster when the statistic\n",
            "     |      is vectorized, but memory usage scales linearly with the batch size.\n",
            "     |      Default is ``None``, which processes all resamples in a single batch.\n",
            "     |  random_state : {None, int, `numpy.random.Generator`,\n",
            "     |                  `numpy.random.RandomState`}, optional\n",
            "     |  \n",
            "     |      Pseudorandom number generator state used to generate resamples.\n",
            "     |  \n",
            "     |      If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |      instance, then that instance is used.\n",
            "     |      If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `random_state`.\n",
            "     |      If `random_state` is ``None`` (default), the\n",
            "     |      `numpy.random.RandomState` singleton is used.\n",
            "     |  \n",
            "     |  method : {'bca', 'percentile', 'basic'}\n",
            "     |      Whether to use the 'percentile' bootstrap ('percentile'), the 'basic'\n",
            "     |      (AKA 'reverse') bootstrap ('basic'), or the bias-corrected and\n",
            "     |      accelerated bootstrap ('BCa', default).\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      BootstrapMethod\n",
            "     |      ResamplingMethod\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __init__(self, n_resamples: 'int' = 9999, batch: 'int' = None, random_state: 'object' = None, method: 'str' = 'BCa') -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __annotations__ = {'method': 'str', 'random_state': 'object'}\n",
            "     |  \n",
            "     |  __dataclass_fields__ = {'batch': Field(name='batch',type='int',default...\n",
            "     |  \n",
            "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            "     |  \n",
            "     |  __hash__ = None\n",
            "     |  \n",
            "     |  __match_args__ = ('n_resamples', 'batch', 'random_state', 'method')\n",
            "     |  \n",
            "     |  method = 'BCa'\n",
            "     |  \n",
            "     |  random_state = None\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  batch = None\n",
            "     |  \n",
            "     |  n_resamples = 9999\n",
            "    \n",
            "    class CensoredData(builtins.object)\n",
            "     |  CensoredData(uncensored=None, *, left=None, right=None, interval=None)\n",
            "     |  \n",
            "     |  Instances of this class represent censored data.\n",
            "     |  \n",
            "     |  Instances may be passed to the ``fit`` method of continuous\n",
            "     |  univariate SciPy distributions for maximum likelihood estimation.\n",
            "     |  The *only* method of the univariate continuous distributions that\n",
            "     |  understands `CensoredData` is the ``fit`` method.  An instance of\n",
            "     |  `CensoredData` can not be passed to methods such as ``pdf`` and\n",
            "     |  ``cdf``.\n",
            "     |  \n",
            "     |  An observation is said to be *censored* when the precise value is unknown,\n",
            "     |  but it has a known upper and/or lower bound.  The conventional terminology\n",
            "     |  is:\n",
            "     |  \n",
            "     |  * left-censored: an observation is below a certain value but it is\n",
            "     |    unknown by how much.\n",
            "     |  * right-censored: an observation is above a certain value but it is\n",
            "     |    unknown by how much.\n",
            "     |  * interval-censored: an observation lies somewhere on an interval between\n",
            "     |    two values.\n",
            "     |  \n",
            "     |  Left-, right-, and interval-censored data can be represented by\n",
            "     |  `CensoredData`.\n",
            "     |  \n",
            "     |  For convenience, the class methods ``left_censored`` and\n",
            "     |  ``right_censored`` are provided to create a `CensoredData`\n",
            "     |  instance from a single one-dimensional array of measurements\n",
            "     |  and a corresponding boolean array to indicate which measurements\n",
            "     |  are censored.  The class method ``interval_censored`` accepts two\n",
            "     |  one-dimensional arrays that hold the lower and upper bounds of the\n",
            "     |  intervals.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  uncensored : array_like, 1D\n",
            "     |      Uncensored observations.\n",
            "     |  left : array_like, 1D\n",
            "     |      Left-censored observations.\n",
            "     |  right : array_like, 1D\n",
            "     |      Right-censored observations.\n",
            "     |  interval : array_like, 2D, with shape (m, 2)\n",
            "     |      Interval-censored observations.  Each row ``interval[k, :]``\n",
            "     |      represents the interval for the kth interval-censored observation.\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  In the input array `interval`, the lower bound of the interval may\n",
            "     |  be ``-inf``, and the upper bound may be ``inf``, but at least one must be\n",
            "     |  finite. When the lower bound is ``-inf``, the row represents a left-\n",
            "     |  censored observation, and when the upper bound is ``inf``, the row\n",
            "     |  represents a right-censored observation.  If the length of an interval\n",
            "     |  is 0 (i.e. ``interval[k, 0] == interval[k, 1]``, the observation is\n",
            "     |  treated as uncensored.  So one can represent all the types of censored\n",
            "     |  and uncensored data in ``interval``, but it is generally more convenient\n",
            "     |  to use `uncensored`, `left` and `right` for uncensored, left-censored and\n",
            "     |  right-censored observations, respectively.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  In the most general case, a censored data set may contain values that\n",
            "     |  are left-censored, right-censored, interval-censored, and uncensored.\n",
            "     |  For example, here we create a data set with five observations.  Two\n",
            "     |  are uncensored (values 1 and 1.5), one is a left-censored observation\n",
            "     |  of 0, one is a right-censored observation of 10 and one is\n",
            "     |  interval-censored in the interval [2, 3].\n",
            "     |  \n",
            "     |  >>> import numpy as np\n",
            "     |  >>> from scipy.stats import CensoredData\n",
            "     |  >>> data = CensoredData(uncensored=[1, 1.5], left=[0], right=[10],\n",
            "     |  ...                     interval=[[2, 3]])\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(5 values: 2 not censored, 1 left-censored,\n",
            "     |  1 right-censored, 1 interval-censored)\n",
            "     |  \n",
            "     |  Equivalently,\n",
            "     |  \n",
            "     |  >>> data = CensoredData(interval=[[1, 1],\n",
            "     |  ...                               [1.5, 1.5],\n",
            "     |  ...                               [-np.inf, 0],\n",
            "     |  ...                               [10, np.inf],\n",
            "     |  ...                               [2, 3]])\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(5 values: 2 not censored, 1 left-censored,\n",
            "     |  1 right-censored, 1 interval-censored)\n",
            "     |  \n",
            "     |  A common case is to have a mix of uncensored observations and censored\n",
            "     |  observations that are all right-censored (or all left-censored). For\n",
            "     |  example, consider an experiment in which six devices are started at\n",
            "     |  various times and left running until they fail.  Assume that time is\n",
            "     |  measured in hours, and the experiment is stopped after 30 hours, even\n",
            "     |  if all the devices have not failed by that time.  We might end up with\n",
            "     |  data such as this::\n",
            "     |  \n",
            "     |      Device  Start-time  Fail-time  Time-to-failure\n",
            "     |         1         0         13           13\n",
            "     |         2         2         24           22\n",
            "     |         3         5         22           17\n",
            "     |         4         8         23           15\n",
            "     |         5        10        ***          >20\n",
            "     |         6        12        ***          >18\n",
            "     |  \n",
            "     |  Two of the devices had not failed when the experiment was stopped;\n",
            "     |  the observations of the time-to-failure for these two devices are\n",
            "     |  right-censored.  We can represent this data with\n",
            "     |  \n",
            "     |  >>> data = CensoredData(uncensored=[13, 22, 17, 15], right=[20, 18])\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(6 values: 4 not censored, 2 right-censored)\n",
            "     |  \n",
            "     |  Alternatively, we can use the method `CensoredData.right_censored` to\n",
            "     |  create a representation of this data.  The time-to-failure observations\n",
            "     |  are put the list ``ttf``.  The ``censored`` list indicates which values\n",
            "     |  in ``ttf`` are censored.\n",
            "     |  \n",
            "     |  >>> ttf = [13, 22, 17, 15, 20, 18]\n",
            "     |  >>> censored = [False, False, False, False, True, True]\n",
            "     |  \n",
            "     |  Pass these lists to `CensoredData.right_censored` to create an\n",
            "     |  instance of `CensoredData`.\n",
            "     |  \n",
            "     |  >>> data = CensoredData.right_censored(ttf, censored)\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(6 values: 4 not censored, 2 right-censored)\n",
            "     |  \n",
            "     |  If the input data is interval censored and already stored in two\n",
            "     |  arrays, one holding the low end of the intervals and another\n",
            "     |  holding the high ends, the class method ``interval_censored`` can\n",
            "     |  be used to create the `CensoredData` instance.\n",
            "     |  \n",
            "     |  This example creates an instance with four interval-censored values.\n",
            "     |  The intervals are [10, 11], [0.5, 1], [2, 3], and [12.5, 13.5].\n",
            "     |  \n",
            "     |  >>> a = [10, 0.5, 2, 12.5]  # Low ends of the intervals\n",
            "     |  >>> b = [11, 1.0, 3, 13.5]  # High ends of the intervals\n",
            "     |  >>> data = CensoredData.interval_censored(low=a, high=b)\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(4 values: 0 not censored, 4 interval-censored)\n",
            "     |  \n",
            "     |  Finally, we create and censor some data from the `weibull_min`\n",
            "     |  distribution, and then fit `weibull_min` to that data. We'll assume\n",
            "     |  that the location parameter is known to be 0.\n",
            "     |  \n",
            "     |  >>> from scipy.stats import weibull_min\n",
            "     |  >>> rng = np.random.default_rng()\n",
            "     |  \n",
            "     |  Create the random data set.\n",
            "     |  \n",
            "     |  >>> x = weibull_min.rvs(2.5, loc=0, scale=30, size=250, random_state=rng)\n",
            "     |  >>> x[x > 40] = 40  # Right-censor values greater or equal to 40.\n",
            "     |  \n",
            "     |  Create the `CensoredData` instance with the `right_censored` method.\n",
            "     |  The censored values are those where the value is 40.\n",
            "     |  \n",
            "     |  >>> data = CensoredData.right_censored(x, x == 40)\n",
            "     |  >>> print(data)\n",
            "     |  CensoredData(250 values: 215 not censored, 35 right-censored)\n",
            "     |  \n",
            "     |  35 values have been right-censored.\n",
            "     |  \n",
            "     |  Fit `weibull_min` to the censored data.  We expect to shape and scale\n",
            "     |  to be approximately 2.5 and 30, respectively.\n",
            "     |  \n",
            "     |  >>> weibull_min.fit(data, floc=0)\n",
            "     |  (2.3575922823897315, 0, 30.40650074451254)\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, uncensored=None, *, left=None, right=None, interval=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __len__(self)\n",
            "     |      The number of values (censored and not censored).\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __str__(self)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  __sub__(self, other)\n",
            "     |  \n",
            "     |  __truediv__(self, other)\n",
            "     |  \n",
            "     |  num_censored(self)\n",
            "     |      Number of censored values.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  interval_censored(low, high)\n",
            "     |      Create a `CensoredData` instance of interval-censored data.\n",
            "     |      \n",
            "     |      This method is useful when all the data is interval-censored, and\n",
            "     |      the low and high ends of the intervals are already stored in\n",
            "     |      separate one-dimensional arrays.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      low : array_like\n",
            "     |          The one-dimensional array containing the low ends of the\n",
            "     |          intervals.\n",
            "     |      high : array_like\n",
            "     |          The one-dimensional array containing the high ends of the\n",
            "     |          intervals.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      data : `CensoredData`\n",
            "     |          An instance of `CensoredData` that represents the\n",
            "     |          collection of censored values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats import CensoredData\n",
            "     |      \n",
            "     |      ``a`` and ``b`` are the low and high ends of a collection of\n",
            "     |      interval-censored values.\n",
            "     |      \n",
            "     |      >>> a = [0.5, 2.0, 3.0, 5.5]\n",
            "     |      >>> b = [1.0, 2.5, 3.5, 7.0]\n",
            "     |      >>> data = CensoredData.interval_censored(low=a, high=b)\n",
            "     |      >>> print(data)\n",
            "     |      CensoredData(4 values: 0 not censored, 4 interval-censored)\n",
            "     |  \n",
            "     |  left_censored(x, censored)\n",
            "     |      Create a `CensoredData` instance of left-censored data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          `x` is the array of observed data or measurements.\n",
            "     |          `x` must be a one-dimensional sequence of finite numbers.\n",
            "     |      censored : array_like of bool\n",
            "     |          `censored` must be a one-dimensional sequence of boolean\n",
            "     |          values.  If ``censored[k]`` is True, the corresponding value\n",
            "     |          in `x` is left-censored.  That is, the value ``x[k]``\n",
            "     |          is the upper bound of the true (but unknown) value.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      data : `CensoredData`\n",
            "     |          An instance of `CensoredData` that represents the\n",
            "     |          collection of uncensored and left-censored values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from scipy.stats import CensoredData\n",
            "     |      \n",
            "     |      Two uncensored values (0.12 and 0.033) and two left-censored values\n",
            "     |      (both 1e-3).\n",
            "     |      \n",
            "     |      >>> data = CensoredData.left_censored([0.12, 0.033, 1e-3, 1e-3],\n",
            "     |      ...                                   [False, False, True, True])\n",
            "     |      >>> data\n",
            "     |      CensoredData(uncensored=array([0.12 , 0.033]),\n",
            "     |      left=array([0.001, 0.001]), right=array([], dtype=float64),\n",
            "     |      interval=array([], shape=(0, 2), dtype=float64))\n",
            "     |      >>> print(data)\n",
            "     |      CensoredData(4 values: 2 not censored, 2 left-censored)\n",
            "     |  \n",
            "     |  right_censored(x, censored)\n",
            "     |      Create a `CensoredData` instance of right-censored data.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          `x` is the array of observed data or measurements.\n",
            "     |          `x` must be a one-dimensional sequence of finite numbers.\n",
            "     |      censored : array_like of bool\n",
            "     |          `censored` must be a one-dimensional sequence of boolean\n",
            "     |          values.  If ``censored[k]`` is True, the corresponding value\n",
            "     |          in `x` is right-censored.  That is, the value ``x[k]``\n",
            "     |          is the lower bound of the true (but unknown) value.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      data : `CensoredData`\n",
            "     |          An instance of `CensoredData` that represents the\n",
            "     |          collection of uncensored and right-censored values.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> from scipy.stats import CensoredData\n",
            "     |      \n",
            "     |      Two uncensored values (4 and 10) and two right-censored values\n",
            "     |      (24 and 25).\n",
            "     |      \n",
            "     |      >>> data = CensoredData.right_censored([4, 10, 24, 25],\n",
            "     |      ...                                    [False, False, True, True])\n",
            "     |      >>> data\n",
            "     |      CensoredData(uncensored=array([ 4., 10.]),\n",
            "     |      left=array([], dtype=float64), right=array([24., 25.]),\n",
            "     |      interval=array([], shape=(0, 2), dtype=float64))\n",
            "     |      >>> print(data)\n",
            "     |      CensoredData(4 values: 2 not censored, 2 right-censored)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "    \n",
            "    class ConstantInputWarning(DegenerateDataWarning)\n",
            "     |  ConstantInputWarning(msg=None)\n",
            "     |  \n",
            "     |  Warns when all values in data are exactly equal.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      ConstantInputWarning\n",
            "     |      DegenerateDataWarning\n",
            "     |      builtins.RuntimeWarning\n",
            "     |      builtins.Warning\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, msg=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from DegenerateDataWarning:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.RuntimeWarning:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) class method of builtins.RuntimeWarning\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  add_note(...)\n",
            "     |      Exception.add_note(note) --\n",
            "     |      add a note to the exception\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "    \n",
            "    class Covariance(builtins.object)\n",
            "     |  Representation of a covariance matrix\n",
            "     |  \n",
            "     |  Calculations involving covariance matrices (e.g. data whitening,\n",
            "     |  multivariate normal function evaluation) are often performed more\n",
            "     |  efficiently using a decomposition of the covariance matrix instead of the\n",
            "     |  covariance matrix itself. This class allows the user to construct an\n",
            "     |  object representing a covariance matrix using any of several\n",
            "     |  decompositions and perform calculations using a common interface.\n",
            "     |  \n",
            "     |  .. note::\n",
            "     |  \n",
            "     |      The `Covariance` class cannot be instantiated directly. Instead, use\n",
            "     |      one of the factory methods (e.g. `Covariance.from_diagonal`).\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  The `Covariance` class is is used by calling one of its\n",
            "     |  factory methods to create a `Covariance` object, then pass that\n",
            "     |  representation of the `Covariance` matrix as a shape parameter of a\n",
            "     |  multivariate distribution.\n",
            "     |  \n",
            "     |  For instance, the multivariate normal distribution can accept an array\n",
            "     |  representing a covariance matrix:\n",
            "     |  \n",
            "     |  >>> from scipy import stats\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> d = [1, 2, 3]\n",
            "     |  >>> A = np.diag(d)  # a diagonal covariance matrix\n",
            "     |  >>> x = [4, -2, 5]  # a point of interest\n",
            "     |  >>> dist = stats.multivariate_normal(mean=[0, 0, 0], cov=A)\n",
            "     |  >>> dist.pdf(x)\n",
            "     |  4.9595685102808205e-08\n",
            "     |  \n",
            "     |  but the calculations are performed in a very generic way that does not\n",
            "     |  take advantage of any special properties of the covariance matrix. Because\n",
            "     |  our covariance matrix is diagonal, we can use ``Covariance.from_diagonal``\n",
            "     |  to create an object representing the covariance matrix, and\n",
            "     |  `multivariate_normal` can use this to compute the probability density\n",
            "     |  function more efficiently.\n",
            "     |  \n",
            "     |  >>> cov = stats.Covariance.from_diagonal(d)\n",
            "     |  >>> dist = stats.multivariate_normal(mean=[0, 0, 0], cov=cov)\n",
            "     |  >>> dist.pdf(x)\n",
            "     |  4.9595685102808205e-08\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  colorize(self, x)\n",
            "     |      Perform a colorizing transformation on data.\n",
            "     |      \n",
            "     |      \"Colorizing\" (\"color\" as in \"colored noise\", in which different\n",
            "     |      frequencies may have different magnitudes) transforms a set of\n",
            "     |      uncorrelated random variables into a new set of random variables with\n",
            "     |      the desired covariance. When a coloring transform is applied to a\n",
            "     |      sample of points distributed according to a multivariate normal\n",
            "     |      distribution with identity covariance and zero mean, the covariance of\n",
            "     |      the transformed sample is approximately the covariance matrix used\n",
            "     |      in the coloring transform.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          An array of points. The last dimension must correspond with the\n",
            "     |          dimensionality of the space, i.e., the number of columns in the\n",
            "     |          covariance matrix.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x_ : array_like\n",
            "     |          The transformed array of points.\n",
            "     |      \n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] \"Whitening Transformation\". Wikipedia.\n",
            "     |             https://en.wikipedia.org/wiki/Whitening_transformation\n",
            "     |      .. [2] Novak, Lukas, and Miroslav Vorechovsky. \"Generalization of\n",
            "     |             coloring linear transformation\". Transactions of VSB 18.2\n",
            "     |             (2018): 31-35. :doi:`10.31490/tces-2018-0013`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng(1638083107694713882823079058616272161)\n",
            "     |      >>> n = 3\n",
            "     |      >>> A = rng.random(size=(n, n))\n",
            "     |      >>> cov_array = A @ A.T  # make matrix symmetric positive definite\n",
            "     |      >>> cholesky = np.linalg.cholesky(cov_array)\n",
            "     |      >>> cov_object = stats.Covariance.from_cholesky(cholesky)\n",
            "     |      >>> x = rng.multivariate_normal(np.zeros(n), np.eye(n), size=(10000))\n",
            "     |      >>> x_ = cov_object.colorize(x)\n",
            "     |      >>> cov_data = np.cov(x_, rowvar=False)\n",
            "     |      >>> np.allclose(cov_data, cov_array, rtol=3e-2)\n",
            "     |      True\n",
            "     |  \n",
            "     |  whiten(self, x)\n",
            "     |      Perform a whitening transformation on data.\n",
            "     |      \n",
            "     |      \"Whitening\" (\"white\" as in \"white noise\", in which each frequency has\n",
            "     |      equal magnitude) transforms a set of random variables into a new set of\n",
            "     |      random variables with unit-diagonal covariance. When a whitening\n",
            "     |      transform is applied to a sample of points distributed according to\n",
            "     |      a multivariate normal distribution with zero mean, the covariance of\n",
            "     |      the transformed sample is approximately the identity matrix.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          An array of points. The last dimension must correspond with the\n",
            "     |          dimensionality of the space, i.e., the number of columns in the\n",
            "     |          covariance matrix.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x_ : array_like\n",
            "     |          The transformed array of points.\n",
            "     |      \n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] \"Whitening Transformation\". Wikipedia.\n",
            "     |             https://en.wikipedia.org/wiki/Whitening_transformation\n",
            "     |      .. [2] Novak, Lukas, and Miroslav Vorechovsky. \"Generalization of\n",
            "     |             coloring linear transformation\". Transactions of VSB 18.2\n",
            "     |             (2018): 31-35. :doi:`10.31490/tces-2018-0013`\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng()\n",
            "     |      >>> n = 3\n",
            "     |      >>> A = rng.random(size=(n, n))\n",
            "     |      >>> cov_array = A @ A.T  # make matrix symmetric positive definite\n",
            "     |      >>> precision = np.linalg.inv(cov_array)\n",
            "     |      >>> cov_object = stats.Covariance.from_precision(precision)\n",
            "     |      >>> x = rng.multivariate_normal(np.zeros(n), cov_array, size=(10000))\n",
            "     |      >>> x_ = cov_object.whiten(x)\n",
            "     |      >>> np.cov(x_, rowvar=False)  # near-identity covariance\n",
            "     |      array([[0.97862122, 0.00893147, 0.02430451],\n",
            "     |             [0.00893147, 0.96719062, 0.02201312],\n",
            "     |             [0.02430451, 0.02201312, 0.99206881]])\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods defined here:\n",
            "     |  \n",
            "     |  from_cholesky(cholesky)\n",
            "     |      Representation of a covariance provided via the (lower) Cholesky factor\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      cholesky : array_like\n",
            "     |          The lower triangular Cholesky factor of the covariance matrix.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Let the covariance matrix be :math:`A` and :math:`L` be the lower\n",
            "     |      Cholesky factor such that :math:`L L^T = A`.\n",
            "     |      Whitening of a data point :math:`x` is performed by computing\n",
            "     |      :math:`L^{-1} x`. :math:`\\log\\det{A}` is calculated as\n",
            "     |      :math:`2tr(\\log{L})`, where the :math:`\\log` operation is performed\n",
            "     |      element-wise.\n",
            "     |      \n",
            "     |      This `Covariance` class does not support singular covariance matrices\n",
            "     |      because the Cholesky decomposition does not exist for a singular\n",
            "     |      covariance matrix.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Prepare a symmetric positive definite covariance matrix ``A`` and a\n",
            "     |      data point ``x``.\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng()\n",
            "     |      >>> n = 5\n",
            "     |      >>> A = rng.random(size=(n, n))\n",
            "     |      >>> A = A @ A.T  # make the covariance symmetric positive definite\n",
            "     |      >>> x = rng.random(size=n)\n",
            "     |      \n",
            "     |      Perform the Cholesky decomposition of ``A`` and create the\n",
            "     |      `Covariance` object.\n",
            "     |      \n",
            "     |      >>> L = np.linalg.cholesky(A)\n",
            "     |      >>> cov = stats.Covariance.from_cholesky(L)\n",
            "     |      \n",
            "     |      Compare the functionality of the `Covariance` object against\n",
            "     |      reference implementation.\n",
            "     |      \n",
            "     |      >>> from scipy.linalg import solve_triangular\n",
            "     |      >>> res = cov.whiten(x)\n",
            "     |      >>> ref = solve_triangular(L, x, lower=True)\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |      >>> res = cov.log_pdet\n",
            "     |      >>> ref = np.linalg.slogdet(A)[-1]\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |  \n",
            "     |  from_diagonal(diagonal)\n",
            "     |      Return a representation of a covariance matrix from its diagonal.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      diagonal : array_like\n",
            "     |          The diagonal elements of a diagonal matrix.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Let the diagonal elements of a diagonal covariance matrix :math:`D` be\n",
            "     |      stored in the vector :math:`d`.\n",
            "     |      \n",
            "     |      When all elements of :math:`d` are strictly positive, whitening of a\n",
            "     |      data point :math:`x` is performed by computing\n",
            "     |      :math:`x \\cdot d^{-1/2}`, where the inverse square root can be taken\n",
            "     |      element-wise.\n",
            "     |      :math:`\\log\\det{D}` is calculated as :math:`-2 \\sum(\\log{d})`,\n",
            "     |      where the :math:`\\log` operation is performed element-wise.\n",
            "     |      \n",
            "     |      This `Covariance` class supports singular covariance matrices. When\n",
            "     |      computing ``_log_pdet``, non-positive elements of :math:`d` are\n",
            "     |      ignored. Whitening is not well defined when the point to be whitened\n",
            "     |      does not lie in the span of the columns of the covariance matrix. The\n",
            "     |      convention taken here is to treat the inverse square root of\n",
            "     |      non-positive elements of :math:`d` as zeros.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Prepare a symmetric positive definite covariance matrix ``A`` and a\n",
            "     |      data point ``x``.\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng()\n",
            "     |      >>> n = 5\n",
            "     |      >>> A = np.diag(rng.random(n))\n",
            "     |      >>> x = rng.random(size=n)\n",
            "     |      \n",
            "     |      Extract the diagonal from ``A`` and create the `Covariance` object.\n",
            "     |      \n",
            "     |      >>> d = np.diag(A)\n",
            "     |      >>> cov = stats.Covariance.from_diagonal(d)\n",
            "     |      \n",
            "     |      Compare the functionality of the `Covariance` object against a\n",
            "     |      reference implementations.\n",
            "     |      \n",
            "     |      >>> res = cov.whiten(x)\n",
            "     |      >>> ref = np.diag(d**-0.5) @ x\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |      >>> res = cov.log_pdet\n",
            "     |      >>> ref = np.linalg.slogdet(A)[-1]\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |  \n",
            "     |  from_eigendecomposition(eigendecomposition)\n",
            "     |      Representation of a covariance provided via eigendecomposition\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      eigendecomposition : sequence\n",
            "     |          A sequence (nominally a tuple) containing the eigenvalue and\n",
            "     |          eigenvector arrays as computed by `scipy.linalg.eigh` or\n",
            "     |          `numpy.linalg.eigh`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Let the covariance matrix be :math:`A`, let :math:`V` be matrix of\n",
            "     |      eigenvectors, and let :math:`W` be the diagonal matrix of eigenvalues\n",
            "     |      such that `V W V^T = A`.\n",
            "     |      \n",
            "     |      When all of the eigenvalues are strictly positive, whitening of a\n",
            "     |      data point :math:`x` is performed by computing\n",
            "     |      :math:`x^T (V W^{-1/2})`, where the inverse square root can be taken\n",
            "     |      element-wise.\n",
            "     |      :math:`\\log\\det{A}` is calculated as  :math:`tr(\\log{W})`,\n",
            "     |      where the :math:`\\log` operation is performed element-wise.\n",
            "     |      \n",
            "     |      This `Covariance` class supports singular covariance matrices. When\n",
            "     |      computing ``_log_pdet``, non-positive eigenvalues are ignored.\n",
            "     |      Whitening is not well defined when the point to be whitened\n",
            "     |      does not lie in the span of the columns of the covariance matrix. The\n",
            "     |      convention taken here is to treat the inverse square root of\n",
            "     |      non-positive eigenvalues as zeros.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Prepare a symmetric positive definite covariance matrix ``A`` and a\n",
            "     |      data point ``x``.\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng()\n",
            "     |      >>> n = 5\n",
            "     |      >>> A = rng.random(size=(n, n))\n",
            "     |      >>> A = A @ A.T  # make the covariance symmetric positive definite\n",
            "     |      >>> x = rng.random(size=n)\n",
            "     |      \n",
            "     |      Perform the eigendecomposition of ``A`` and create the `Covariance`\n",
            "     |      object.\n",
            "     |      \n",
            "     |      >>> w, v = np.linalg.eigh(A)\n",
            "     |      >>> cov = stats.Covariance.from_eigendecomposition((w, v))\n",
            "     |      \n",
            "     |      Compare the functionality of the `Covariance` object against\n",
            "     |      reference implementations.\n",
            "     |      \n",
            "     |      >>> res = cov.whiten(x)\n",
            "     |      >>> ref = x @ (v @ np.diag(w**-0.5))\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |      >>> res = cov.log_pdet\n",
            "     |      >>> ref = np.linalg.slogdet(A)[-1]\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |  \n",
            "     |  from_precision(precision, covariance=None)\n",
            "     |      Return a representation of a covariance from its precision matrix.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      precision : array_like\n",
            "     |          The precision matrix; that is, the inverse of a square, symmetric,\n",
            "     |          positive definite covariance matrix.\n",
            "     |      covariance : array_like, optional\n",
            "     |          The square, symmetric, positive definite covariance matrix. If not\n",
            "     |          provided, this may need to be calculated (e.g. to evaluate the\n",
            "     |          cumulative distribution function of\n",
            "     |          `scipy.stats.multivariate_normal`) by inverting `precision`.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Let the covariance matrix be :math:`A`, its precision matrix be\n",
            "     |      :math:`P = A^{-1}`, and :math:`L` be the lower Cholesky factor such\n",
            "     |      that :math:`L L^T = P`.\n",
            "     |      Whitening of a data point :math:`x` is performed by computing\n",
            "     |      :math:`x^T L`. :math:`\\log\\det{A}` is calculated as\n",
            "     |      :math:`-2tr(\\log{L})`, where the :math:`\\log` operation is performed\n",
            "     |      element-wise.\n",
            "     |      \n",
            "     |      This `Covariance` class does not support singular covariance matrices\n",
            "     |      because the precision matrix does not exist for a singular covariance\n",
            "     |      matrix.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      Prepare a symmetric positive definite precision matrix ``P`` and a\n",
            "     |      data point ``x``. (If the precision matrix is not already available,\n",
            "     |      consider the other factory methods of the `Covariance` class.)\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy import stats\n",
            "     |      >>> rng = np.random.default_rng()\n",
            "     |      >>> n = 5\n",
            "     |      >>> P = rng.random(size=(n, n))\n",
            "     |      >>> P = P @ P.T  # a precision matrix must be positive definite\n",
            "     |      >>> x = rng.random(size=n)\n",
            "     |      \n",
            "     |      Create the `Covariance` object.\n",
            "     |      \n",
            "     |      >>> cov = stats.Covariance.from_precision(P)\n",
            "     |      \n",
            "     |      Compare the functionality of the `Covariance` object against\n",
            "     |      reference implementations.\n",
            "     |      \n",
            "     |      >>> res = cov.whiten(x)\n",
            "     |      >>> ref = x @ np.linalg.cholesky(P)\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |      >>> res = cov.log_pdet\n",
            "     |      >>> ref = -np.linalg.slogdet(P)[-1]\n",
            "     |      >>> np.allclose(res, ref)\n",
            "     |      True\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |  \n",
            "     |  covariance\n",
            "     |      Explicit representation of the covariance matrix\n",
            "     |  \n",
            "     |  log_pdet\n",
            "     |      Log of the pseudo-determinant of the covariance matrix\n",
            "     |  \n",
            "     |  rank\n",
            "     |      Rank of the covariance matrix\n",
            "     |  \n",
            "     |  shape\n",
            "     |      Shape of the covariance array\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "    \n",
            "    class DegenerateDataWarning(builtins.RuntimeWarning)\n",
            "     |  DegenerateDataWarning(msg=None)\n",
            "     |  \n",
            "     |  Warns when data is degenerate and results may not be reliable.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      DegenerateDataWarning\n",
            "     |      builtins.RuntimeWarning\n",
            "     |      builtins.Warning\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, msg=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.RuntimeWarning:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) class method of builtins.RuntimeWarning\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  add_note(...)\n",
            "     |      Exception.add_note(note) --\n",
            "     |      add a note to the exception\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "    \n",
            "    class FitError(builtins.RuntimeError)\n",
            "     |  FitError(msg=None)\n",
            "     |  \n",
            "     |  Represents an error condition when fitting a distribution to data.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      FitError\n",
            "     |      builtins.RuntimeError\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, msg=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.RuntimeError:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) class method of builtins.RuntimeError\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  add_note(...)\n",
            "     |      Exception.add_note(note) --\n",
            "     |      add a note to the exception\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "    \n",
            "    class MonteCarloMethod(ResamplingMethod)\n",
            "     |  MonteCarloMethod(n_resamples: 'int' = 9999, batch: 'int' = None, rvs: 'object' = None) -> None\n",
            "     |  \n",
            "     |  Configuration information for a Monte Carlo hypothesis test.\n",
            "     |  \n",
            "     |  Instances of this class can be passed into the `method` parameter of some\n",
            "     |  hypothesis test functions to perform a Monte Carlo version of the\n",
            "     |  hypothesis tests.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  n_resamples : int, optional\n",
            "     |      The number of Monte Carlo samples to draw. Default is 9999.\n",
            "     |  batch : int, optional\n",
            "     |      The number of Monte Carlo samples to process in each vectorized call to\n",
            "     |      the statistic. Batch sizes >>1 tend to be faster when the statistic\n",
            "     |      is vectorized, but memory usage scales linearly with the batch size.\n",
            "     |      Default is ``None``, which processes all samples in a single batch.\n",
            "     |  rvs : callable or tuple of callables, optional\n",
            "     |      A callable or sequence of callables that generates random variates\n",
            "     |      under the null hypothesis. Each element of `rvs` must be a callable\n",
            "     |      that accepts keyword argument ``size`` (e.g. ``rvs(size=(m, n))``) and\n",
            "     |      returns an N-d array sample of that shape. If `rvs` is a sequence, the\n",
            "     |      number of callables in `rvs` must match the number of samples passed\n",
            "     |      to the hypothesis test in which the `MonteCarloMethod` is used. Default\n",
            "     |      is ``None``, in which case the hypothesis test function chooses values\n",
            "     |      to match the standard version of the hypothesis test. For example,\n",
            "     |      the null hypothesis of `scipy.stats.pearsonr` is typically that the\n",
            "     |      samples are drawn from the standard normal distribution, so\n",
            "     |      ``rvs = (rng.normal, rng.normal)`` where\n",
            "     |      ``rng = np.random.default_rng()``.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      MonteCarloMethod\n",
            "     |      ResamplingMethod\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __init__(self, n_resamples: 'int' = 9999, batch: 'int' = None, rvs: 'object' = None) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __annotations__ = {'rvs': 'object'}\n",
            "     |  \n",
            "     |  __dataclass_fields__ = {'batch': Field(name='batch',type='int',default...\n",
            "     |  \n",
            "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            "     |  \n",
            "     |  __hash__ = None\n",
            "     |  \n",
            "     |  __match_args__ = ('n_resamples', 'batch', 'rvs')\n",
            "     |  \n",
            "     |  rvs = None\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  batch = None\n",
            "     |  \n",
            "     |  n_resamples = 9999\n",
            "    \n",
            "    class NearConstantInputWarning(DegenerateDataWarning)\n",
            "     |  NearConstantInputWarning(msg=None)\n",
            "     |  \n",
            "     |  Warns when all values in data are nearly equal.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      NearConstantInputWarning\n",
            "     |      DegenerateDataWarning\n",
            "     |      builtins.RuntimeWarning\n",
            "     |      builtins.Warning\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, msg=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from DegenerateDataWarning:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods inherited from builtins.RuntimeWarning:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) class method of builtins.RuntimeWarning\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  add_note(...)\n",
            "     |      Exception.add_note(note) --\n",
            "     |      add a note to the exception\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "    \n",
            "    class PermutationMethod(ResamplingMethod)\n",
            "     |  PermutationMethod(n_resamples: 'int' = 9999, batch: 'int' = None, random_state: 'object' = None) -> None\n",
            "     |  \n",
            "     |  Configuration information for a permutation hypothesis test.\n",
            "     |  \n",
            "     |  Instances of this class can be passed into the `method` parameter of some\n",
            "     |  hypothesis test functions to perform a permutation version of the\n",
            "     |  hypothesis tests.\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  n_resamples : int, optional\n",
            "     |      The number of resamples to perform. Default is 9999.\n",
            "     |  batch : int, optional\n",
            "     |      The number of resamples to process in each vectorized call to\n",
            "     |      the statistic. Batch sizes >>1 tend to be faster when the statistic\n",
            "     |      is vectorized, but memory usage scales linearly with the batch size.\n",
            "     |      Default is ``None``, which processes all resamples in a single batch.\n",
            "     |  random_state : {None, int, `numpy.random.Generator`,\n",
            "     |                  `numpy.random.RandomState`}, optional\n",
            "     |  \n",
            "     |      Pseudorandom number generator state used to generate resamples.\n",
            "     |  \n",
            "     |      If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |      instance, then that instance is used.\n",
            "     |      If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `random_state`.\n",
            "     |      If `random_state` is ``None`` (default), the\n",
            "     |      `numpy.random.RandomState` singleton is used.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      PermutationMethod\n",
            "     |      ResamplingMethod\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __eq__(self, other)\n",
            "     |      Return self==value.\n",
            "     |  \n",
            "     |  __init__(self, n_resamples: 'int' = 9999, batch: 'int' = None, random_state: 'object' = None) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  __repr__(self)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  __annotations__ = {'random_state': 'object'}\n",
            "     |  \n",
            "     |  __dataclass_fields__ = {'batch': Field(name='batch',type='int',default...\n",
            "     |  \n",
            "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            "     |  \n",
            "     |  __hash__ = None\n",
            "     |  \n",
            "     |  __match_args__ = ('n_resamples', 'batch', 'random_state')\n",
            "     |  \n",
            "     |  random_state = None\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from ResamplingMethod:\n",
            "     |  \n",
            "     |  batch = None\n",
            "     |  \n",
            "     |  n_resamples = 9999\n",
            "    \n",
            "    class gaussian_kde(builtins.object)\n",
            "     |  gaussian_kde(dataset, bw_method=None, weights=None)\n",
            "     |  \n",
            "     |  Representation of a kernel-density estimate using Gaussian kernels.\n",
            "     |  \n",
            "     |  Kernel density estimation is a way to estimate the probability density\n",
            "     |  function (PDF) of a random variable in a non-parametric way.\n",
            "     |  `gaussian_kde` works for both uni-variate and multi-variate data.   It\n",
            "     |  includes automatic bandwidth determination.  The estimation works best for\n",
            "     |  a unimodal distribution; bimodal or multi-modal distributions tend to be\n",
            "     |  oversmoothed.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  dataset : array_like\n",
            "     |      Datapoints to estimate from. In case of univariate data this is a 1-D\n",
            "     |      array, otherwise a 2-D array with shape (# of dims, # of data).\n",
            "     |  bw_method : str, scalar or callable, optional\n",
            "     |      The method used to calculate the estimator bandwidth.  This can be\n",
            "     |      'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n",
            "     |      this will be used directly as `kde.factor`.  If a callable, it should\n",
            "     |      take a `gaussian_kde` instance as only parameter and return a scalar.\n",
            "     |      If None (default), 'scott' is used.  See Notes for more details.\n",
            "     |  weights : array_like, optional\n",
            "     |      weights of datapoints. This must be the same shape as dataset.\n",
            "     |      If None (default), the samples are assumed to be equally weighted\n",
            "     |  \n",
            "     |  Attributes\n",
            "     |  ----------\n",
            "     |  dataset : ndarray\n",
            "     |      The dataset with which `gaussian_kde` was initialized.\n",
            "     |  d : int\n",
            "     |      Number of dimensions.\n",
            "     |  n : int\n",
            "     |      Number of datapoints.\n",
            "     |  neff : int\n",
            "     |      Effective number of datapoints.\n",
            "     |  \n",
            "     |      .. versionadded:: 1.2.0\n",
            "     |  factor : float\n",
            "     |      The bandwidth factor, obtained from `kde.covariance_factor`. The square\n",
            "     |      of `kde.factor` multiplies the covariance matrix of the data in the kde\n",
            "     |      estimation.\n",
            "     |  covariance : ndarray\n",
            "     |      The covariance matrix of `dataset`, scaled by the calculated bandwidth\n",
            "     |      (`kde.factor`).\n",
            "     |  inv_cov : ndarray\n",
            "     |      The inverse of `covariance`.\n",
            "     |  \n",
            "     |  Methods\n",
            "     |  -------\n",
            "     |  evaluate\n",
            "     |  __call__\n",
            "     |  integrate_gaussian\n",
            "     |  integrate_box_1d\n",
            "     |  integrate_box\n",
            "     |  integrate_kde\n",
            "     |  pdf\n",
            "     |  logpdf\n",
            "     |  resample\n",
            "     |  set_bandwidth\n",
            "     |  covariance_factor\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  Bandwidth selection strongly influences the estimate obtained from the KDE\n",
            "     |  (much more so than the actual shape of the kernel).  Bandwidth selection\n",
            "     |  can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n",
            "     |  methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n",
            "     |  uses a rule of thumb, the default is Scott's Rule.\n",
            "     |  \n",
            "     |  Scott's Rule [1]_, implemented as `scotts_factor`, is::\n",
            "     |  \n",
            "     |      n**(-1./(d+4)),\n",
            "     |  \n",
            "     |  with ``n`` the number of data points and ``d`` the number of dimensions.\n",
            "     |  In the case of unequally weighted points, `scotts_factor` becomes::\n",
            "     |  \n",
            "     |      neff**(-1./(d+4)),\n",
            "     |  \n",
            "     |  with ``neff`` the effective number of datapoints.\n",
            "     |  Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n",
            "     |  \n",
            "     |      (n * (d + 2) / 4.)**(-1. / (d + 4)).\n",
            "     |  \n",
            "     |  or in the case of unequally weighted points::\n",
            "     |  \n",
            "     |      (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n",
            "     |  \n",
            "     |  Good general descriptions of kernel density estimation can be found in [1]_\n",
            "     |  and [2]_, the mathematics for this multi-dimensional implementation can be\n",
            "     |  found in [1]_.\n",
            "     |  \n",
            "     |  With a set of weighted samples, the effective number of datapoints ``neff``\n",
            "     |  is defined by::\n",
            "     |  \n",
            "     |      neff = sum(weights)^2 / sum(weights^2)\n",
            "     |  \n",
            "     |  as detailed in [5]_.\n",
            "     |  \n",
            "     |  `gaussian_kde` does not currently support data that lies in a\n",
            "     |  lower-dimensional subspace of the space in which it is expressed. For such\n",
            "     |  data, consider performing principle component analysis / dimensionality\n",
            "     |  reduction and using `gaussian_kde` with the transformed data.\n",
            "     |  \n",
            "     |  References\n",
            "     |  ----------\n",
            "     |  .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n",
            "     |         Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n",
            "     |  .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n",
            "     |         Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n",
            "     |         Chapman and Hall, London, 1986.\n",
            "     |  .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n",
            "     |         Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n",
            "     |  .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n",
            "     |         conditional density estimation\", Computational Statistics & Data\n",
            "     |         Analysis, Vol. 36, pp. 279-298, 2001.\n",
            "     |  .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n",
            "     |         Series A (General), 132, 272\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  Generate some random two-dimensional data:\n",
            "     |  \n",
            "     |  >>> import numpy as np\n",
            "     |  >>> from scipy import stats\n",
            "     |  >>> def measure(n):\n",
            "     |  ...     \"Measurement model, return two coupled measurements.\"\n",
            "     |  ...     m1 = np.random.normal(size=n)\n",
            "     |  ...     m2 = np.random.normal(scale=0.5, size=n)\n",
            "     |  ...     return m1+m2, m1-m2\n",
            "     |  \n",
            "     |  >>> m1, m2 = measure(2000)\n",
            "     |  >>> xmin = m1.min()\n",
            "     |  >>> xmax = m1.max()\n",
            "     |  >>> ymin = m2.min()\n",
            "     |  >>> ymax = m2.max()\n",
            "     |  \n",
            "     |  Perform a kernel density estimate on the data:\n",
            "     |  \n",
            "     |  >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
            "     |  >>> positions = np.vstack([X.ravel(), Y.ravel()])\n",
            "     |  >>> values = np.vstack([m1, m2])\n",
            "     |  >>> kernel = stats.gaussian_kde(values)\n",
            "     |  >>> Z = np.reshape(kernel(positions).T, X.shape)\n",
            "     |  \n",
            "     |  Plot the results:\n",
            "     |  \n",
            "     |  >>> import matplotlib.pyplot as plt\n",
            "     |  >>> fig, ax = plt.subplots()\n",
            "     |  >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
            "     |  ...           extent=[xmin, xmax, ymin, ymax])\n",
            "     |  >>> ax.plot(m1, m2, 'k.', markersize=2)\n",
            "     |  >>> ax.set_xlim([xmin, xmax])\n",
            "     |  >>> ax.set_ylim([ymin, ymax])\n",
            "     |  >>> plt.show()\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __call__ = evaluate(self, points)\n",
            "     |  \n",
            "     |  __init__(self, dataset, bw_method=None, weights=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  covariance_factor = scotts_factor(self)\n",
            "     |  \n",
            "     |  evaluate(self, points)\n",
            "     |      Evaluate the estimated pdf on a set of points.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      points : (# of dimensions, # of points)-array\n",
            "     |          Alternatively, a (# of dimensions,) vector can be passed in and\n",
            "     |          treated as a single point.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      values : (# of points,)-array\n",
            "     |          The values at each point.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      ValueError : if the dimensionality of the input points is different than\n",
            "     |                   the dimensionality of the KDE.\n",
            "     |  \n",
            "     |  integrate_box(self, low_bounds, high_bounds, maxpts=None)\n",
            "     |      Computes the integral of a pdf over a rectangular interval.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      low_bounds : array_like\n",
            "     |          A 1-D array containing the lower bounds of integration.\n",
            "     |      high_bounds : array_like\n",
            "     |          A 1-D array containing the upper bounds of integration.\n",
            "     |      maxpts : int, optional\n",
            "     |          The maximum number of points to use for integration.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      value : scalar\n",
            "     |          The result of the integral.\n",
            "     |  \n",
            "     |  integrate_box_1d(self, low, high)\n",
            "     |      Computes the integral of a 1D pdf between two bounds.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      low : scalar\n",
            "     |          Lower bound of integration.\n",
            "     |      high : scalar\n",
            "     |          Upper bound of integration.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      value : scalar\n",
            "     |          The result of the integral.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      ValueError\n",
            "     |          If the KDE is over more than one dimension.\n",
            "     |  \n",
            "     |  integrate_gaussian(self, mean, cov)\n",
            "     |      Multiply estimated density by a multivariate Gaussian and integrate\n",
            "     |      over the whole space.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      mean : aray_like\n",
            "     |          A 1-D array, specifying the mean of the Gaussian.\n",
            "     |      cov : array_like\n",
            "     |          A 2-D array, specifying the covariance matrix of the Gaussian.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      result : scalar\n",
            "     |          The value of the integral.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      ValueError\n",
            "     |          If the mean or covariance of the input Gaussian differs from\n",
            "     |          the KDE's dimensionality.\n",
            "     |  \n",
            "     |  integrate_kde(self, other)\n",
            "     |      Computes the integral of the product of this  kernel density estimate\n",
            "     |      with another.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      other : gaussian_kde instance\n",
            "     |          The other kde.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      value : scalar\n",
            "     |          The result of the integral.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      ValueError\n",
            "     |          If the KDEs have different dimensionality.\n",
            "     |  \n",
            "     |  logpdf(self, x)\n",
            "     |      Evaluate the log of the estimated pdf on a provided set of points.\n",
            "     |  \n",
            "     |  marginal(self, dimensions)\n",
            "     |      Return a marginal KDE distribution\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      dimensions : int or 1-d array_like\n",
            "     |          The dimensions of the multivariate distribution corresponding\n",
            "     |          with the marginal variables, that is, the indices of the dimensions\n",
            "     |          that are being retained. The other dimensions are marginalized out.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      marginal_kde : gaussian_kde\n",
            "     |          An object representing the marginal distribution.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      .. versionadded:: 1.10.0\n",
            "     |  \n",
            "     |  pdf(self, x)\n",
            "     |      Evaluate the estimated pdf on a provided set of points.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is an alias for `gaussian_kde.evaluate`.  See the ``evaluate``\n",
            "     |      docstring for more details.\n",
            "     |  \n",
            "     |  resample(self, size=None, seed=None)\n",
            "     |      Randomly sample a dataset from the estimated pdf.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      size : int, optional\n",
            "     |          The number of samples to draw.  If not provided, then the size is\n",
            "     |          the same as the effective number of samples in the underlying\n",
            "     |          dataset.\n",
            "     |      seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n",
            "     |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "     |          singleton is used.\n",
            "     |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "     |          seeded with `seed`.\n",
            "     |          If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "     |          that instance is used.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      resample : (self.d, `size`) ndarray\n",
            "     |          The sampled dataset.\n",
            "     |  \n",
            "     |  scotts_factor(self)\n",
            "     |      Computes the coefficient (`kde.factor`) that\n",
            "     |      multiplies the data covariance matrix to obtain the kernel covariance\n",
            "     |      matrix. The default is `scotts_factor`.  A subclass can overwrite this\n",
            "     |      method to provide a different method, or set it through a call to\n",
            "     |      `kde.set_bandwidth`.\n",
            "     |  \n",
            "     |  set_bandwidth(self, bw_method=None)\n",
            "     |      Compute the estimator bandwidth with given method.\n",
            "     |      \n",
            "     |      The new bandwidth calculated after a call to `set_bandwidth` is used\n",
            "     |      for subsequent evaluations of the estimated density.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      bw_method : str, scalar or callable, optional\n",
            "     |          The method used to calculate the estimator bandwidth.  This can be\n",
            "     |          'scott', 'silverman', a scalar constant or a callable.  If a\n",
            "     |          scalar, this will be used directly as `kde.factor`.  If a callable,\n",
            "     |          it should take a `gaussian_kde` instance as only parameter and\n",
            "     |          return a scalar.  If None (default), nothing happens; the current\n",
            "     |          `kde.covariance_factor` method is kept.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      .. versionadded:: 0.11\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      >>> import numpy as np\n",
            "     |      >>> import scipy.stats as stats\n",
            "     |      >>> x1 = np.array([-7, -5, 1, 4, 5.])\n",
            "     |      >>> kde = stats.gaussian_kde(x1)\n",
            "     |      >>> xs = np.linspace(-10, 10, num=50)\n",
            "     |      >>> y1 = kde(xs)\n",
            "     |      >>> kde.set_bandwidth(bw_method='silverman')\n",
            "     |      >>> y2 = kde(xs)\n",
            "     |      >>> kde.set_bandwidth(bw_method=kde.factor / 3.)\n",
            "     |      >>> y3 = kde(xs)\n",
            "     |      \n",
            "     |      >>> import matplotlib.pyplot as plt\n",
            "     |      >>> fig, ax = plt.subplots()\n",
            "     |      >>> ax.plot(x1, np.full(x1.shape, 1 / (4. * x1.size)), 'bo',\n",
            "     |      ...         label='Data points (rescaled)')\n",
            "     |      >>> ax.plot(xs, y1, label='Scott (default)')\n",
            "     |      >>> ax.plot(xs, y2, label='Silverman')\n",
            "     |      >>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')\n",
            "     |      >>> ax.legend()\n",
            "     |      >>> plt.show()\n",
            "     |  \n",
            "     |  silverman_factor(self)\n",
            "     |      Compute the Silverman factor.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      s : float\n",
            "     |          The silverman factor.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |  \n",
            "     |  inv_cov\n",
            "     |  \n",
            "     |  neff\n",
            "     |  \n",
            "     |  weights\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "    \n",
            "    class rv_continuous(rv_generic)\n",
            "     |  rv_continuous(momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, seed=None)\n",
            "     |  \n",
            "     |  A generic continuous random variable class meant for subclassing.\n",
            "     |  \n",
            "     |  `rv_continuous` is a base class to construct specific distribution classes\n",
            "     |  and instances for continuous random variables. It cannot be used\n",
            "     |  directly as a distribution.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  momtype : int, optional\n",
            "     |      The type of generic moment calculation to use: 0 for pdf, 1 (default)\n",
            "     |      for ppf.\n",
            "     |  a : float, optional\n",
            "     |      Lower bound of the support of the distribution, default is minus\n",
            "     |      infinity.\n",
            "     |  b : float, optional\n",
            "     |      Upper bound of the support of the distribution, default is plus\n",
            "     |      infinity.\n",
            "     |  xtol : float, optional\n",
            "     |      The tolerance for fixed point calculation for generic ppf.\n",
            "     |  badvalue : float, optional\n",
            "     |      The value in a result arrays that indicates a value that for which\n",
            "     |      some argument restriction is violated, default is np.nan.\n",
            "     |  name : str, optional\n",
            "     |      The name of the instance. This string is used to construct the default\n",
            "     |      example for distributions.\n",
            "     |  longname : str, optional\n",
            "     |      This string is used as part of the first line of the docstring returned\n",
            "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
            "     |      for backwards compatibility, do not use for new subclasses.\n",
            "     |  shapes : str, optional\n",
            "     |      The shape of the distribution. For example ``\"m, n\"`` for a\n",
            "     |      distribution that takes two integers as the two shape arguments for all\n",
            "     |      its methods. If not provided, shape parameters will be inferred from\n",
            "     |      the signature of the private methods, ``_pdf`` and ``_cdf`` of the\n",
            "     |      instance.\n",
            "     |  seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n",
            "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "     |      singleton is used.\n",
            "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `seed`.\n",
            "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "     |      that instance is used.\n",
            "     |  \n",
            "     |  Methods\n",
            "     |  -------\n",
            "     |  rvs\n",
            "     |  pdf\n",
            "     |  logpdf\n",
            "     |  cdf\n",
            "     |  logcdf\n",
            "     |  sf\n",
            "     |  logsf\n",
            "     |  ppf\n",
            "     |  isf\n",
            "     |  moment\n",
            "     |  stats\n",
            "     |  entropy\n",
            "     |  expect\n",
            "     |  median\n",
            "     |  mean\n",
            "     |  std\n",
            "     |  var\n",
            "     |  interval\n",
            "     |  __call__\n",
            "     |  fit\n",
            "     |  fit_loc_scale\n",
            "     |  nnlf\n",
            "     |  support\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  Public methods of an instance of a distribution class (e.g., ``pdf``,\n",
            "     |  ``cdf``) check their arguments and pass valid arguments to private,\n",
            "     |  computational methods (``_pdf``, ``_cdf``). For ``pdf(x)``, ``x`` is valid\n",
            "     |  if it is within the support of the distribution.\n",
            "     |  Whether a shape parameter is valid is decided by an ``_argcheck`` method\n",
            "     |  (which defaults to checking that its arguments are strictly positive.)\n",
            "     |  \n",
            "     |  **Subclassing**\n",
            "     |  \n",
            "     |  New random variables can be defined by subclassing the `rv_continuous` class\n",
            "     |  and re-defining at least the ``_pdf`` or the ``_cdf`` method (normalized\n",
            "     |  to location 0 and scale 1).\n",
            "     |  \n",
            "     |  If positive argument checking is not correct for your RV\n",
            "     |  then you will also need to re-define the ``_argcheck`` method.\n",
            "     |  \n",
            "     |  For most of the scipy.stats distributions, the support interval doesn't\n",
            "     |  depend on the shape parameters. ``x`` being in the support interval is\n",
            "     |  equivalent to ``self.a <= x <= self.b``.  If either of the endpoints of\n",
            "     |  the support do depend on the shape parameters, then\n",
            "     |  i) the distribution must implement the ``_get_support`` method; and\n",
            "     |  ii) those dependent endpoints must be omitted from the distribution's\n",
            "     |  call to the ``rv_continuous`` initializer.\n",
            "     |  \n",
            "     |  Correct, but potentially slow defaults exist for the remaining\n",
            "     |  methods but for speed and/or accuracy you can over-ride::\n",
            "     |  \n",
            "     |    _logpdf, _cdf, _logcdf, _ppf, _rvs, _isf, _sf, _logsf\n",
            "     |  \n",
            "     |  The default method ``_rvs`` relies on the inverse of the cdf, ``_ppf``,\n",
            "     |  applied to a uniform random variate. In order to generate random variates\n",
            "     |  efficiently, either the default ``_ppf`` needs to be overwritten (e.g.\n",
            "     |  if the inverse cdf can expressed in an explicit form) or a sampling\n",
            "     |  method needs to be implemented in a custom ``_rvs`` method.\n",
            "     |  \n",
            "     |  If possible, you should override ``_isf``, ``_sf`` or ``_logsf``.\n",
            "     |  The main reason would be to improve numerical accuracy: for example,\n",
            "     |  the survival function ``_sf`` is computed as ``1 - _cdf`` which can\n",
            "     |  result in loss of precision if ``_cdf(x)`` is close to one.\n",
            "     |  \n",
            "     |  **Methods that can be overwritten by subclasses**\n",
            "     |  ::\n",
            "     |  \n",
            "     |    _rvs\n",
            "     |    _pdf\n",
            "     |    _cdf\n",
            "     |    _sf\n",
            "     |    _ppf\n",
            "     |    _isf\n",
            "     |    _stats\n",
            "     |    _munp\n",
            "     |    _entropy\n",
            "     |    _argcheck\n",
            "     |    _get_support\n",
            "     |  \n",
            "     |  There are additional (internal and private) generic methods that can\n",
            "     |  be useful for cross-checking and for debugging, but might work in all\n",
            "     |  cases when directly called.\n",
            "     |  \n",
            "     |  A note on ``shapes``: subclasses need not specify them explicitly. In this\n",
            "     |  case, `shapes` will be automatically deduced from the signatures of the\n",
            "     |  overridden methods (`pdf`, `cdf` etc).\n",
            "     |  If, for some reason, you prefer to avoid relying on introspection, you can\n",
            "     |  specify ``shapes`` explicitly as an argument to the instance constructor.\n",
            "     |  \n",
            "     |  \n",
            "     |  **Frozen Distributions**\n",
            "     |  \n",
            "     |  Normally, you must provide shape parameters (and, optionally, location and\n",
            "     |  scale parameters to each call of a method of a distribution.\n",
            "     |  \n",
            "     |  Alternatively, the object may be called (as a function) to fix the shape,\n",
            "     |  location, and scale parameters returning a \"frozen\" continuous RV object:\n",
            "     |  \n",
            "     |  rv = generic(<shape(s)>, loc=0, scale=1)\n",
            "     |      `rv_frozen` object with the same methods but holding the given shape,\n",
            "     |      location, and scale fixed\n",
            "     |  \n",
            "     |  **Statistics**\n",
            "     |  \n",
            "     |  Statistics are computed using numerical integration by default.\n",
            "     |  For speed you can redefine this using ``_stats``:\n",
            "     |  \n",
            "     |   - take shape parameters and return mu, mu2, g1, g2\n",
            "     |   - If you can't compute one of these, return it as None\n",
            "     |   - Can also be defined with a keyword argument ``moments``, which is a\n",
            "     |     string composed of \"m\", \"v\", \"s\", and/or \"k\".\n",
            "     |     Only the components appearing in string should be computed and\n",
            "     |     returned in the order \"m\", \"v\", \"s\", or \"k\"  with missing values\n",
            "     |     returned as None.\n",
            "     |  \n",
            "     |  Alternatively, you can override ``_munp``, which takes ``n`` and shape\n",
            "     |  parameters and returns the n-th non-central moment of the distribution.\n",
            "     |  \n",
            "     |  **Deepcopying / Pickling**\n",
            "     |  \n",
            "     |  If a distribution or frozen distribution is deepcopied (pickled/unpickled,\n",
            "     |  etc.), any underlying random number generator is deepcopied with it. An\n",
            "     |  implication is that if a distribution relies on the singleton RandomState\n",
            "     |  before copying, it will rely on a copy of that random state after copying,\n",
            "     |  and ``np.random.seed`` will no longer control the state.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  To create a new Gaussian distribution, we would do the following:\n",
            "     |  \n",
            "     |  >>> from scipy.stats import rv_continuous\n",
            "     |  >>> class gaussian_gen(rv_continuous):\n",
            "     |  ...     \"Gaussian distribution\"\n",
            "     |  ...     def _pdf(self, x):\n",
            "     |  ...         return np.exp(-x**2 / 2.) / np.sqrt(2.0 * np.pi)\n",
            "     |  >>> gaussian = gaussian_gen(name='gaussian')\n",
            "     |  \n",
            "     |  ``scipy.stats`` distributions are *instances*, so here we subclass\n",
            "     |  `rv_continuous` and create an instance. With this, we now have\n",
            "     |  a fully functional distribution with all relevant methods automagically\n",
            "     |  generated by the framework.\n",
            "     |  \n",
            "     |  Note that above we defined a standard normal distribution, with zero mean\n",
            "     |  and unit variance. Shifting and scaling of the distribution can be done\n",
            "     |  by using ``loc`` and ``scale`` parameters: ``gaussian.pdf(x, loc, scale)``\n",
            "     |  essentially computes ``y = (x - loc) / scale`` and\n",
            "     |  ``gaussian._pdf(y) / scale``.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      rv_continuous\n",
            "     |      rv_generic\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __init__(self, momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, seed=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  cdf(self, x, *args, **kwds)\n",
            "     |      Cumulative distribution function of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      cdf : ndarray\n",
            "     |          Cumulative distribution function evaluated at `x`\n",
            "     |  \n",
            "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "     |      Calculate expected value of a function with respect to the\n",
            "     |      distribution by numerical integration.\n",
            "     |      \n",
            "     |      The expected value of a function ``f(x)`` with respect to a\n",
            "     |      distribution ``dist`` is defined as::\n",
            "     |      \n",
            "     |                  ub\n",
            "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
            "     |                  lb\n",
            "     |      \n",
            "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
            "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
            "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
            "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
            "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
            "     |      outside a finite interval in which case the expectation is\n",
            "     |      calculated within the finite range ``[lb, ub]``.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : callable, optional\n",
            "     |          Function for which integral is calculated. Takes only one argument.\n",
            "     |          The default is the identity mapping f(x) = x.\n",
            "     |      args : tuple, optional\n",
            "     |          Shape parameters of the distribution.\n",
            "     |      loc : float, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : float, optional\n",
            "     |          Scale parameter (default=1).\n",
            "     |      lb, ub : scalar, optional\n",
            "     |          Lower and upper bound for integration. Default is set to the\n",
            "     |          support of the distribution.\n",
            "     |      conditional : bool, optional\n",
            "     |          If True, the integral is corrected by the conditional probability\n",
            "     |          of the integration interval.  The return value is the expectation\n",
            "     |          of the function, conditional on being in the given interval.\n",
            "     |          Default is False.\n",
            "     |      \n",
            "     |      Additional keyword arguments are passed to the integration routine.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      expect : float\n",
            "     |          The calculated expected value.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The integration behavior of this function is inherited from\n",
            "     |      `scipy.integrate.quad`. Neither this function nor\n",
            "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
            "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
            "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
            "     |      \n",
            "     |      Likewise, the accuracy of results is not verified by the function.\n",
            "     |      `scipy.integrate.quad` is typically reliable for integrals that are\n",
            "     |      numerically favorable, but it is not guaranteed to converge\n",
            "     |      to a correct value for all possible intervals and integrands. This\n",
            "     |      function is provided for convenience; for critical applications,\n",
            "     |      check results against other integration methods.\n",
            "     |      \n",
            "     |      The function is not vectorized.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      \n",
            "     |      To understand the effect of the bounds of integration consider\n",
            "     |      \n",
            "     |      >>> from scipy.stats import expon\n",
            "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
            "     |      0.6321205588285578\n",
            "     |      \n",
            "     |      This is close to\n",
            "     |      \n",
            "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
            "     |      0.6321205588285577\n",
            "     |      \n",
            "     |      If ``conditional=True``\n",
            "     |      \n",
            "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
            "     |      1.0000000000000002\n",
            "     |      \n",
            "     |      The slight deviation from 1 is due to numerical integration.\n",
            "     |      \n",
            "     |      The integrand can be treated as a complex-valued function\n",
            "     |      by passing ``complex_func=True`` to `scipy.integrate.quad` .\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats import vonmises\n",
            "     |      >>> res = vonmises(loc=2, kappa=1).expect(lambda x: np.exp(1j*x),\n",
            "     |      ...                                       complex_func=True)\n",
            "     |      >>> res\n",
            "     |      (-0.18576377217422957+0.40590124735052263j)\n",
            "     |      \n",
            "     |      >>> np.angle(res)  # location of the (circular) distribution\n",
            "     |      2.0\n",
            "     |  \n",
            "     |  fit(self, data, *args, **kwds)\n",
            "     |      Return estimates of shape (if applicable), location, and scale\n",
            "     |      parameters from data. The default estimation method is Maximum\n",
            "     |      Likelihood Estimation (MLE), but Method of Moments (MM)\n",
            "     |      is also available.\n",
            "     |      \n",
            "     |      Starting estimates for the fit are given by input arguments;\n",
            "     |      for any arguments not provided with starting estimates,\n",
            "     |      ``self._fitstart(data)`` is called to generate such.\n",
            "     |      \n",
            "     |      One can hold some parameters fixed to specific values by passing in\n",
            "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
            "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
            "     |      respectively).\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data : array_like or `CensoredData` instance\n",
            "     |          Data to use in estimating the distribution parameters.\n",
            "     |      arg1, arg2, arg3,... : floats, optional\n",
            "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
            "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
            "     |          No default value.\n",
            "     |      **kwds : floats, optional\n",
            "     |          - `loc`: initial guess of the distribution's location parameter.\n",
            "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
            "     |      \n",
            "     |          Special keyword arguments are recognized as holding certain\n",
            "     |          parameters fixed:\n",
            "     |      \n",
            "     |          - f0...fn : hold respective shape parameters fixed.\n",
            "     |            Alternatively, shape parameters to fix can be specified by name.\n",
            "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
            "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
            "     |            equivalent to ``f1``.\n",
            "     |      \n",
            "     |          - floc : hold location parameter fixed to specified value.\n",
            "     |      \n",
            "     |          - fscale : hold scale parameter fixed to specified value.\n",
            "     |      \n",
            "     |          - optimizer : The optimizer to use.  The optimizer must take\n",
            "     |            ``func`` and starting position as the first two arguments,\n",
            "     |            plus ``args`` (for extra arguments to pass to the\n",
            "     |            function to be optimized) and ``disp``. \n",
            "     |            The ``fit`` method calls the optimizer with ``disp=0`` to suppress output.\n",
            "     |            The optimizer must return the estimated parameters.\n",
            "     |      \n",
            "     |          - method : The method to use. The default is \"MLE\" (Maximum\n",
            "     |            Likelihood Estimate); \"MM\" (Method of Moments)\n",
            "     |            is also available.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      TypeError, ValueError\n",
            "     |          If an input is invalid\n",
            "     |      `~scipy.stats.FitError`\n",
            "     |          If fitting fails or the fit produced would be invalid\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      parameter_tuple : tuple of floats\n",
            "     |          Estimates for any shape parameters (if applicable), followed by\n",
            "     |          those for location and scale. For most random variables, shape\n",
            "     |          statistics will be returned, but there are exceptions (e.g.\n",
            "     |          ``norm``).\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      With ``method=\"MLE\"`` (default), the fit is computed by minimizing\n",
            "     |      the negative log-likelihood function. A large, finite penalty\n",
            "     |      (rather than infinite negative log-likelihood) is applied for\n",
            "     |      observations beyond the support of the distribution.\n",
            "     |      \n",
            "     |      With ``method=\"MM\"``, the fit is computed by minimizing the L2 norm\n",
            "     |      of the relative errors between the first *k* raw (about zero) data\n",
            "     |      moments and the corresponding distribution moments, where *k* is the\n",
            "     |      number of non-fixed parameters.\n",
            "     |      More precisely, the objective function is::\n",
            "     |      \n",
            "     |          (((data_moments - dist_moments)\n",
            "     |            / np.maximum(np.abs(data_moments), 1e-8))**2).sum()\n",
            "     |      \n",
            "     |      where the constant ``1e-8`` avoids division by zero in case of\n",
            "     |      vanishing data moments. Typically, this error norm can be reduced to\n",
            "     |      zero.\n",
            "     |      Note that the standard method of moments can produce parameters for\n",
            "     |      which some data are outside the support of the fitted distribution;\n",
            "     |      this implementation does nothing to prevent this.\n",
            "     |      \n",
            "     |      For either method,\n",
            "     |      the returned answer is not guaranteed to be globally optimal; it\n",
            "     |      may only be locally optimal, or the optimization may fail altogether.\n",
            "     |      If the data contain any of ``np.nan``, ``np.inf``, or ``-np.inf``,\n",
            "     |      the `fit` method will raise a ``RuntimeError``.\n",
            "     |      \n",
            "     |      When passing a ``CensoredData`` instance to ``data``, the log-likelihood\n",
            "     |      function is defined as:\n",
            "     |      \n",
            "     |      .. math::\n",
            "     |      \n",
            "     |          l(\\pmb{\\theta}; k) & = \\sum\n",
            "     |                                  \\log(f(k_u; \\pmb{\\theta}))\n",
            "     |                              + \\sum\n",
            "     |                                  \\log(F(k_l; \\pmb{\\theta})) \\\\\n",
            "     |                              & + \\sum \n",
            "     |                                  \\log(1 - F(k_r; \\pmb{\\theta})) \\\\\n",
            "     |                              & + \\sum\n",
            "     |                                  \\log(F(k_{\\text{high}, i}; \\pmb{\\theta})\n",
            "     |                                  - F(k_{\\text{low}, i}; \\pmb{\\theta}))\n",
            "     |      \n",
            "     |      where :math:`f` and :math:`F` are the pdf and cdf, respectively, of the\n",
            "     |      function being fitted, :math:`\\pmb{\\theta}` is the parameter vector,\n",
            "     |      :math:`u` are the indices of uncensored observations,\n",
            "     |      :math:`l` are the indices of left-censored observations,\n",
            "     |      :math:`r` are the indices of right-censored observations,\n",
            "     |      subscripts \"low\"/\"high\" denote endpoints of interval-censored observations, and\n",
            "     |      :math:`i` are the indices of interval-censored observations.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      \n",
            "     |      Generate some data to fit: draw random variates from the `beta`\n",
            "     |      distribution\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats import beta\n",
            "     |      >>> a, b = 1., 2.\n",
            "     |      >>> rng = np.random.default_rng(172786373191770012695001057628748821561)\n",
            "     |      >>> x = beta.rvs(a, b, size=1000, random_state=rng)\n",
            "     |      \n",
            "     |      Now we can fit all four parameters (``a``, ``b``, ``loc`` and\n",
            "     |      ``scale``):\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
            "     |      >>> a1, b1, loc1, scale1\n",
            "     |      (1.0198945204435628, 1.9484708982737828, 4.372241314917588e-05, 0.9979078845964814)\n",
            "     |      \n",
            "     |      The fit can be done also using a custom optimizer:\n",
            "     |      \n",
            "     |      >>> from scipy.optimize import minimize\n",
            "     |      >>> def custom_optimizer(func, x0, args=(), disp=0):\n",
            "     |      ...     res = minimize(func, x0, args, method=\"slsqp\", options={\"disp\": disp})\n",
            "     |      ...     if res.success:\n",
            "     |      ...         return res.x\n",
            "     |      ...     raise RuntimeError('optimization routine failed')\n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, method=\"MLE\", optimizer=custom_optimizer)\n",
            "     |      >>> a1, b1, loc1, scale1\n",
            "     |      (1.0198821087258905, 1.948484145914738, 4.3705304486881485e-05, 0.9979104663953395)\n",
            "     |      \n",
            "     |      We can also use some prior knowledge about the dataset: let's keep\n",
            "     |      ``loc`` and ``scale`` fixed:\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
            "     |      >>> loc1, scale1\n",
            "     |      (0, 1)\n",
            "     |      \n",
            "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
            "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
            "     |      equivalently, ``fa=1``:\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
            "     |      >>> a1\n",
            "     |      1\n",
            "     |      \n",
            "     |      Not all distributions return estimates for the shape parameters.\n",
            "     |      ``norm`` for example just returns estimates for location and scale:\n",
            "     |      \n",
            "     |      >>> from scipy.stats import norm\n",
            "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
            "     |      >>> loc1, scale1 = norm.fit(x)\n",
            "     |      >>> loc1, scale1\n",
            "     |      (0.92087172783841631, 2.0015750750324668)\n",
            "     |  \n",
            "     |  fit_loc_scale(self, data, *args)\n",
            "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data : array_like\n",
            "     |          Data to fit.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      Lhat : float\n",
            "     |          Estimated location parameter for the data.\n",
            "     |      Shat : float\n",
            "     |          Estimated scale parameter for the data.\n",
            "     |  \n",
            "     |  isf(self, q, *args, **kwds)\n",
            "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          upper tail probability\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x : ndarray or scalar\n",
            "     |          Quantile corresponding to the upper tail probability q.\n",
            "     |  \n",
            "     |  logcdf(self, x, *args, **kwds)\n",
            "     |      Log of the cumulative distribution function at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logcdf : array_like\n",
            "     |          Log of the cumulative distribution function evaluated at x\n",
            "     |  \n",
            "     |  logpdf(self, x, *args, **kwds)\n",
            "     |      Log of the probability density function at x of the given RV.\n",
            "     |      \n",
            "     |      This uses a more numerically accurate calculation if available.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logpdf : array_like\n",
            "     |          Log of the probability density function evaluated at x\n",
            "     |  \n",
            "     |  logsf(self, x, *args, **kwds)\n",
            "     |      Log of the survival function of the given RV.\n",
            "     |      \n",
            "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
            "     |      evaluated at `x`.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logsf : ndarray\n",
            "     |          Log of the survival function evaluated at `x`.\n",
            "     |  \n",
            "     |  pdf(self, x, *args, **kwds)\n",
            "     |      Probability density function at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      pdf : ndarray\n",
            "     |          Probability density function evaluated at x\n",
            "     |  \n",
            "     |  ppf(self, q, *args, **kwds)\n",
            "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          lower tail probability\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x : array_like\n",
            "     |          quantile corresponding to the lower tail probability q.\n",
            "     |  \n",
            "     |  sf(self, x, *args, **kwds)\n",
            "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      sf : array_like\n",
            "     |          Survival function evaluated at x\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from rv_generic:\n",
            "     |  \n",
            "     |  __call__(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  entropy(self, *args, **kwds)\n",
            "     |      Differential entropy of the RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : array_like, optional  (continuous distributions only).\n",
            "     |          Scale parameter (default=1).\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Entropy is defined base `e`:\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats._distn_infrastructure import rv_discrete\n",
            "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
            "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
            "     |      True\n",
            "     |  \n",
            "     |  freeze(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  interval(self, confidence, *args, **kwds)\n",
            "     |      Confidence interval with equal areas around the median.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      confidence : array_like of float\n",
            "     |          Probability that an rv will be drawn from the returned range.\n",
            "     |          Each value should be in the range [0, 1].\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : ndarray of float\n",
            "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
            "     |          possible values.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is implemented as ``ppf([p_tail, 1-p_tail])``, where\n",
            "     |      ``ppf`` is the inverse cumulative distribution function and\n",
            "     |      ``p_tail = (1-confidence)/2``. Suppose ``[c, d]`` is the support of a\n",
            "     |      discrete distribution; then ``ppf([0, 1]) == (c-1, d)``. Therefore,\n",
            "     |      when ``confidence=1`` and the distribution is discrete, the left end\n",
            "     |      of the interval will be beyond the support of the distribution.\n",
            "     |      For discrete distributions, the interval will limit the probability\n",
            "     |      in each tail to be less than or equal to ``p_tail`` (usually\n",
            "     |      strictly less).\n",
            "     |  \n",
            "     |  mean(self, *args, **kwds)\n",
            "     |      Mean of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      mean : float\n",
            "     |          the mean of the distribution\n",
            "     |  \n",
            "     |  median(self, *args, **kwds)\n",
            "     |      Median of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          Scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      median : float\n",
            "     |          The median of the distribution.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      rv_discrete.ppf\n",
            "     |          Inverse of the CDF\n",
            "     |  \n",
            "     |  moment(self, order, *args, **kwds)\n",
            "     |      non-central moment of distribution of specified order.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      order : int, order >= 1\n",
            "     |          Order of moment.\n",
            "     |      arg1, arg2, arg3,... : float\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |  \n",
            "     |  nnlf(self, theta, x)\n",
            "     |      Negative loglikelihood function.\n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
            "     |      parameters (including loc and scale).\n",
            "     |  \n",
            "     |  rvs(self, *args, **kwds)\n",
            "     |      Random variates of given type.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : array_like, optional\n",
            "     |          Scale parameter (default=1).\n",
            "     |      size : int or tuple of ints, optional\n",
            "     |          Defining number of random variates (default is 1).\n",
            "     |      random_state : {None, int, `numpy.random.Generator`,\n",
            "     |                      `numpy.random.RandomState`}, optional\n",
            "     |      \n",
            "     |          If `random_state` is None (or `np.random`), the\n",
            "     |          `numpy.random.RandomState` singleton is used.\n",
            "     |          If `random_state` is an int, a new ``RandomState`` instance is\n",
            "     |          used, seeded with `random_state`.\n",
            "     |          If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |          instance, that instance is used.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rvs : ndarray or scalar\n",
            "     |          Random variates of given `size`.\n",
            "     |  \n",
            "     |  stats(self, *args, **kwds)\n",
            "     |      Some statistics of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional (continuous RVs only)\n",
            "     |          scale parameter (default=1)\n",
            "     |      moments : str, optional\n",
            "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
            "     |          'm' = mean,\n",
            "     |          'v' = variance,\n",
            "     |          's' = (Fisher's) skew,\n",
            "     |          'k' = (Fisher's) kurtosis.\n",
            "     |          (default is 'mv')\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      stats : sequence\n",
            "     |          of requested moments.\n",
            "     |  \n",
            "     |  std(self, *args, **kwds)\n",
            "     |      Standard deviation of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      std : float\n",
            "     |          standard deviation of the distribution\n",
            "     |  \n",
            "     |  support(self, *args, **kwargs)\n",
            "     |      Support of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : array_like\n",
            "     |          end-points of the distribution's support.\n",
            "     |  \n",
            "     |  var(self, *args, **kwds)\n",
            "     |      Variance of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      var : float\n",
            "     |          the variance of the distribution\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from rv_generic:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  random_state\n",
            "     |      Get or set the generator object for generating random variates.\n",
            "     |      \n",
            "     |      If `random_state` is None (or `np.random`), the\n",
            "     |      `numpy.random.RandomState` singleton is used.\n",
            "     |      If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `random_state`.\n",
            "     |      If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |      instance, that instance is used.\n",
            "    \n",
            "    class rv_discrete(rv_generic)\n",
            "     |  rv_discrete(a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, seed=None)\n",
            "     |  \n",
            "     |  A generic discrete random variable class meant for subclassing.\n",
            "     |  \n",
            "     |  `rv_discrete` is a base class to construct specific distribution classes\n",
            "     |  and instances for discrete random variables. It can also be used\n",
            "     |  to construct an arbitrary distribution defined by a list of support\n",
            "     |  points and corresponding probabilities.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  a : float, optional\n",
            "     |      Lower bound of the support of the distribution, default: 0\n",
            "     |  b : float, optional\n",
            "     |      Upper bound of the support of the distribution, default: plus infinity\n",
            "     |  moment_tol : float, optional\n",
            "     |      The tolerance for the generic calculation of moments.\n",
            "     |  values : tuple of two array_like, optional\n",
            "     |      ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero\n",
            "     |      probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``\n",
            "     |      and ``pk`` must have the same shape, and ``xk`` must be unique.\n",
            "     |  inc : integer, optional\n",
            "     |      Increment for the support of the distribution.\n",
            "     |      Default is 1. (other values have not been tested)\n",
            "     |  badvalue : float, optional\n",
            "     |      The value in a result arrays that indicates a value that for which\n",
            "     |      some argument restriction is violated, default is np.nan.\n",
            "     |  name : str, optional\n",
            "     |      The name of the instance. This string is used to construct the default\n",
            "     |      example for distributions.\n",
            "     |  longname : str, optional\n",
            "     |      This string is used as part of the first line of the docstring returned\n",
            "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
            "     |      for backwards compatibility, do not use for new subclasses.\n",
            "     |  shapes : str, optional\n",
            "     |      The shape of the distribution. For example \"m, n\" for a distribution\n",
            "     |      that takes two integers as the two shape arguments for all its methods\n",
            "     |      If not provided, shape parameters will be inferred from\n",
            "     |      the signatures of the private methods, ``_pmf`` and ``_cdf`` of\n",
            "     |      the instance.\n",
            "     |  seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n",
            "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "     |      singleton is used.\n",
            "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `seed`.\n",
            "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "     |      that instance is used.\n",
            "     |  \n",
            "     |  Methods\n",
            "     |  -------\n",
            "     |  rvs\n",
            "     |  pmf\n",
            "     |  logpmf\n",
            "     |  cdf\n",
            "     |  logcdf\n",
            "     |  sf\n",
            "     |  logsf\n",
            "     |  ppf\n",
            "     |  isf\n",
            "     |  moment\n",
            "     |  stats\n",
            "     |  entropy\n",
            "     |  expect\n",
            "     |  median\n",
            "     |  mean\n",
            "     |  std\n",
            "     |  var\n",
            "     |  interval\n",
            "     |  __call__\n",
            "     |  support\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  This class is similar to `rv_continuous`. Whether a shape parameter is\n",
            "     |  valid is decided by an ``_argcheck`` method (which defaults to checking\n",
            "     |  that its arguments are strictly positive.)\n",
            "     |  The main differences are as follows.\n",
            "     |  \n",
            "     |  - The support of the distribution is a set of integers.\n",
            "     |  - Instead of the probability density function, ``pdf`` (and the\n",
            "     |    corresponding private ``_pdf``), this class defines the\n",
            "     |    *probability mass function*, `pmf` (and the corresponding\n",
            "     |    private ``_pmf``.)\n",
            "     |  - There is no ``scale`` parameter.\n",
            "     |  - The default implementations of methods (e.g. ``_cdf``) are not designed\n",
            "     |    for distributions with support that is unbounded below (i.e.\n",
            "     |    ``a=-np.inf``), so they must be overridden.\n",
            "     |  \n",
            "     |  To create a new discrete distribution, we would do the following:\n",
            "     |  \n",
            "     |  >>> from scipy.stats import rv_discrete\n",
            "     |  >>> class poisson_gen(rv_discrete):\n",
            "     |  ...     \"Poisson distribution\"\n",
            "     |  ...     def _pmf(self, k, mu):\n",
            "     |  ...         return exp(-mu) * mu**k / factorial(k)\n",
            "     |  \n",
            "     |  and create an instance::\n",
            "     |  \n",
            "     |  >>> poisson = poisson_gen(name=\"poisson\")\n",
            "     |  \n",
            "     |  Note that above we defined the Poisson distribution in the standard form.\n",
            "     |  Shifting the distribution can be done by providing the ``loc`` parameter\n",
            "     |  to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``\n",
            "     |  delegates the work to ``poisson._pmf(x-loc, mu)``.\n",
            "     |  \n",
            "     |  **Discrete distributions from a list of probabilities**\n",
            "     |  \n",
            "     |  Alternatively, you can construct an arbitrary discrete rv defined\n",
            "     |  on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the\n",
            "     |  ``values`` keyword argument to the `rv_discrete` constructor.\n",
            "     |  \n",
            "     |  **Deepcopying / Pickling**\n",
            "     |  \n",
            "     |  If a distribution or frozen distribution is deepcopied (pickled/unpickled,\n",
            "     |  etc.), any underlying random number generator is deepcopied with it. An\n",
            "     |  implication is that if a distribution relies on the singleton RandomState\n",
            "     |  before copying, it will rely on a copy of that random state after copying,\n",
            "     |  and ``np.random.seed`` will no longer control the state.\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  Custom made discrete distribution:\n",
            "     |  \n",
            "     |  >>> import numpy as np\n",
            "     |  >>> from scipy import stats\n",
            "     |  >>> xk = np.arange(7)\n",
            "     |  >>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\n",
            "     |  >>> custm = stats.rv_discrete(name='custm', values=(xk, pk))\n",
            "     |  >>>\n",
            "     |  >>> import matplotlib.pyplot as plt\n",
            "     |  >>> fig, ax = plt.subplots(1, 1)\n",
            "     |  >>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')\n",
            "     |  >>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)\n",
            "     |  >>> plt.show()\n",
            "     |  \n",
            "     |  Random number generation:\n",
            "     |  \n",
            "     |  >>> R = custm.rvs(size=100)\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      rv_discrete\n",
            "     |      rv_generic\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  __init__(self, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, seed=None)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  cdf(self, k, *args, **kwds)\n",
            "     |      Cumulative distribution function of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like, int\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      cdf : ndarray\n",
            "     |          Cumulative distribution function evaluated at `k`.\n",
            "     |  \n",
            "     |  expect(self, func=None, args=(), loc=0, lb=None, ub=None, conditional=False, maxcount=1000, tolerance=1e-10, chunksize=32)\n",
            "     |      Calculate expected value of a function with respect to the distribution\n",
            "     |      for discrete distribution by numerical summation.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : callable, optional\n",
            "     |          Function for which the expectation value is calculated.\n",
            "     |          Takes only one argument.\n",
            "     |          The default is the identity mapping f(k) = k.\n",
            "     |      args : tuple, optional\n",
            "     |          Shape parameters of the distribution.\n",
            "     |      loc : float, optional\n",
            "     |          Location parameter.\n",
            "     |          Default is 0.\n",
            "     |      lb, ub : int, optional\n",
            "     |          Lower and upper bound for the summation, default is set to the\n",
            "     |          support of the distribution, inclusive (``lb <= k <= ub``).\n",
            "     |      conditional : bool, optional\n",
            "     |          If true then the expectation is corrected by the conditional\n",
            "     |          probability of the summation interval. The return value is the\n",
            "     |          expectation of the function, `func`, conditional on being in\n",
            "     |          the given interval (k such that ``lb <= k <= ub``).\n",
            "     |          Default is False.\n",
            "     |      maxcount : int, optional\n",
            "     |          Maximal number of terms to evaluate (to avoid an endless loop for\n",
            "     |          an infinite sum). Default is 1000.\n",
            "     |      tolerance : float, optional\n",
            "     |          Absolute tolerance for the summation. Default is 1e-10.\n",
            "     |      chunksize : int, optional\n",
            "     |          Iterate over the support of a distributions in chunks of this size.\n",
            "     |          Default is 32.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      expect : float\n",
            "     |          Expected value.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      For heavy-tailed distributions, the expected value may or\n",
            "     |      may not exist,\n",
            "     |      depending on the function, `func`. If it does exist, but the\n",
            "     |      sum converges\n",
            "     |      slowly, the accuracy of the result may be rather low. For instance, for\n",
            "     |      ``zipf(4)``, accuracy for mean, variance in example is only 1e-5.\n",
            "     |      increasing `maxcount` and/or `chunksize` may improve the result,\n",
            "     |      but may also make zipf very slow.\n",
            "     |      \n",
            "     |      The function is not vectorized.\n",
            "     |  \n",
            "     |  isf(self, q, *args, **kwds)\n",
            "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          Upper tail probability.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      k : ndarray or scalar\n",
            "     |          Quantile corresponding to the upper tail probability, q.\n",
            "     |  \n",
            "     |  logcdf(self, k, *args, **kwds)\n",
            "     |      Log of the cumulative distribution function at k of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like, int\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logcdf : array_like\n",
            "     |          Log of the cumulative distribution function evaluated at k.\n",
            "     |  \n",
            "     |  logpmf(self, k, *args, **kwds)\n",
            "     |      Log of the probability mass function at k of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter. Default is 0.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logpmf : array_like\n",
            "     |          Log of the probability mass function evaluated at k.\n",
            "     |  \n",
            "     |  logsf(self, k, *args, **kwds)\n",
            "     |      Log of the survival function of the given RV.\n",
            "     |      \n",
            "     |      Returns the log of the \"survival function,\" defined as 1 - `cdf`,\n",
            "     |      evaluated at `k`.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logsf : ndarray\n",
            "     |          Log of the survival function evaluated at `k`.\n",
            "     |  \n",
            "     |  pmf(self, k, *args, **kwds)\n",
            "     |      Probability mass function at k of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      pmf : array_like\n",
            "     |          Probability mass function evaluated at k\n",
            "     |  \n",
            "     |  ppf(self, q, *args, **kwds)\n",
            "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          Lower tail probability.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      k : array_like\n",
            "     |          Quantile corresponding to the lower tail probability, q.\n",
            "     |  \n",
            "     |  rvs(self, *args, **kwargs)\n",
            "     |      Random variates of given type.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      size : int or tuple of ints, optional\n",
            "     |          Defining number of random variates (Default is 1). Note that `size`\n",
            "     |          has to be given as keyword, not as positional argument.\n",
            "     |      random_state : {None, int, `numpy.random.Generator`,\n",
            "     |                      `numpy.random.RandomState`}, optional\n",
            "     |      \n",
            "     |          If `random_state` is None (or `np.random`), the\n",
            "     |          `numpy.random.RandomState` singleton is used.\n",
            "     |          If `random_state` is an int, a new ``RandomState`` instance is\n",
            "     |          used, seeded with `random_state`.\n",
            "     |          If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |          instance, that instance is used.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rvs : ndarray or scalar\n",
            "     |          Random variates of given `size`.\n",
            "     |  \n",
            "     |  sf(self, k, *args, **kwds)\n",
            "     |      Survival function (1 - `cdf`) at k of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      k : array_like\n",
            "     |          Quantiles.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      sf : array_like\n",
            "     |          Survival function evaluated at k.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Static methods defined here:\n",
            "     |  \n",
            "     |  __new__(cls, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, seed=None)\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from rv_generic:\n",
            "     |  \n",
            "     |  __call__(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  entropy(self, *args, **kwds)\n",
            "     |      Differential entropy of the RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : array_like, optional  (continuous distributions only).\n",
            "     |          Scale parameter (default=1).\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Entropy is defined base `e`:\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats._distn_infrastructure import rv_discrete\n",
            "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
            "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
            "     |      True\n",
            "     |  \n",
            "     |  freeze(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  interval(self, confidence, *args, **kwds)\n",
            "     |      Confidence interval with equal areas around the median.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      confidence : array_like of float\n",
            "     |          Probability that an rv will be drawn from the returned range.\n",
            "     |          Each value should be in the range [0, 1].\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : ndarray of float\n",
            "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
            "     |          possible values.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is implemented as ``ppf([p_tail, 1-p_tail])``, where\n",
            "     |      ``ppf`` is the inverse cumulative distribution function and\n",
            "     |      ``p_tail = (1-confidence)/2``. Suppose ``[c, d]`` is the support of a\n",
            "     |      discrete distribution; then ``ppf([0, 1]) == (c-1, d)``. Therefore,\n",
            "     |      when ``confidence=1`` and the distribution is discrete, the left end\n",
            "     |      of the interval will be beyond the support of the distribution.\n",
            "     |      For discrete distributions, the interval will limit the probability\n",
            "     |      in each tail to be less than or equal to ``p_tail`` (usually\n",
            "     |      strictly less).\n",
            "     |  \n",
            "     |  mean(self, *args, **kwds)\n",
            "     |      Mean of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      mean : float\n",
            "     |          the mean of the distribution\n",
            "     |  \n",
            "     |  median(self, *args, **kwds)\n",
            "     |      Median of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          Scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      median : float\n",
            "     |          The median of the distribution.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      rv_discrete.ppf\n",
            "     |          Inverse of the CDF\n",
            "     |  \n",
            "     |  moment(self, order, *args, **kwds)\n",
            "     |      non-central moment of distribution of specified order.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      order : int, order >= 1\n",
            "     |          Order of moment.\n",
            "     |      arg1, arg2, arg3,... : float\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |  \n",
            "     |  nnlf(self, theta, x)\n",
            "     |      Negative loglikelihood function.\n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
            "     |      parameters (including loc and scale).\n",
            "     |  \n",
            "     |  stats(self, *args, **kwds)\n",
            "     |      Some statistics of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional (continuous RVs only)\n",
            "     |          scale parameter (default=1)\n",
            "     |      moments : str, optional\n",
            "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
            "     |          'm' = mean,\n",
            "     |          'v' = variance,\n",
            "     |          's' = (Fisher's) skew,\n",
            "     |          'k' = (Fisher's) kurtosis.\n",
            "     |          (default is 'mv')\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      stats : sequence\n",
            "     |          of requested moments.\n",
            "     |  \n",
            "     |  std(self, *args, **kwds)\n",
            "     |      Standard deviation of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      std : float\n",
            "     |          standard deviation of the distribution\n",
            "     |  \n",
            "     |  support(self, *args, **kwargs)\n",
            "     |      Support of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : array_like\n",
            "     |          end-points of the distribution's support.\n",
            "     |  \n",
            "     |  var(self, *args, **kwds)\n",
            "     |      Variance of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      var : float\n",
            "     |          the variance of the distribution\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from rv_generic:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  random_state\n",
            "     |      Get or set the generator object for generating random variates.\n",
            "     |      \n",
            "     |      If `random_state` is None (or `np.random`), the\n",
            "     |      `numpy.random.RandomState` singleton is used.\n",
            "     |      If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `random_state`.\n",
            "     |      If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |      instance, that instance is used.\n",
            "    \n",
            "    class rv_histogram(scipy.stats._distn_infrastructure.rv_continuous)\n",
            "     |  rv_histogram(histogram, *args, density=None, **kwargs)\n",
            "     |  \n",
            "     |  Generates a distribution given by a histogram.\n",
            "     |  This is useful to generate a template distribution from a binned\n",
            "     |  datasample.\n",
            "     |  \n",
            "     |  As a subclass of the `rv_continuous` class, `rv_histogram` inherits from it\n",
            "     |  a collection of generic methods (see `rv_continuous` for the full list),\n",
            "     |  and implements them based on the properties of the provided binned\n",
            "     |  datasample.\n",
            "     |  \n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  histogram : tuple of array_like\n",
            "     |      Tuple containing two array_like objects.\n",
            "     |      The first containing the content of n bins,\n",
            "     |      the second containing the (n+1) bin boundaries.\n",
            "     |      In particular, the return value of `numpy.histogram` is accepted.\n",
            "     |  \n",
            "     |  density : bool, optional\n",
            "     |      If False, assumes the histogram is proportional to counts per bin;\n",
            "     |      otherwise, assumes it is proportional to a density.\n",
            "     |      For constant bin widths, these are equivalent, but the distinction\n",
            "     |      is important when bin widths vary (see Notes).\n",
            "     |      If None (default), sets ``density=True`` for backwards compatibility,\n",
            "     |      but warns if the bin widths are variable. Set `density` explicitly\n",
            "     |      to silence the warning.\n",
            "     |  \n",
            "     |      .. versionadded:: 1.10.0\n",
            "     |  \n",
            "     |  Notes\n",
            "     |  -----\n",
            "     |  When a histogram has unequal bin widths, there is a distinction between\n",
            "     |  histograms that are proportional to counts per bin and histograms that are\n",
            "     |  proportional to probability density over a bin. If `numpy.histogram` is\n",
            "     |  called with its default ``density=False``, the resulting histogram is the\n",
            "     |  number of counts per bin, so ``density=False`` should be passed to\n",
            "     |  `rv_histogram`. If `numpy.histogram` is called with ``density=True``, the\n",
            "     |  resulting histogram is in terms of probability density, so ``density=True``\n",
            "     |  should be passed to `rv_histogram`. To avoid warnings, always pass\n",
            "     |  ``density`` explicitly when the input histogram has unequal bin widths.\n",
            "     |  \n",
            "     |  There are no additional shape parameters except for the loc and scale.\n",
            "     |  The pdf is defined as a stepwise function from the provided histogram.\n",
            "     |  The cdf is a linear interpolation of the pdf.\n",
            "     |  \n",
            "     |  .. versionadded:: 0.19.0\n",
            "     |  \n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |  \n",
            "     |  Create a scipy.stats distribution from a numpy histogram\n",
            "     |  \n",
            "     |  >>> import scipy.stats\n",
            "     |  >>> import numpy as np\n",
            "     |  >>> data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5,\n",
            "     |  ...                             random_state=123)\n",
            "     |  >>> hist = np.histogram(data, bins=100)\n",
            "     |  >>> hist_dist = scipy.stats.rv_histogram(hist, density=False)\n",
            "     |  \n",
            "     |  Behaves like an ordinary scipy rv_continuous distribution\n",
            "     |  \n",
            "     |  >>> hist_dist.pdf(1.0)\n",
            "     |  0.20538577847618705\n",
            "     |  >>> hist_dist.cdf(2.0)\n",
            "     |  0.90818568543056499\n",
            "     |  \n",
            "     |  PDF is zero above (below) the highest (lowest) bin of the histogram,\n",
            "     |  defined by the max (min) of the original dataset\n",
            "     |  \n",
            "     |  >>> hist_dist.pdf(np.max(data))\n",
            "     |  0.0\n",
            "     |  >>> hist_dist.cdf(np.max(data))\n",
            "     |  1.0\n",
            "     |  >>> hist_dist.pdf(np.min(data))\n",
            "     |  7.7591907244498314e-05\n",
            "     |  >>> hist_dist.cdf(np.min(data))\n",
            "     |  0.0\n",
            "     |  \n",
            "     |  PDF and CDF follow the histogram\n",
            "     |  \n",
            "     |  >>> import matplotlib.pyplot as plt\n",
            "     |  >>> X = np.linspace(-5.0, 5.0, 100)\n",
            "     |  >>> fig, ax = plt.subplots()\n",
            "     |  >>> ax.set_title(\"PDF from Template\")\n",
            "     |  >>> ax.hist(data, density=True, bins=100)\n",
            "     |  >>> ax.plot(X, hist_dist.pdf(X), label='PDF')\n",
            "     |  >>> ax.plot(X, hist_dist.cdf(X), label='CDF')\n",
            "     |  >>> ax.legend()\n",
            "     |  >>> fig.show()\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      rv_histogram\n",
            "     |      scipy.stats._distn_infrastructure.rv_continuous\n",
            "     |      scipy.stats._distn_infrastructure.rv_generic\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, histogram, *args, density=None, **kwargs)\n",
            "     |      Create a new distribution using the given histogram\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      histogram : tuple of array_like\n",
            "     |          Tuple containing two array_like objects.\n",
            "     |          The first containing the content of n bins,\n",
            "     |          the second containing the (n+1) bin boundaries.\n",
            "     |          In particular, the return value of np.histogram is accepted.\n",
            "     |      density : bool, optional\n",
            "     |          If False, assumes the histogram is proportional to counts per bin;\n",
            "     |          otherwise, assumes it is proportional to a density.\n",
            "     |          For constant bin widths, these are equivalent.\n",
            "     |          If None (default), sets ``density=True`` for backward\n",
            "     |          compatibility, but warns if the bin widths are variable. Set\n",
            "     |          `density` explicitly to silence the warning.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_continuous:\n",
            "     |  \n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |  \n",
            "     |  cdf(self, x, *args, **kwds)\n",
            "     |      Cumulative distribution function of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      cdf : ndarray\n",
            "     |          Cumulative distribution function evaluated at `x`\n",
            "     |  \n",
            "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "     |      Calculate expected value of a function with respect to the\n",
            "     |      distribution by numerical integration.\n",
            "     |      \n",
            "     |      The expected value of a function ``f(x)`` with respect to a\n",
            "     |      distribution ``dist`` is defined as::\n",
            "     |      \n",
            "     |                  ub\n",
            "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
            "     |                  lb\n",
            "     |      \n",
            "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
            "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
            "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
            "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
            "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
            "     |      outside a finite interval in which case the expectation is\n",
            "     |      calculated within the finite range ``[lb, ub]``.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      func : callable, optional\n",
            "     |          Function for which integral is calculated. Takes only one argument.\n",
            "     |          The default is the identity mapping f(x) = x.\n",
            "     |      args : tuple, optional\n",
            "     |          Shape parameters of the distribution.\n",
            "     |      loc : float, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : float, optional\n",
            "     |          Scale parameter (default=1).\n",
            "     |      lb, ub : scalar, optional\n",
            "     |          Lower and upper bound for integration. Default is set to the\n",
            "     |          support of the distribution.\n",
            "     |      conditional : bool, optional\n",
            "     |          If True, the integral is corrected by the conditional probability\n",
            "     |          of the integration interval.  The return value is the expectation\n",
            "     |          of the function, conditional on being in the given interval.\n",
            "     |          Default is False.\n",
            "     |      \n",
            "     |      Additional keyword arguments are passed to the integration routine.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      expect : float\n",
            "     |          The calculated expected value.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The integration behavior of this function is inherited from\n",
            "     |      `scipy.integrate.quad`. Neither this function nor\n",
            "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
            "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
            "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
            "     |      \n",
            "     |      Likewise, the accuracy of results is not verified by the function.\n",
            "     |      `scipy.integrate.quad` is typically reliable for integrals that are\n",
            "     |      numerically favorable, but it is not guaranteed to converge\n",
            "     |      to a correct value for all possible intervals and integrands. This\n",
            "     |      function is provided for convenience; for critical applications,\n",
            "     |      check results against other integration methods.\n",
            "     |      \n",
            "     |      The function is not vectorized.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      \n",
            "     |      To understand the effect of the bounds of integration consider\n",
            "     |      \n",
            "     |      >>> from scipy.stats import expon\n",
            "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
            "     |      0.6321205588285578\n",
            "     |      \n",
            "     |      This is close to\n",
            "     |      \n",
            "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
            "     |      0.6321205588285577\n",
            "     |      \n",
            "     |      If ``conditional=True``\n",
            "     |      \n",
            "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
            "     |      1.0000000000000002\n",
            "     |      \n",
            "     |      The slight deviation from 1 is due to numerical integration.\n",
            "     |      \n",
            "     |      The integrand can be treated as a complex-valued function\n",
            "     |      by passing ``complex_func=True`` to `scipy.integrate.quad` .\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats import vonmises\n",
            "     |      >>> res = vonmises(loc=2, kappa=1).expect(lambda x: np.exp(1j*x),\n",
            "     |      ...                                       complex_func=True)\n",
            "     |      >>> res\n",
            "     |      (-0.18576377217422957+0.40590124735052263j)\n",
            "     |      \n",
            "     |      >>> np.angle(res)  # location of the (circular) distribution\n",
            "     |      2.0\n",
            "     |  \n",
            "     |  fit(self, data, *args, **kwds)\n",
            "     |      Return estimates of shape (if applicable), location, and scale\n",
            "     |      parameters from data. The default estimation method is Maximum\n",
            "     |      Likelihood Estimation (MLE), but Method of Moments (MM)\n",
            "     |      is also available.\n",
            "     |      \n",
            "     |      Starting estimates for the fit are given by input arguments;\n",
            "     |      for any arguments not provided with starting estimates,\n",
            "     |      ``self._fitstart(data)`` is called to generate such.\n",
            "     |      \n",
            "     |      One can hold some parameters fixed to specific values by passing in\n",
            "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
            "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
            "     |      respectively).\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data : array_like or `CensoredData` instance\n",
            "     |          Data to use in estimating the distribution parameters.\n",
            "     |      arg1, arg2, arg3,... : floats, optional\n",
            "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
            "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
            "     |          No default value.\n",
            "     |      **kwds : floats, optional\n",
            "     |          - `loc`: initial guess of the distribution's location parameter.\n",
            "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
            "     |      \n",
            "     |          Special keyword arguments are recognized as holding certain\n",
            "     |          parameters fixed:\n",
            "     |      \n",
            "     |          - f0...fn : hold respective shape parameters fixed.\n",
            "     |            Alternatively, shape parameters to fix can be specified by name.\n",
            "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
            "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
            "     |            equivalent to ``f1``.\n",
            "     |      \n",
            "     |          - floc : hold location parameter fixed to specified value.\n",
            "     |      \n",
            "     |          - fscale : hold scale parameter fixed to specified value.\n",
            "     |      \n",
            "     |          - optimizer : The optimizer to use.  The optimizer must take\n",
            "     |            ``func`` and starting position as the first two arguments,\n",
            "     |            plus ``args`` (for extra arguments to pass to the\n",
            "     |            function to be optimized) and ``disp``. \n",
            "     |            The ``fit`` method calls the optimizer with ``disp=0`` to suppress output.\n",
            "     |            The optimizer must return the estimated parameters.\n",
            "     |      \n",
            "     |          - method : The method to use. The default is \"MLE\" (Maximum\n",
            "     |            Likelihood Estimate); \"MM\" (Method of Moments)\n",
            "     |            is also available.\n",
            "     |      \n",
            "     |      Raises\n",
            "     |      ------\n",
            "     |      TypeError, ValueError\n",
            "     |          If an input is invalid\n",
            "     |      `~scipy.stats.FitError`\n",
            "     |          If fitting fails or the fit produced would be invalid\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      parameter_tuple : tuple of floats\n",
            "     |          Estimates for any shape parameters (if applicable), followed by\n",
            "     |          those for location and scale. For most random variables, shape\n",
            "     |          statistics will be returned, but there are exceptions (e.g.\n",
            "     |          ``norm``).\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      With ``method=\"MLE\"`` (default), the fit is computed by minimizing\n",
            "     |      the negative log-likelihood function. A large, finite penalty\n",
            "     |      (rather than infinite negative log-likelihood) is applied for\n",
            "     |      observations beyond the support of the distribution.\n",
            "     |      \n",
            "     |      With ``method=\"MM\"``, the fit is computed by minimizing the L2 norm\n",
            "     |      of the relative errors between the first *k* raw (about zero) data\n",
            "     |      moments and the corresponding distribution moments, where *k* is the\n",
            "     |      number of non-fixed parameters.\n",
            "     |      More precisely, the objective function is::\n",
            "     |      \n",
            "     |          (((data_moments - dist_moments)\n",
            "     |            / np.maximum(np.abs(data_moments), 1e-8))**2).sum()\n",
            "     |      \n",
            "     |      where the constant ``1e-8`` avoids division by zero in case of\n",
            "     |      vanishing data moments. Typically, this error norm can be reduced to\n",
            "     |      zero.\n",
            "     |      Note that the standard method of moments can produce parameters for\n",
            "     |      which some data are outside the support of the fitted distribution;\n",
            "     |      this implementation does nothing to prevent this.\n",
            "     |      \n",
            "     |      For either method,\n",
            "     |      the returned answer is not guaranteed to be globally optimal; it\n",
            "     |      may only be locally optimal, or the optimization may fail altogether.\n",
            "     |      If the data contain any of ``np.nan``, ``np.inf``, or ``-np.inf``,\n",
            "     |      the `fit` method will raise a ``RuntimeError``.\n",
            "     |      \n",
            "     |      When passing a ``CensoredData`` instance to ``data``, the log-likelihood\n",
            "     |      function is defined as:\n",
            "     |      \n",
            "     |      .. math::\n",
            "     |      \n",
            "     |          l(\\pmb{\\theta}; k) & = \\sum\n",
            "     |                                  \\log(f(k_u; \\pmb{\\theta}))\n",
            "     |                              + \\sum\n",
            "     |                                  \\log(F(k_l; \\pmb{\\theta})) \\\\\n",
            "     |                              & + \\sum \n",
            "     |                                  \\log(1 - F(k_r; \\pmb{\\theta})) \\\\\n",
            "     |                              & + \\sum\n",
            "     |                                  \\log(F(k_{\\text{high}, i}; \\pmb{\\theta})\n",
            "     |                                  - F(k_{\\text{low}, i}; \\pmb{\\theta}))\n",
            "     |      \n",
            "     |      where :math:`f` and :math:`F` are the pdf and cdf, respectively, of the\n",
            "     |      function being fitted, :math:`\\pmb{\\theta}` is the parameter vector,\n",
            "     |      :math:`u` are the indices of uncensored observations,\n",
            "     |      :math:`l` are the indices of left-censored observations,\n",
            "     |      :math:`r` are the indices of right-censored observations,\n",
            "     |      subscripts \"low\"/\"high\" denote endpoints of interval-censored observations, and\n",
            "     |      :math:`i` are the indices of interval-censored observations.\n",
            "     |      \n",
            "     |      Examples\n",
            "     |      --------\n",
            "     |      \n",
            "     |      Generate some data to fit: draw random variates from the `beta`\n",
            "     |      distribution\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats import beta\n",
            "     |      >>> a, b = 1., 2.\n",
            "     |      >>> rng = np.random.default_rng(172786373191770012695001057628748821561)\n",
            "     |      >>> x = beta.rvs(a, b, size=1000, random_state=rng)\n",
            "     |      \n",
            "     |      Now we can fit all four parameters (``a``, ``b``, ``loc`` and\n",
            "     |      ``scale``):\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
            "     |      >>> a1, b1, loc1, scale1\n",
            "     |      (1.0198945204435628, 1.9484708982737828, 4.372241314917588e-05, 0.9979078845964814)\n",
            "     |      \n",
            "     |      The fit can be done also using a custom optimizer:\n",
            "     |      \n",
            "     |      >>> from scipy.optimize import minimize\n",
            "     |      >>> def custom_optimizer(func, x0, args=(), disp=0):\n",
            "     |      ...     res = minimize(func, x0, args, method=\"slsqp\", options={\"disp\": disp})\n",
            "     |      ...     if res.success:\n",
            "     |      ...         return res.x\n",
            "     |      ...     raise RuntimeError('optimization routine failed')\n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, method=\"MLE\", optimizer=custom_optimizer)\n",
            "     |      >>> a1, b1, loc1, scale1\n",
            "     |      (1.0198821087258905, 1.948484145914738, 4.3705304486881485e-05, 0.9979104663953395)\n",
            "     |      \n",
            "     |      We can also use some prior knowledge about the dataset: let's keep\n",
            "     |      ``loc`` and ``scale`` fixed:\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
            "     |      >>> loc1, scale1\n",
            "     |      (0, 1)\n",
            "     |      \n",
            "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
            "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
            "     |      equivalently, ``fa=1``:\n",
            "     |      \n",
            "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
            "     |      >>> a1\n",
            "     |      1\n",
            "     |      \n",
            "     |      Not all distributions return estimates for the shape parameters.\n",
            "     |      ``norm`` for example just returns estimates for location and scale:\n",
            "     |      \n",
            "     |      >>> from scipy.stats import norm\n",
            "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
            "     |      >>> loc1, scale1 = norm.fit(x)\n",
            "     |      >>> loc1, scale1\n",
            "     |      (0.92087172783841631, 2.0015750750324668)\n",
            "     |  \n",
            "     |  fit_loc_scale(self, data, *args)\n",
            "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data : array_like\n",
            "     |          Data to fit.\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      Lhat : float\n",
            "     |          Estimated location parameter for the data.\n",
            "     |      Shat : float\n",
            "     |          Estimated scale parameter for the data.\n",
            "     |  \n",
            "     |  isf(self, q, *args, **kwds)\n",
            "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          upper tail probability\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x : ndarray or scalar\n",
            "     |          Quantile corresponding to the upper tail probability q.\n",
            "     |  \n",
            "     |  logcdf(self, x, *args, **kwds)\n",
            "     |      Log of the cumulative distribution function at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logcdf : array_like\n",
            "     |          Log of the cumulative distribution function evaluated at x\n",
            "     |  \n",
            "     |  logpdf(self, x, *args, **kwds)\n",
            "     |      Log of the probability density function at x of the given RV.\n",
            "     |      \n",
            "     |      This uses a more numerically accurate calculation if available.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logpdf : array_like\n",
            "     |          Log of the probability density function evaluated at x\n",
            "     |  \n",
            "     |  logsf(self, x, *args, **kwds)\n",
            "     |      Log of the survival function of the given RV.\n",
            "     |      \n",
            "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
            "     |      evaluated at `x`.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      logsf : ndarray\n",
            "     |          Log of the survival function evaluated at `x`.\n",
            "     |  \n",
            "     |  pdf(self, x, *args, **kwds)\n",
            "     |      Probability density function at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      pdf : ndarray\n",
            "     |          Probability density function evaluated at x\n",
            "     |  \n",
            "     |  ppf(self, q, *args, **kwds)\n",
            "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      q : array_like\n",
            "     |          lower tail probability\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      x : array_like\n",
            "     |          quantile corresponding to the lower tail probability q.\n",
            "     |  \n",
            "     |  sf(self, x, *args, **kwds)\n",
            "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      x : array_like\n",
            "     |          quantiles\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      sf : array_like\n",
            "     |          Survival function evaluated at x\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
            "     |  \n",
            "     |  __call__(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  __setstate__(self, state)\n",
            "     |  \n",
            "     |  entropy(self, *args, **kwds)\n",
            "     |      Differential entropy of the RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : array_like, optional  (continuous distributions only).\n",
            "     |          Scale parameter (default=1).\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      Entropy is defined base `e`:\n",
            "     |      \n",
            "     |      >>> import numpy as np\n",
            "     |      >>> from scipy.stats._distn_infrastructure import rv_discrete\n",
            "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
            "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
            "     |      True\n",
            "     |  \n",
            "     |  freeze(self, *args, **kwds)\n",
            "     |      Freeze the distribution for the given arguments.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution.  Should include all\n",
            "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rv_frozen : rv_frozen instance\n",
            "     |          The frozen distribution.\n",
            "     |  \n",
            "     |  interval(self, confidence, *args, **kwds)\n",
            "     |      Confidence interval with equal areas around the median.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      confidence : array_like of float\n",
            "     |          Probability that an rv will be drawn from the returned range.\n",
            "     |          Each value should be in the range [0, 1].\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : ndarray of float\n",
            "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
            "     |          possible values.\n",
            "     |      \n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is implemented as ``ppf([p_tail, 1-p_tail])``, where\n",
            "     |      ``ppf`` is the inverse cumulative distribution function and\n",
            "     |      ``p_tail = (1-confidence)/2``. Suppose ``[c, d]`` is the support of a\n",
            "     |      discrete distribution; then ``ppf([0, 1]) == (c-1, d)``. Therefore,\n",
            "     |      when ``confidence=1`` and the distribution is discrete, the left end\n",
            "     |      of the interval will be beyond the support of the distribution.\n",
            "     |      For discrete distributions, the interval will limit the probability\n",
            "     |      in each tail to be less than or equal to ``p_tail`` (usually\n",
            "     |      strictly less).\n",
            "     |  \n",
            "     |  mean(self, *args, **kwds)\n",
            "     |      Mean of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      mean : float\n",
            "     |          the mean of the distribution\n",
            "     |  \n",
            "     |  median(self, *args, **kwds)\n",
            "     |      Median of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          Scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      median : float\n",
            "     |          The median of the distribution.\n",
            "     |      \n",
            "     |      See Also\n",
            "     |      --------\n",
            "     |      rv_discrete.ppf\n",
            "     |          Inverse of the CDF\n",
            "     |  \n",
            "     |  moment(self, order, *args, **kwds)\n",
            "     |      non-central moment of distribution of specified order.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      order : int, order >= 1\n",
            "     |          Order of moment.\n",
            "     |      arg1, arg2, arg3,... : float\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |  \n",
            "     |  nnlf(self, theta, x)\n",
            "     |      Negative loglikelihood function.\n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
            "     |      parameters (including loc and scale).\n",
            "     |  \n",
            "     |  rvs(self, *args, **kwds)\n",
            "     |      Random variates of given type.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          Location parameter (default=0).\n",
            "     |      scale : array_like, optional\n",
            "     |          Scale parameter (default=1).\n",
            "     |      size : int or tuple of ints, optional\n",
            "     |          Defining number of random variates (default is 1).\n",
            "     |      random_state : {None, int, `numpy.random.Generator`,\n",
            "     |                      `numpy.random.RandomState`}, optional\n",
            "     |      \n",
            "     |          If `random_state` is None (or `np.random`), the\n",
            "     |          `numpy.random.RandomState` singleton is used.\n",
            "     |          If `random_state` is an int, a new ``RandomState`` instance is\n",
            "     |          used, seeded with `random_state`.\n",
            "     |          If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |          instance, that instance is used.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      rvs : ndarray or scalar\n",
            "     |          Random variates of given `size`.\n",
            "     |  \n",
            "     |  stats(self, *args, **kwds)\n",
            "     |      Some statistics of the given RV.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional (continuous RVs only)\n",
            "     |          scale parameter (default=1)\n",
            "     |      moments : str, optional\n",
            "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
            "     |          'm' = mean,\n",
            "     |          'v' = variance,\n",
            "     |          's' = (Fisher's) skew,\n",
            "     |          'k' = (Fisher's) kurtosis.\n",
            "     |          (default is 'mv')\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      stats : sequence\n",
            "     |          of requested moments.\n",
            "     |  \n",
            "     |  std(self, *args, **kwds)\n",
            "     |      Standard deviation of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      std : float\n",
            "     |          standard deviation of the distribution\n",
            "     |  \n",
            "     |  support(self, *args, **kwargs)\n",
            "     |      Support of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, ... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information).\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter, Default is 0.\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter, Default is 1.\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a, b : array_like\n",
            "     |          end-points of the distribution's support.\n",
            "     |  \n",
            "     |  var(self, *args, **kwds)\n",
            "     |      Variance of the distribution.\n",
            "     |      \n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      arg1, arg2, arg3,... : array_like\n",
            "     |          The shape parameter(s) for the distribution (see docstring of the\n",
            "     |          instance object for more information)\n",
            "     |      loc : array_like, optional\n",
            "     |          location parameter (default=0)\n",
            "     |      scale : array_like, optional\n",
            "     |          scale parameter (default=1)\n",
            "     |      \n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      var : float\n",
            "     |          the variance of the distribution\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |  \n",
            "     |  random_state\n",
            "     |      Get or set the generator object for generating random variates.\n",
            "     |      \n",
            "     |      If `random_state` is None (or `np.random`), the\n",
            "     |      `numpy.random.RandomState` singleton is used.\n",
            "     |      If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "     |      seeded with `random_state`.\n",
            "     |      If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "     |      instance, that instance is used.\n",
            "\n",
            "FUNCTIONS\n",
            "    alexandergovern(*samples, nan_policy='propagate', axis=0, keepdims=False)\n",
            "        Performs the Alexander Govern test.\n",
            "        \n",
            "        The Alexander-Govern approximation tests the equality of k independent\n",
            "        means in the face of heterogeneity of variance. The test is applied to\n",
            "        samples from two or more groups, possibly with differing sizes.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            The sample measurements for each group.  There must be at least\n",
            "            two samples.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : AlexanderGovernResult\n",
            "            An object with attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                The computed A statistic of the test.\n",
            "            pvalue : float\n",
            "                The associated p-value from the chi-squared distribution.\n",
            "        \n",
            "        Warns\n",
            "        -----\n",
            "        `~scipy.stats.ConstantInputWarning`\n",
            "            Raised if an input is a constant array.  The statistic is not defined\n",
            "            in this case, so ``np.nan`` is returned.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`f_oneway`\n",
            "            one-way ANOVA\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The use of this test relies on several assumptions.\n",
            "        \n",
            "        1. The samples are independent.\n",
            "        2. Each sample is from a normally distributed population.\n",
            "        3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n",
            "           instead relaxing the assumption of equal variances.\n",
            "        \n",
            "        Input samples must be finite, one dimensional, and with size greater than\n",
            "        one.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n",
            "               Approximation for ANOVA under Variance Heterogeneity.\" Journal\n",
            "               of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n",
            "               JSTOR, www.jstor.org/stable/1165140. Accessed 12 Sept. 2020.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import alexandergovern\n",
            "        \n",
            "        Here are some data on annual percentage rate of interest charged on\n",
            "        new car loans at nine of the largest banks in four American cities\n",
            "        taken from the National Institute of Standards and Technology's\n",
            "        ANOVA dataset.\n",
            "        \n",
            "        We use `alexandergovern` to test the null hypothesis that all cities\n",
            "        have the same mean APR against the alternative that the cities do not\n",
            "        all have the same mean APR. We decide that a significance level of 5%\n",
            "        is required to reject the null hypothesis in favor of the alternative.\n",
            "        \n",
            "        >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n",
            "        >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n",
            "        >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n",
            "        >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n",
            "        ...           11.89]\n",
            "        >>> alexandergovern(atlanta, chicago, houston, memphis)\n",
            "        AlexanderGovernResult(statistic=4.65087071883494,\n",
            "                              pvalue=0.19922132490385214)\n",
            "        \n",
            "        The p-value is 0.1992, indicating a nearly 20% chance of observing\n",
            "        such an extreme value of the test statistic under the null hypothesis.\n",
            "        This exceeds 5%, so we do not reject the null hypothesis in favor of\n",
            "        the alternative.\n",
            "    \n",
            "    anderson(x, dist='norm')\n",
            "        Anderson-Darling test for data coming from a particular distribution.\n",
            "        \n",
            "        The Anderson-Darling test tests the null hypothesis that a sample is\n",
            "        drawn from a population that follows a particular distribution.\n",
            "        For the Anderson-Darling test, the critical values depend on\n",
            "        which distribution is being tested against.  This function works\n",
            "        for normal, exponential, logistic, weibull_min, or Gumbel (Extreme Value\n",
            "        Type I) distributions.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Array of sample data.\n",
            "        dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1', 'weibull_min'}, optional\n",
            "            The type of distribution to test against.  The default is 'norm'.\n",
            "            The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n",
            "            same distribution.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : AndersonResult\n",
            "            An object with the following attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                The Anderson-Darling test statistic.\n",
            "            critical_values : list\n",
            "                The critical values for this distribution.\n",
            "            significance_level : list\n",
            "                The significance levels for the corresponding critical values\n",
            "                in percents.  The function returns critical values for a\n",
            "                differing set of significance levels depending on the\n",
            "                distribution that is being tested against.\n",
            "            fit_result : `~scipy.stats._result_classes.FitResult`\n",
            "                An object containing the results of fitting the distribution to\n",
            "                the data.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Critical values provided are for the following significance levels:\n",
            "        \n",
            "        normal/exponential\n",
            "            15%, 10%, 5%, 2.5%, 1%\n",
            "        logistic\n",
            "            25%, 10%, 5%, 2.5%, 1%, 0.5%\n",
            "        gumbel_l / gumbel_r\n",
            "            25%, 10%, 5%, 2.5%, 1%\n",
            "        weibull_min\n",
            "            50%, 25%, 15%, 10%, 5%, 2.5%, 1%, 0.5%\n",
            "        \n",
            "        If the returned statistic is larger than these critical values then\n",
            "        for the corresponding significance level, the null hypothesis that\n",
            "        the data come from the chosen distribution can be rejected.\n",
            "        The returned statistic is referred to as 'A2' in the references.\n",
            "        \n",
            "        For `weibull_min`, maximum likelihood estimation is known to be\n",
            "        challenging. If the test returns successfully, then the first order\n",
            "        conditions for a maximum likehood estimate have been verified and\n",
            "        the critical values correspond relatively well to the significance levels,\n",
            "        provided that the sample is sufficiently large (>10 observations [7]).\n",
            "        However, for some data - especially data with no left tail - `anderson`\n",
            "        is likely to result in an error message. In this case, consider\n",
            "        performing a custom goodness of fit test using\n",
            "        `scipy.stats.monte_carlo_test`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
            "        .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n",
            "               Some Comparisons, Journal of the American Statistical Association,\n",
            "               Vol. 69, pp. 730-737.\n",
            "        .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n",
            "               Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n",
            "               pp. 357-369.\n",
            "        .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n",
            "               Distribution, Biometrika, Vol. 64, pp. 583-588.\n",
            "        .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n",
            "               to Tests for Exponentiality , Technical Report No. 262,\n",
            "               Department of Statistics, Stanford University, Stanford, CA.\n",
            "        .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n",
            "               Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n",
            "               pp. 591-595.\n",
            "        .. [7] Richard A. Lockhart and Michael A. Stephens \"Estimation and Tests of\n",
            "               Fit for the Three-Parameter Weibull Distribution\"\n",
            "               Journal of the Royal Statistical Society.Series B(Methodological)\n",
            "               Vol. 56, No. 3 (1994), pp. 491-500, Table 0.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Test the null hypothesis that a random sample was drawn from a normal\n",
            "        distribution (with unspecified mean and standard deviation).\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import anderson\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> data = rng.random(size=35)\n",
            "        >>> res = anderson(data)\n",
            "        >>> res.statistic\n",
            "        0.8398018749744764\n",
            "        >>> res.critical_values\n",
            "        array([0.527, 0.6  , 0.719, 0.839, 0.998])\n",
            "        >>> res.significance_level\n",
            "        array([15. , 10. ,  5. ,  2.5,  1. ])\n",
            "        \n",
            "        The value of the statistic (barely) exceeds the critical value associated\n",
            "        with a significance level of 2.5%, so the null hypothesis may be rejected\n",
            "        at a significance level of 2.5%, but not at a significance level of 1%.\n",
            "    \n",
            "    anderson_ksamp(samples, midrank=True, *, method=None)\n",
            "        The Anderson-Darling test for k-samples.\n",
            "        \n",
            "        The k-sample Anderson-Darling test is a modification of the\n",
            "        one-sample Anderson-Darling test. It tests the null hypothesis\n",
            "        that k-samples are drawn from the same population without having\n",
            "        to specify the distribution function of that population. The\n",
            "        critical values depend on the number of samples.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        samples : sequence of 1-D array_like\n",
            "            Array of sample data in arrays.\n",
            "        midrank : bool, optional\n",
            "            Type of Anderson-Darling test which is computed. Default\n",
            "            (True) is the midrank test applicable to continuous and\n",
            "            discrete populations. If False, the right side empirical\n",
            "            distribution is used.\n",
            "        method : PermutationMethod, optional\n",
            "            Defines the method used to compute the p-value. If `method` is an\n",
            "            instance of `PermutationMethod`, the p-value is computed using\n",
            "            `scipy.stats.permutation_test` with the provided configuration options\n",
            "            and other appropriate settings. Otherwise, the p-value is interpolated\n",
            "            from tabulated values.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : Anderson_ksampResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                Normalized k-sample Anderson-Darling test statistic.\n",
            "            critical_values : array\n",
            "                The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n",
            "                0.5%, 0.1%.\n",
            "            pvalue : float\n",
            "                The approximate p-value of the test. If `method` is not\n",
            "                provided, the value is floored / capped at 0.1% / 25%.\n",
            "        \n",
            "        Raises\n",
            "        ------\n",
            "        ValueError\n",
            "            If fewer than 2 samples are provided, a sample is empty, or no\n",
            "            distinct observations are in the samples.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ks_2samp : 2 sample Kolmogorov-Smirnov test\n",
            "        anderson : 1 sample Anderson-Darling test\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        [1]_ defines three versions of the k-sample Anderson-Darling test:\n",
            "        one for continuous distributions and two for discrete\n",
            "        distributions, in which ties between samples may occur. The\n",
            "        default of this routine is to compute the version based on the\n",
            "        midrank empirical distribution function. This test is applicable\n",
            "        to continuous and discrete data. If midrank is set to False, the\n",
            "        right side empirical distribution is used for a test for discrete\n",
            "        data. According to [1]_, the two discrete test statistics differ\n",
            "        only slightly if a few collisions due to round-off errors occur in\n",
            "        the test not adjusted for ties between samples.\n",
            "        \n",
            "        The critical values corresponding to the significance levels from 0.01\n",
            "        to 0.25 are taken from [1]_. p-values are floored / capped\n",
            "        at 0.1% / 25%. Since the range of critical values might be extended in\n",
            "        future releases, it is recommended not to test ``p == 0.25``, but rather\n",
            "        ``p >= 0.25`` (analogously for the lower bound).\n",
            "        \n",
            "        .. versionadded:: 0.14.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n",
            "               Anderson-Darling Tests, Journal of the American Statistical\n",
            "               Association, Vol. 82, pp. 918-924.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> res = stats.anderson_ksamp([rng.normal(size=50),\n",
            "        ... rng.normal(loc=0.5, size=30)])\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (1.974403288713695, 0.04991293614572478)\n",
            "        >>> res.critical_values\n",
            "        array([0.325, 1.226, 1.961, 2.718, 3.752, 4.592, 6.546])\n",
            "        \n",
            "        The null hypothesis that the two random samples come from the same\n",
            "        distribution can be rejected at the 5% level because the returned\n",
            "        test value is greater than the critical value for 5% (1.961) but\n",
            "        not at the 2.5% level. The interpolation gives an approximate\n",
            "        p-value of 4.99%.\n",
            "        \n",
            "        >>> samples = [rng.normal(size=50), rng.normal(size=30),\n",
            "        ...            rng.normal(size=20)]\n",
            "        >>> res = stats.anderson_ksamp(samples)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (-0.29103725200789504, 0.25)\n",
            "        >>> res.critical_values\n",
            "        array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,\n",
            "          4.07210043, 5.56419101])\n",
            "        \n",
            "        The null hypothesis cannot be rejected for three samples from an\n",
            "        identical distribution. The reported p-value (25%) has been capped and\n",
            "        may not be very accurate (since it corresponds to the value 0.449\n",
            "        whereas the statistic is -0.291).\n",
            "        \n",
            "        In such cases where the p-value is capped or when sample sizes are\n",
            "        small, a permutation test may be more accurate.\n",
            "        \n",
            "        >>> method = stats.PermutationMethod(n_resamples=9999, random_state=rng)\n",
            "        >>> res = stats.anderson_ksamp(samples, method=method)\n",
            "        >>> res.pvalue\n",
            "        0.5254\n",
            "    \n",
            "    ansari(x, y, alternative='two-sided', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform the Ansari-Bradley test for equal scale parameters.\n",
            "        \n",
            "        The Ansari-Bradley test ([1]_, [2]_) is a non-parametric test\n",
            "        for the equality of the scale parameter of the distributions\n",
            "        from which two samples were drawn. The null hypothesis states that\n",
            "        the ratio of the scale of the distribution underlying `x` to the scale\n",
            "        of the distribution underlying `y` is 1.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Arrays of sample data.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "            \n",
            "            * 'two-sided': the ratio of scales is not equal to 1.\n",
            "            * 'less': the ratio of scales is less than 1.\n",
            "            * 'greater': the ratio of scales is greater than 1.\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The Ansari-Bradley test statistic.\n",
            "        pvalue : float\n",
            "            The p-value of the hypothesis test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`fligner`\n",
            "            A non-parametric test for the equality of k variances\n",
            "        :func:`mood`\n",
            "            A non-parametric test for the equality of two scale parameters\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The p-value given is exact when the sample sizes are both less than\n",
            "        55 and there are no ties, otherwise a normal approximation for the\n",
            "        p-value is used.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for\n",
            "               dispersions, Annals of Mathematical Statistics, 31, 1174-1189.\n",
            "        .. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric\n",
            "               statistical methods.  3rd ed. Chapman and Hall/CRC. 2001.\n",
            "               Section 5.8.2.\n",
            "        .. [3] Nathaniel E. Helwig \"Nonparametric Dispersion and Equality\n",
            "               Tests\" at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ansari\n",
            "        >>> rng = np.random.default_rng()\n",
            "        \n",
            "        For these examples, we'll create three random data sets.  The first\n",
            "        two, with sizes 35 and 25, are drawn from a normal distribution with\n",
            "        mean 0 and standard deviation 2.  The third data set has size 25 and\n",
            "        is drawn from a normal distribution with standard deviation 1.25.\n",
            "        \n",
            "        >>> x1 = rng.normal(loc=0, scale=2, size=35)\n",
            "        >>> x2 = rng.normal(loc=0, scale=2, size=25)\n",
            "        >>> x3 = rng.normal(loc=0, scale=1.25, size=25)\n",
            "        \n",
            "        First we apply `ansari` to `x1` and `x2`.  These samples are drawn\n",
            "        from the same distribution, so we expect the Ansari-Bradley test\n",
            "        should not lead us to conclude that the scales of the distributions\n",
            "        are different.\n",
            "        \n",
            "        >>> ansari(x1, x2)\n",
            "        AnsariResult(statistic=541.0, pvalue=0.9762532927399098)\n",
            "        \n",
            "        With a p-value close to 1, we cannot conclude that there is a\n",
            "        significant difference in the scales (as expected).\n",
            "        \n",
            "        Now apply the test to `x1` and `x3`:\n",
            "        \n",
            "        >>> ansari(x1, x3)\n",
            "        AnsariResult(statistic=425.0, pvalue=0.0003087020407974518)\n",
            "        \n",
            "        The probability of observing such an extreme value of the statistic\n",
            "        under the null hypothesis of equal scales is only 0.03087%. We take this\n",
            "        as evidence against the null hypothesis in favor of the alternative:\n",
            "        the scales of the distributions from which the samples were drawn\n",
            "        are not equal.\n",
            "        \n",
            "        We can use the `alternative` parameter to perform a one-tailed test.\n",
            "        In the above example, the scale of `x1` is greater than `x3` and so\n",
            "        the ratio of scales of `x1` and `x3` is greater than 1. This means\n",
            "        that the p-value when ``alternative='greater'`` should be near 0 and\n",
            "        hence we should be able to reject the null hypothesis:\n",
            "        \n",
            "        >>> ansari(x1, x3, alternative='greater')\n",
            "        AnsariResult(statistic=425.0, pvalue=0.0001543510203987259)\n",
            "        \n",
            "        As we can see, the p-value is indeed quite low. Use of\n",
            "        ``alternative='less'`` should thus yield a large p-value:\n",
            "        \n",
            "        >>> ansari(x1, x3, alternative='less')\n",
            "        AnsariResult(statistic=425.0, pvalue=0.9998643258449039)\n",
            "    \n",
            "    barnard_exact(table, alternative='two-sided', pooled=True, n=32)\n",
            "        Perform a Barnard exact test on a 2x2 contingency table.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        table : array_like of ints\n",
            "            A 2x2 contingency table.  Elements should be non-negative integers.\n",
            "        \n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
            "            Please see explanations in the Notes section below.\n",
            "        \n",
            "        pooled : bool, optional\n",
            "            Whether to compute score statistic with pooled variance (as in\n",
            "            Student's t-test, for example) or unpooled variance (as in Welch's\n",
            "            t-test). Default is ``True``.\n",
            "        \n",
            "        n : int, optional\n",
            "            Number of sampling points used in the construction of the sampling\n",
            "            method. Note that this argument will automatically be converted to\n",
            "            the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n",
            "            select sample points. Default is 32. Must be positive. In most cases,\n",
            "            32 points is enough to reach good precision. More points comes at\n",
            "            performance cost.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        ber : BarnardExactResult\n",
            "            A result object with the following attributes.\n",
            "        \n",
            "            statistic : float\n",
            "                The Wald statistic with pooled or unpooled variance, depending\n",
            "                on the user choice of `pooled`.\n",
            "        \n",
            "            pvalue : float\n",
            "                P-value, the probability of obtaining a distribution at least as\n",
            "                extreme as the one that was actually observed, assuming that the\n",
            "                null hypothesis is true.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        chi2_contingency : Chi-square test of independence of variables in a\n",
            "            contingency table.\n",
            "        fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
            "        boschloo_exact : Boschloo's exact test on a 2x2 contingency table,\n",
            "            which is an uniformly more powerful alternative to Fisher's exact test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Barnard's test is an exact test used in the analysis of contingency\n",
            "        tables. It examines the association of two categorical variables, and\n",
            "        is a more powerful alternative than Fisher's exact test\n",
            "        for 2x2 contingency tables.\n",
            "        \n",
            "        Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n",
            "        where each column stores the binomial experiment, as in the example\n",
            "        below. Let's also define :math:`p_1, p_2` the theoretical binomial\n",
            "        probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n",
            "        Barnard exact test, we can assert three different null hypotheses :\n",
            "        \n",
            "        - :math:`H_0 : p_1 \\geq p_2` versus :math:`H_1 : p_1 < p_2`,\n",
            "          with `alternative` = \"less\"\n",
            "        \n",
            "        - :math:`H_0 : p_1 \\leq p_2` versus :math:`H_1 : p_1 > p_2`,\n",
            "          with `alternative` = \"greater\"\n",
            "        \n",
            "        - :math:`H_0 : p_1 = p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n",
            "          with `alternative` = \"two-sided\" (default one)\n",
            "        \n",
            "        In order to compute Barnard's exact test, we are using the Wald\n",
            "        statistic [3]_ with pooled or unpooled variance.\n",
            "        Under the default assumption that both variances are equal\n",
            "        (``pooled = True``), the statistic is computed as:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            T(X) = \\frac{\n",
            "                \\hat{p}_1 - \\hat{p}_2\n",
            "            }{\n",
            "                \\sqrt{\n",
            "                    \\hat{p}(1 - \\hat{p})\n",
            "                    (\\frac{1}{c_1} +\n",
            "                    \\frac{1}{c_2})\n",
            "                }\n",
            "            }\n",
            "        \n",
            "        with :math:`\\hat{p}_1, \\hat{p}_2` and :math:`\\hat{p}` the estimator of\n",
            "        :math:`p_1, p_2` and :math:`p`, the latter being the combined probability,\n",
            "        given the assumption that :math:`p_1 = p_2`.\n",
            "        \n",
            "        If this assumption is invalid (``pooled = False``), the statistic is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            T(X) = \\frac{\n",
            "                \\hat{p}_1 - \\hat{p}_2\n",
            "            }{\n",
            "                \\sqrt{\n",
            "                    \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{c_1} +\n",
            "                    \\frac{\\hat{p}_2 (1 - \\hat{p}_2)}{c_2}\n",
            "                }\n",
            "            }\n",
            "        \n",
            "        The p-value is then computed as:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\sum\n",
            "                \\binom{c_1}{x_{11}}\n",
            "                \\binom{c_2}{x_{12}}\n",
            "                \\pi^{x_{11} + x_{12}}\n",
            "                (1 - \\pi)^{t - x_{11} - x_{12}}\n",
            "        \n",
            "        where the sum is over all  2x2 contingency tables :math:`X` such that:\n",
            "        * :math:`T(X) \\leq T(X_0)` when `alternative` = \"less\",\n",
            "        * :math:`T(X) \\geq T(X_0)` when `alternative` = \"greater\", or\n",
            "        * :math:`T(X) \\geq |T(X_0)|` when `alternative` = \"two-sided\".\n",
            "        Above, :math:`c_1, c_2` are the sum of the columns 1 and 2,\n",
            "        and :math:`t` the total (sum of the 4 sample's element).\n",
            "        \n",
            "        The returned p-value is the maximum p-value taken over the nuisance\n",
            "        parameter :math:`\\pi`, where :math:`0 \\leq \\pi \\leq 1`.\n",
            "        \n",
            "        This function's complexity is :math:`O(n c_1 c_2)`, where `n` is the\n",
            "        number of sample points.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Barnard, G. A. \"Significance Tests for 2x2 Tables\". *Biometrika*.\n",
            "               34.1/2 (1947): 123-138. :doi:`dpgkg3`\n",
            "        \n",
            "        .. [2] Mehta, Cyrus R., and Pralay Senchaudhuri. \"Conditional versus\n",
            "               unconditional exact tests for comparing two binomials.\"\n",
            "               *Cytel Software Corporation* 675 (2003): 1-5.\n",
            "        \n",
            "        .. [3] \"Wald Test\". *Wikipedia*. https://en.wikipedia.org/wiki/Wald_test\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        An example use of Barnard's test is presented in [2]_.\n",
            "        \n",
            "            Consider the following example of a vaccine efficacy study\n",
            "            (Chan, 1998). In a randomized clinical trial of 30 subjects, 15 were\n",
            "            inoculated with a recombinant DNA influenza vaccine and the 15 were\n",
            "            inoculated with a placebo. Twelve of the 15 subjects in the placebo\n",
            "            group (80%) eventually became infected with influenza whereas for the\n",
            "            vaccine group, only 7 of the 15 subjects (47%) became infected. The\n",
            "            data are tabulated as a 2 x 2 table::\n",
            "        \n",
            "                    Vaccine  Placebo\n",
            "                Yes     7        12\n",
            "                No      8        3\n",
            "        \n",
            "        When working with statistical hypothesis testing, we usually use a\n",
            "        threshold probability or significance level upon which we decide\n",
            "        to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n",
            "        significance level of 5%.\n",
            "        \n",
            "        Our alternative hypothesis is that the vaccine will lower the chance of\n",
            "        becoming infected with the virus; that is, the probability :math:`p_1` of\n",
            "        catching the virus with the vaccine will be *less than* the probability\n",
            "        :math:`p_2` of catching the virus without the vaccine.  Therefore, we call\n",
            "        `barnard_exact` with the ``alternative=\"less\"`` option:\n",
            "        \n",
            "        >>> import scipy.stats as stats\n",
            "        >>> res = stats.barnard_exact([[7, 12], [8, 3]], alternative=\"less\")\n",
            "        >>> res.statistic\n",
            "        -1.894...\n",
            "        >>> res.pvalue\n",
            "        0.03407...\n",
            "        \n",
            "        Under the null hypothesis that the vaccine will not lower the chance of\n",
            "        becoming infected, the probability of obtaining test results at least as\n",
            "        extreme as the observed data is approximately 3.4%. Since this p-value is\n",
            "        less than our chosen significance level, we have evidence to reject\n",
            "        :math:`H_0` in favor of the alternative.\n",
            "        \n",
            "        Suppose we had used Fisher's exact test instead:\n",
            "        \n",
            "        >>> _, pvalue = stats.fisher_exact([[7, 12], [8, 3]], alternative=\"less\")\n",
            "        >>> pvalue\n",
            "        0.0640...\n",
            "        \n",
            "        With the same threshold significance of 5%, we would not have been able\n",
            "        to reject the null hypothesis in favor of the alternative. As stated in\n",
            "        [2]_, Barnard's test is uniformly more powerful than Fisher's exact test\n",
            "        because Barnard's test does not condition on any margin. Fisher's test\n",
            "        should only be used when both sets of marginals are fixed.\n",
            "    \n",
            "    bartlett(*samples, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform Bartlett's test for equal variances.\n",
            "        \n",
            "        Bartlett's test tests the null hypothesis that all input samples\n",
            "        are from populations with equal variances.  For samples\n",
            "        from significantly non-normal populations, Levene's test\n",
            "        `levene` is more robust.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            arrays of sample data.  Only 1d arrays are accepted, they may have\n",
            "            different lengths.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic.\n",
            "        pvalue : float\n",
            "            The p-value of the test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`fligner`\n",
            "            A non-parametric test for the equality of k variances\n",
            "        :func:`levene`\n",
            "            A robust parametric test for equality of k variances\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Conover et al. (1981) examine many of the existing parametric and\n",
            "        nonparametric tests by extensive simulations and they conclude that the\n",
            "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
            "        superior in terms of robustness of departures from normality and power\n",
            "        ([3]_).\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm\n",
            "        .. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical\n",
            "                  Methods, Eighth Edition, Iowa State University Press.\n",
            "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
            "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
            "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
            "               University.\n",
            "        .. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical\n",
            "               Tests. Proceedings of the Royal Society of London. Series A,\n",
            "               Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.\n",
            "        .. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n",
            "               Reference to the Vitamins, pp 499-503,\n",
            "               :doi:`10.1016/C2013-0-12584-6`.\n",
            "        .. [6] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [7] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n",
            "               superior to t and F tests in biomedical research. The American\n",
            "               Statistician, 52(2), 127-132.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [5]_, the influence of vitamin C on the tooth growth of guinea pigs\n",
            "        was investigated. In a control study, 60 subjects were divided into\n",
            "        small dose, medium dose, and large dose groups that received\n",
            "        daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\n",
            "        After 42 days, the tooth growth was measured.\n",
            "        \n",
            "        The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\n",
            "        tooth growth measurements of the three groups in microns.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> small_dose = np.array([\n",
            "        ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n",
            "        ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n",
            "        ... ])\n",
            "        >>> medium_dose = np.array([\n",
            "        ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n",
            "        ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n",
            "        ... ])\n",
            "        >>> large_dose = np.array([\n",
            "        ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n",
            "        ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n",
            "        ... ])\n",
            "        \n",
            "        The `bartlett` statistic is sensitive to differences in variances\n",
            "        between the samples.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.bartlett(small_dose, medium_dose, large_dose)\n",
            "        >>> res.statistic\n",
            "        0.6654670663030519\n",
            "        \n",
            "        The value of the statistic tends to be high when there is a large\n",
            "        difference in variances.\n",
            "        \n",
            "        We can test for inequality of variance among the groups by comparing the\n",
            "        observed value of the statistic against the null distribution: the\n",
            "        distribution of statistic values derived under the null hypothesis that\n",
            "        the population variances of the three groups are equal.\n",
            "        \n",
            "        For this test, the null distribution follows the chi-square distribution\n",
            "        as shown below.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> k = 3  # number of samples\n",
            "        >>> dist = stats.chi2(df=k-1)\n",
            "        >>> val = np.linspace(0, 5, 100)\n",
            "        >>> pdf = dist.pdf(val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(val, pdf, color='C0')\n",
            "        ...     ax.set_title(\"Bartlett Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        ...     ax.set_xlim(0, 5)\n",
            "        ...     ax.set_ylim(0, 1)\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution greater than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n",
            "        >>> i = val >= res.statistic\n",
            "        >>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> res.pvalue\n",
            "        0.71696121509966\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from distributions with identical variances that produces\n",
            "        such an extreme value of the statistic - this may be taken as evidence\n",
            "        against the null hypothesis in favor of the alternative: the variances of\n",
            "        the groups are not equal. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [6]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        - Small p-values are not evidence for a *large* effect; rather, they can\n",
            "          only provide evidence for a \"significant\" effect, meaning that they are\n",
            "          unlikely to have occurred under the null hypothesis.\n",
            "        \n",
            "        Note that the chi-square distribution provides the null distribution\n",
            "        when the observations are normally distributed. For small samples\n",
            "        drawn from non-normal populations, it may be more appropriate to\n",
            "        perform a\n",
            "        permutation test: Under the null hypothesis that all three samples were\n",
            "        drawn from the same population, each of the measurements is equally likely\n",
            "        to have been observed in any of the three samples. Therefore, we can form\n",
            "        a randomized null distribution by calculating the statistic under many\n",
            "        randomly-generated partitionings of the observations into the three\n",
            "        samples.\n",
            "        \n",
            "        >>> def statistic(*samples):\n",
            "        ...     return stats.bartlett(*samples).statistic\n",
            "        >>> ref = stats.permutation_test(\n",
            "        ...     (small_dose, medium_dose, large_dose), statistic,\n",
            "        ...     permutation_type='independent', alternative='greater'\n",
            "        ... )\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> bins = np.linspace(0, 5, 25)\n",
            "        >>> ax.hist(\n",
            "        ...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n",
            "        ... )\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'randomized null distribution'])\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> ref.pvalue  # randomized test p-value\n",
            "        0.5387  # may vary\n",
            "        \n",
            "        Note that there is significant disagreement between the p-value calculated\n",
            "        here and the asymptotic approximation returned by `bartlett` above.\n",
            "        The statistical inferences that can be drawn rigorously from a permutation\n",
            "        test are limited; nonetheless, they may be the preferred approach in many\n",
            "        circumstances [7]_.\n",
            "        \n",
            "        Following is another generic example where the null hypothesis would be\n",
            "        rejected.\n",
            "        \n",
            "        Test whether the lists `a`, `b` and `c` come from populations\n",
            "        with equal variances.\n",
            "        \n",
            "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
            "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
            "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
            "        >>> stat, p = stats.bartlett(a, b, c)\n",
            "        >>> p\n",
            "        1.1254782518834628e-05\n",
            "        \n",
            "        The very small p-value suggests that the populations do not have equal\n",
            "        variances.\n",
            "        \n",
            "        This is not surprising, given that the sample variance of `b` is much\n",
            "        larger than that of `a` and `c`:\n",
            "        \n",
            "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
            "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
            "    \n",
            "    bayes_mvs(data, alpha=0.9)\n",
            "        Bayesian confidence intervals for the mean, var, and std.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array_like\n",
            "            Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.\n",
            "            Requires 2 or more data points.\n",
            "        alpha : float, optional\n",
            "            Probability that the returned confidence interval contains\n",
            "            the true parameter.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        mean_cntr, var_cntr, std_cntr : tuple\n",
            "            The three results are for the mean, variance and standard deviation,\n",
            "            respectively.  Each result is a tuple of the form::\n",
            "        \n",
            "                (center, (lower, upper))\n",
            "        \n",
            "            with `center` the mean of the conditional pdf of the value given the\n",
            "            data, and `(lower, upper)` a confidence interval, centered on the\n",
            "            median, containing the estimate to a probability ``alpha``.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        mvsdist\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Each tuple of mean, variance, and standard deviation estimates represent\n",
            "        the (center, (lower, upper)) with center the mean of the conditional pdf\n",
            "        of the value given the data and (lower, upper) is a confidence interval\n",
            "        centered on the median, containing the estimate to a probability\n",
            "        ``alpha``.\n",
            "        \n",
            "        Converts data to 1-D and assumes all data has the same mean and variance.\n",
            "        Uses Jeffrey's prior for variance and std.\n",
            "        \n",
            "        Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
            "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
            "        2006.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        First a basic example to demonstrate the outputs:\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
            "        >>> mean, var, std = stats.bayes_mvs(data)\n",
            "        >>> mean\n",
            "        Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))\n",
            "        >>> var\n",
            "        Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))\n",
            "        >>> std\n",
            "        Std_dev(statistic=2.9724954732045084,\n",
            "                minmax=(1.7823367265645143, 4.945614605014631))\n",
            "        \n",
            "        Now we generate some normally distributed random data, and get estimates of\n",
            "        mean and standard deviation with 95% confidence intervals for those\n",
            "        estimates:\n",
            "        \n",
            "        >>> n_samples = 100000\n",
            "        >>> data = stats.norm.rvs(size=n_samples)\n",
            "        >>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.hist(data, bins=100, density=True, label='Histogram of data')\n",
            "        >>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')\n",
            "        >>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',\n",
            "        ...            alpha=0.2, label=r'Estimated mean (95% limits)')\n",
            "        >>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')\n",
            "        >>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,\n",
            "        ...            label=r'Estimated scale (95% limits)')\n",
            "        \n",
            "        >>> ax.legend(fontsize=10)\n",
            "        >>> ax.set_xlim([-4, 4])\n",
            "        >>> ax.set_ylim([0, 0.5])\n",
            "        >>> plt.show()\n",
            "    \n",
            "    binned_statistic(x, values, statistic='mean', bins=10, range=None)\n",
            "        Compute a binned statistic for one or more sets of data.\n",
            "        \n",
            "        This is a generalization of a histogram function.  A histogram divides\n",
            "        the space into bins, and returns the count of the number of points in\n",
            "        each bin.  This function allows the computation of the sum, mean, median,\n",
            "        or other statistic of the values (or set of values) within each bin.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : (N,) array_like\n",
            "            A sequence of values to be binned.\n",
            "        values : (N,) array_like or list of (N,) array_like\n",
            "            The data on which the statistic will be computed.  This must be\n",
            "            the same shape as `x`, or a set of sequences - each the same shape as\n",
            "            `x`.  If `values` is a set of sequences, the statistic will be computed\n",
            "            on each independently.\n",
            "        statistic : string or callable, optional\n",
            "            The statistic to compute (default is 'mean').\n",
            "            The following statistics are available:\n",
            "        \n",
            "              * 'mean' : compute the mean of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'std' : compute the standard deviation within each bin. This\n",
            "                is implicitly calculated with ddof=0.\n",
            "              * 'median' : compute the median of values for points within each\n",
            "                bin. Empty bins will be represented by NaN.\n",
            "              * 'count' : compute the count of points within each bin.  This is\n",
            "                identical to an unweighted histogram.  `values` array is not\n",
            "                referenced.\n",
            "              * 'sum' : compute the sum of values for points within each bin.\n",
            "                This is identical to a weighted histogram.\n",
            "              * 'min' : compute the minimum of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'max' : compute the maximum of values for point within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * function : a user-defined function which takes a 1D array of\n",
            "                values, and outputs a single numerical statistic. This function\n",
            "                will be called on the values in each bin.  Empty bins will be\n",
            "                represented by function([]), or NaN if this returns an error.\n",
            "        \n",
            "        bins : int or sequence of scalars, optional\n",
            "            If `bins` is an int, it defines the number of equal-width bins in the\n",
            "            given range (10 by default).  If `bins` is a sequence, it defines the\n",
            "            bin edges, including the rightmost edge, allowing for non-uniform bin\n",
            "            widths.  Values in `x` that are smaller than lowest bin edge are\n",
            "            assigned to bin number 0, values beyond the highest bin are assigned to\n",
            "            ``bins[-1]``.  If the bin edges are specified, the number of bins will\n",
            "            be, (nx = len(bins)-1).\n",
            "        range : (float, float) or [(float, float)], optional\n",
            "            The lower and upper range of the bins.  If not provided, range\n",
            "            is simply ``(x.min(), x.max())``.  Values outside the range are\n",
            "            ignored.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : array\n",
            "            The values of the selected statistic in each bin.\n",
            "        bin_edges : array of dtype float\n",
            "            Return the bin edges ``(length(statistic)+1)``.\n",
            "        binnumber: 1-D ndarray of ints\n",
            "            Indices of the bins (corresponding to `bin_edges`) in which each value\n",
            "            of `x` belongs.  Same length as `values`.  A binnumber of `i` means the\n",
            "            corresponding value is between (bin_edges[i-1], bin_edges[i]).\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.digitize, numpy.histogram, binned_statistic_2d, binned_statistic_dd\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
            "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
            "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
            "        ``[3, 4]``, which *includes* 4.\n",
            "        \n",
            "        .. versionadded:: 0.11.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        First some basic examples:\n",
            "        \n",
            "        Create two evenly spaced bins in the range of the given sample, and sum the\n",
            "        corresponding values in each of those bins:\n",
            "        \n",
            "        >>> values = [1.0, 1.0, 2.0, 1.5, 3.0]\n",
            "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
            "        BinnedStatisticResult(statistic=array([4. , 4.5]),\n",
            "                bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))\n",
            "        \n",
            "        Multiple arrays of values can also be passed.  The statistic is calculated\n",
            "        on each set independently:\n",
            "        \n",
            "        >>> values = [[1.0, 1.0, 2.0, 1.5, 3.0], [2.0, 2.0, 4.0, 3.0, 6.0]]\n",
            "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
            "        BinnedStatisticResult(statistic=array([[4. , 4.5],\n",
            "               [8. , 9. ]]), bin_edges=array([1., 4., 7.]),\n",
            "               binnumber=array([1, 1, 1, 2, 2]))\n",
            "        \n",
            "        >>> stats.binned_statistic([1, 2, 1, 2, 4], np.arange(5), statistic='mean',\n",
            "        ...                        bins=3)\n",
            "        BinnedStatisticResult(statistic=array([1., 2., 4.]),\n",
            "                bin_edges=array([1., 2., 3., 4.]),\n",
            "                binnumber=array([1, 2, 1, 2, 3]))\n",
            "        \n",
            "        As a second example, we now generate some random data of sailing boat speed\n",
            "        as a function of wind speed, and then determine how fast our boat is for\n",
            "        certain wind speeds:\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> windspeed = 8 * rng.random(500)\n",
            "        >>> boatspeed = .3 * windspeed**.5 + .2 * rng.random(500)\n",
            "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(windspeed,\n",
            "        ...                 boatspeed, statistic='median', bins=[1,2,3,4,5,6,7])\n",
            "        >>> plt.figure()\n",
            "        >>> plt.plot(windspeed, boatspeed, 'b.', label='raw data')\n",
            "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n",
            "        ...            label='binned statistic of data')\n",
            "        >>> plt.legend()\n",
            "        \n",
            "        Now we can use ``binnumber`` to select all datapoints with a windspeed\n",
            "        below 1:\n",
            "        \n",
            "        >>> low_boatspeed = boatspeed[binnumber == 0]\n",
            "        \n",
            "        As a final example, we will use ``bin_edges`` and ``binnumber`` to make a\n",
            "        plot of a distribution that shows the mean and distribution around that\n",
            "        mean per bin, on top of a regular histogram and the probability\n",
            "        distribution function:\n",
            "        \n",
            "        >>> x = np.linspace(0, 5, num=500)\n",
            "        >>> x_pdf = stats.maxwell.pdf(x)\n",
            "        >>> samples = stats.maxwell.rvs(size=10000)\n",
            "        \n",
            "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(x, x_pdf,\n",
            "        ...         statistic='mean', bins=25)\n",
            "        >>> bin_width = (bin_edges[1] - bin_edges[0])\n",
            "        >>> bin_centers = bin_edges[1:] - bin_width/2\n",
            "        \n",
            "        >>> plt.figure()\n",
            "        >>> plt.hist(samples, bins=50, density=True, histtype='stepfilled',\n",
            "        ...          alpha=0.2, label='histogram of data')\n",
            "        >>> plt.plot(x, x_pdf, 'r-', label='analytical pdf')\n",
            "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=2,\n",
            "        ...            label='binned statistic of data')\n",
            "        >>> plt.plot((binnumber - 0.5) * bin_width, x_pdf, 'g.', alpha=0.5)\n",
            "        >>> plt.legend(fontsize=10)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    binned_statistic_2d(x, y, values, statistic='mean', bins=10, range=None, expand_binnumbers=False)\n",
            "        Compute a bidimensional binned statistic for one or more sets of data.\n",
            "        \n",
            "        This is a generalization of a histogram2d function.  A histogram divides\n",
            "        the space into bins, and returns the count of the number of points in\n",
            "        each bin.  This function allows the computation of the sum, mean, median,\n",
            "        or other statistic of the values (or set of values) within each bin.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : (N,) array_like\n",
            "            A sequence of values to be binned along the first dimension.\n",
            "        y : (N,) array_like\n",
            "            A sequence of values to be binned along the second dimension.\n",
            "        values : (N,) array_like or list of (N,) array_like\n",
            "            The data on which the statistic will be computed.  This must be\n",
            "            the same shape as `x`, or a list of sequences - each with the same\n",
            "            shape as `x`.  If `values` is such a list, the statistic will be\n",
            "            computed on each independently.\n",
            "        statistic : string or callable, optional\n",
            "            The statistic to compute (default is 'mean').\n",
            "            The following statistics are available:\n",
            "        \n",
            "              * 'mean' : compute the mean of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'std' : compute the standard deviation within each bin. This\n",
            "                is implicitly calculated with ddof=0.\n",
            "              * 'median' : compute the median of values for points within each\n",
            "                bin. Empty bins will be represented by NaN.\n",
            "              * 'count' : compute the count of points within each bin.  This is\n",
            "                identical to an unweighted histogram.  `values` array is not\n",
            "                referenced.\n",
            "              * 'sum' : compute the sum of values for points within each bin.\n",
            "                This is identical to a weighted histogram.\n",
            "              * 'min' : compute the minimum of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'max' : compute the maximum of values for point within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * function : a user-defined function which takes a 1D array of\n",
            "                values, and outputs a single numerical statistic. This function\n",
            "                will be called on the values in each bin.  Empty bins will be\n",
            "                represented by function([]), or NaN if this returns an error.\n",
            "        \n",
            "        bins : int or [int, int] or array_like or [array, array], optional\n",
            "            The bin specification:\n",
            "        \n",
            "              * the number of bins for the two dimensions (nx = ny = bins),\n",
            "              * the number of bins in each dimension (nx, ny = bins),\n",
            "              * the bin edges for the two dimensions (x_edge = y_edge = bins),\n",
            "              * the bin edges in each dimension (x_edge, y_edge = bins).\n",
            "        \n",
            "            If the bin edges are specified, the number of bins will be,\n",
            "            (nx = len(x_edge)-1, ny = len(y_edge)-1).\n",
            "        \n",
            "        range : (2,2) array_like, optional\n",
            "            The leftmost and rightmost edges of the bins along each dimension\n",
            "            (if not specified explicitly in the `bins` parameters):\n",
            "            [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be\n",
            "            considered outliers and not tallied in the histogram.\n",
            "        expand_binnumbers : bool, optional\n",
            "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
            "            linearized bin indices.\n",
            "            'True': the returned `binnumber` is 'unraveled' into a shape (2,N)\n",
            "            ndarray, where each row gives the bin numbers in the corresponding\n",
            "            dimension.\n",
            "            See the `binnumber` returned value, and the `Examples` section.\n",
            "        \n",
            "            .. versionadded:: 0.17.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : (nx, ny) ndarray\n",
            "            The values of the selected statistic in each two-dimensional bin.\n",
            "        x_edge : (nx + 1) ndarray\n",
            "            The bin edges along the first dimension.\n",
            "        y_edge : (ny + 1) ndarray\n",
            "            The bin edges along the second dimension.\n",
            "        binnumber : (N,) array of ints or (2,N) ndarray of ints\n",
            "            This assigns to each element of `sample` an integer that represents the\n",
            "            bin in which this observation falls.  The representation depends on the\n",
            "            `expand_binnumbers` argument.  See `Notes` for details.\n",
            "        \n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.digitize, numpy.histogram2d, binned_statistic, binned_statistic_dd\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Binedges:\n",
            "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
            "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
            "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
            "        ``[3, 4]``, which *includes* 4.\n",
            "        \n",
            "        `binnumber`:\n",
            "        This returned argument assigns to each element of `sample` an integer that\n",
            "        represents the bin in which it belongs.  The representation depends on the\n",
            "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
            "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
            "        element of `sample` to its corresponding bin (using row-major ordering).\n",
            "        Note that the returned linearized bin indices are used for an array with\n",
            "        extra bins on the outer binedges to capture values outside of the defined\n",
            "        bin bounds.\n",
            "        If 'True': The returned `binnumber` is a shape (2,N) ndarray where\n",
            "        each row indicates bin placements for each dimension respectively.  In each\n",
            "        dimension, a binnumber of `i` means the corresponding value is between\n",
            "        (D_edge[i-1], D_edge[i]), where 'D' is either 'x' or 'y'.\n",
            "        \n",
            "        .. versionadded:: 0.11.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        \n",
            "        Calculate the counts with explicit bin-edges:\n",
            "        \n",
            "        >>> x = [0.1, 0.1, 0.1, 0.6]\n",
            "        >>> y = [2.1, 2.6, 2.1, 2.1]\n",
            "        >>> binx = [0.0, 0.5, 1.0]\n",
            "        >>> biny = [2.0, 2.5, 3.0]\n",
            "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny])\n",
            "        >>> ret.statistic\n",
            "        array([[2., 1.],\n",
            "               [1., 0.]])\n",
            "        \n",
            "        The bin in which each sample is placed is given by the `binnumber`\n",
            "        returned parameter.  By default, these are the linearized bin indices:\n",
            "        \n",
            "        >>> ret.binnumber\n",
            "        array([5, 6, 5, 9])\n",
            "        \n",
            "        The bin indices can also be expanded into separate entries for each\n",
            "        dimension using the `expand_binnumbers` parameter:\n",
            "        \n",
            "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny],\n",
            "        ...                                 expand_binnumbers=True)\n",
            "        >>> ret.binnumber\n",
            "        array([[1, 1, 1, 2],\n",
            "               [1, 2, 1, 1]])\n",
            "        \n",
            "        Which shows that the first three elements belong in the xbin 1, and the\n",
            "        fourth into xbin 2; and so on for y.\n",
            "    \n",
            "    binned_statistic_dd(sample, values, statistic='mean', bins=10, range=None, expand_binnumbers=False, binned_statistic_result=None)\n",
            "        Compute a multidimensional binned statistic for a set of data.\n",
            "        \n",
            "        This is a generalization of a histogramdd function.  A histogram divides\n",
            "        the space into bins, and returns the count of the number of points in\n",
            "        each bin.  This function allows the computation of the sum, mean, median,\n",
            "        or other statistic of the values within each bin.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample : array_like\n",
            "            Data to histogram passed as a sequence of N arrays of length D, or\n",
            "            as an (N,D) array.\n",
            "        values : (N,) array_like or list of (N,) array_like\n",
            "            The data on which the statistic will be computed.  This must be\n",
            "            the same shape as `sample`, or a list of sequences - each with the\n",
            "            same shape as `sample`.  If `values` is such a list, the statistic\n",
            "            will be computed on each independently.\n",
            "        statistic : string or callable, optional\n",
            "            The statistic to compute (default is 'mean').\n",
            "            The following statistics are available:\n",
            "        \n",
            "              * 'mean' : compute the mean of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'median' : compute the median of values for points within each\n",
            "                bin. Empty bins will be represented by NaN.\n",
            "              * 'count' : compute the count of points within each bin.  This is\n",
            "                identical to an unweighted histogram.  `values` array is not\n",
            "                referenced.\n",
            "              * 'sum' : compute the sum of values for points within each bin.\n",
            "                This is identical to a weighted histogram.\n",
            "              * 'std' : compute the standard deviation within each bin. This\n",
            "                is implicitly calculated with ddof=0. If the number of values\n",
            "                within a given bin is 0 or 1, the computed standard deviation value\n",
            "                will be 0 for the bin.\n",
            "              * 'min' : compute the minimum of values for points within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * 'max' : compute the maximum of values for point within each bin.\n",
            "                Empty bins will be represented by NaN.\n",
            "              * function : a user-defined function which takes a 1D array of\n",
            "                values, and outputs a single numerical statistic. This function\n",
            "                will be called on the values in each bin.  Empty bins will be\n",
            "                represented by function([]), or NaN if this returns an error.\n",
            "        \n",
            "        bins : sequence or positive int, optional\n",
            "            The bin specification must be in one of the following forms:\n",
            "        \n",
            "              * A sequence of arrays describing the bin edges along each dimension.\n",
            "              * The number of bins for each dimension (nx, ny, ... = bins).\n",
            "              * The number of bins for all dimensions (nx = ny = ... = bins).\n",
            "        range : sequence, optional\n",
            "            A sequence of lower and upper bin edges to be used if the edges are\n",
            "            not given explicitly in `bins`. Defaults to the minimum and maximum\n",
            "            values along each dimension.\n",
            "        expand_binnumbers : bool, optional\n",
            "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
            "            linearized bin indices.\n",
            "            'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n",
            "            ndarray, where each row gives the bin numbers in the corresponding\n",
            "            dimension.\n",
            "            See the `binnumber` returned value, and the `Examples` section of\n",
            "            `binned_statistic_2d`.\n",
            "        binned_statistic_result : binnedStatisticddResult\n",
            "            Result of a previous call to the function in order to reuse bin edges\n",
            "            and bin numbers with new values and/or a different statistic.\n",
            "            To reuse bin numbers, `expand_binnumbers` must have been set to False\n",
            "            (the default)\n",
            "        \n",
            "            .. versionadded:: 0.17.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : ndarray, shape(nx1, nx2, nx3,...)\n",
            "            The values of the selected statistic in each two-dimensional bin.\n",
            "        bin_edges : list of ndarrays\n",
            "            A list of D arrays describing the (nxi + 1) bin edges for each\n",
            "            dimension.\n",
            "        binnumber : (N,) array of ints or (D,N) ndarray of ints\n",
            "            This assigns to each element of `sample` an integer that represents the\n",
            "            bin in which this observation falls.  The representation depends on the\n",
            "            `expand_binnumbers` argument.  See `Notes` for details.\n",
            "        \n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Binedges:\n",
            "        All but the last (righthand-most) bin is half-open in each dimension.  In\n",
            "        other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n",
            "        ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n",
            "        last bin, however, is ``[3, 4]``, which *includes* 4.\n",
            "        \n",
            "        `binnumber`:\n",
            "        This returned argument assigns to each element of `sample` an integer that\n",
            "        represents the bin in which it belongs.  The representation depends on the\n",
            "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
            "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
            "        element of `sample` to its corresponding bin (using row-major ordering).\n",
            "        If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n",
            "        each row indicates bin placements for each dimension respectively.  In each\n",
            "        dimension, a binnumber of `i` means the corresponding value is between\n",
            "        (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n",
            "        \n",
            "        .. versionadded:: 0.11.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
            "        \n",
            "        Take an array of 600 (x, y) coordinates as an example.\n",
            "        `binned_statistic_dd` can handle arrays of higher dimension `D`. But a plot\n",
            "        of dimension `D+1` is required.\n",
            "        \n",
            "        >>> mu = np.array([0., 1.])\n",
            "        >>> sigma = np.array([[1., -0.5],[-0.5, 1.5]])\n",
            "        >>> multinormal = stats.multivariate_normal(mu, sigma)\n",
            "        >>> data = multinormal.rvs(size=600, random_state=235412)\n",
            "        >>> data.shape\n",
            "        (600, 2)\n",
            "        \n",
            "        Create bins and count how many arrays fall in each bin:\n",
            "        \n",
            "        >>> N = 60\n",
            "        >>> x = np.linspace(-3, 3, N)\n",
            "        >>> y = np.linspace(-3, 4, N)\n",
            "        >>> ret = stats.binned_statistic_dd(data, np.arange(600), bins=[x, y],\n",
            "        ...                                 statistic='count')\n",
            "        >>> bincounts = ret.statistic\n",
            "        \n",
            "        Set the volume and the location of bars:\n",
            "        \n",
            "        >>> dx = x[1] - x[0]\n",
            "        >>> dy = y[1] - y[0]\n",
            "        >>> x, y = np.meshgrid(x[:-1]+dx/2, y[:-1]+dy/2)\n",
            "        >>> z = 0\n",
            "        \n",
            "        >>> bincounts = bincounts.ravel()\n",
            "        >>> x = x.ravel()\n",
            "        >>> y = y.ravel()\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111, projection='3d')\n",
            "        >>> with np.errstate(divide='ignore'):   # silence random axes3d warning\n",
            "        ...     ax.bar3d(x, y, z, dx, dy, bincounts)\n",
            "        \n",
            "        Reuse bin numbers and bin edges with new values:\n",
            "        \n",
            "        >>> ret2 = stats.binned_statistic_dd(data, -np.arange(600),\n",
            "        ...                                  binned_statistic_result=ret,\n",
            "        ...                                  statistic='mean')\n",
            "    \n",
            "    binomtest(k, n, p=0.5, alternative='two-sided')\n",
            "        Perform a test that the probability of success is p.\n",
            "        \n",
            "        The binomial test [1]_ is a test of the null hypothesis that the\n",
            "        probability of success in a Bernoulli experiment is `p`.\n",
            "        \n",
            "        Details of the test can be found in many texts on statistics, such\n",
            "        as section 24.5 of [2]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        k : int\n",
            "            The number of successes.\n",
            "        n : int\n",
            "            The number of trials.\n",
            "        p : float, optional\n",
            "            The hypothesized probability of success, i.e. the expected\n",
            "            proportion of successes.  The value must be in the interval\n",
            "            ``0 <= p <= 1``. The default value is ``p = 0.5``.\n",
            "        alternative : {'two-sided', 'greater', 'less'}, optional\n",
            "            Indicates the alternative hypothesis. The default value is\n",
            "            'two-sided'.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.BinomTestResult` instance\n",
            "            The return value is an object with the following attributes:\n",
            "        \n",
            "            k : int\n",
            "                The number of successes (copied from `binomtest` input).\n",
            "            n : int\n",
            "                The number of trials (copied from `binomtest` input).\n",
            "            alternative : str\n",
            "                Indicates the alternative hypothesis specified in the input\n",
            "                to `binomtest`.  It will be one of ``'two-sided'``, ``'greater'``,\n",
            "                or ``'less'``.\n",
            "            statistic : float\n",
            "                The estimate of the proportion of successes.\n",
            "            pvalue : float\n",
            "                The p-value of the hypothesis test.\n",
            "        \n",
            "            The object has the following methods:\n",
            "        \n",
            "            proportion_ci(confidence_level=0.95, method='exact') :\n",
            "                Compute the confidence interval for ``statistic``.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        .. versionadded:: 1.7.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Binomial test, https://en.wikipedia.org/wiki/Binomial_test\n",
            "        .. [2] Jerrold H. Zar, Biostatistical Analysis (fifth edition),\n",
            "               Prentice Hall, Upper Saddle River, New Jersey USA (2010)\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import binomtest\n",
            "        \n",
            "        A car manufacturer claims that no more than 10% of their cars are unsafe.\n",
            "        15 cars are inspected for safety, 3 were found to be unsafe. Test the\n",
            "        manufacturer's claim:\n",
            "        \n",
            "        >>> result = binomtest(3, n=15, p=0.1, alternative='greater')\n",
            "        >>> result.pvalue\n",
            "        0.18406106910639114\n",
            "        \n",
            "        The null hypothesis cannot be rejected at the 5% level of significance\n",
            "        because the returned p-value is greater than the critical value of 5%.\n",
            "        \n",
            "        The test statistic is equal to the estimated proportion, which is simply\n",
            "        ``3/15``:\n",
            "        \n",
            "        >>> result.statistic\n",
            "        0.2\n",
            "        \n",
            "        We can use the `proportion_ci()` method of the result to compute the\n",
            "        confidence interval of the estimate:\n",
            "        \n",
            "        >>> result.proportion_ci(confidence_level=0.95)\n",
            "        ConfidenceInterval(low=0.05684686759024681, high=1.0)\n",
            "    \n",
            "    bootstrap(data, statistic, *, n_resamples=9999, batch=None, vectorized=None, paired=False, axis=0, confidence_level=0.95, alternative='two-sided', method='BCa', bootstrap_result=None, random_state=None)\n",
            "        Compute a two-sided bootstrap confidence interval of a statistic.\n",
            "        \n",
            "        When `method` is ``'percentile'`` and `alternative` is ``'two-sided'``,\n",
            "        a bootstrap confidence interval is computed according to the following\n",
            "        procedure.\n",
            "        \n",
            "        1. Resample the data: for each sample in `data` and for each of\n",
            "           `n_resamples`, take a random sample of the original sample\n",
            "           (with replacement) of the same size as the original sample.\n",
            "        \n",
            "        2. Compute the bootstrap distribution of the statistic: for each set of\n",
            "           resamples, compute the test statistic.\n",
            "        \n",
            "        3. Determine the confidence interval: find the interval of the bootstrap\n",
            "           distribution that is\n",
            "        \n",
            "           - symmetric about the median and\n",
            "           - contains `confidence_level` of the resampled statistic values.\n",
            "        \n",
            "        While the ``'percentile'`` method is the most intuitive, it is rarely\n",
            "        used in practice. Two more common methods are available, ``'basic'``\n",
            "        ('reverse percentile') and ``'BCa'`` ('bias-corrected and accelerated');\n",
            "        they differ in how step 3 is performed.\n",
            "        \n",
            "        If the samples in `data` are  taken at random from their respective\n",
            "        distributions :math:`n` times, the confidence interval returned by\n",
            "        `bootstrap` will contain the true value of the statistic for those\n",
            "        distributions approximately `confidence_level`:math:`\\, \\times \\, n` times.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : sequence of array-like\n",
            "             Each element of data is a sample from an underlying distribution.\n",
            "        statistic : callable\n",
            "            Statistic for which the confidence interval is to be calculated.\n",
            "            `statistic` must be a callable that accepts ``len(data)`` samples\n",
            "            as separate arguments and returns the resulting statistic.\n",
            "            If `vectorized` is set ``True``,\n",
            "            `statistic` must also accept a keyword argument `axis` and be\n",
            "            vectorized to compute the statistic along the provided `axis`.\n",
            "        n_resamples : int, default: ``9999``\n",
            "            The number of resamples performed to form the bootstrap distribution\n",
            "            of the statistic.\n",
            "        batch : int, optional\n",
            "            The number of resamples to process in each vectorized call to\n",
            "            `statistic`. Memory usage is O( `batch` * ``n`` ), where ``n`` is the\n",
            "            sample size. Default is ``None``, in which case ``batch = n_resamples``\n",
            "            (or ``batch = max(n_resamples, n)`` for ``method='BCa'``).\n",
            "        vectorized : bool, optional\n",
            "            If `vectorized` is set ``False``, `statistic` will not be passed\n",
            "            keyword argument `axis` and is expected to calculate the statistic\n",
            "            only for 1D samples. If ``True``, `statistic` will be passed keyword\n",
            "            argument `axis` and is expected to calculate the statistic along `axis`\n",
            "            when passed an ND sample array. If ``None`` (default), `vectorized`\n",
            "            will be set ``True`` if ``axis`` is a parameter of `statistic`. Use of\n",
            "            a vectorized statistic typically reduces computation time.\n",
            "        paired : bool, default: ``False``\n",
            "            Whether the statistic treats corresponding elements of the samples\n",
            "            in `data` as paired.\n",
            "        axis : int, default: ``0``\n",
            "            The axis of the samples in `data` along which the `statistic` is\n",
            "            calculated.\n",
            "        confidence_level : float, default: ``0.95``\n",
            "            The confidence level of the confidence interval.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, default: ``'two-sided'``\n",
            "            Choose ``'two-sided'`` (default) for a two-sided confidence interval,\n",
            "            ``'less'`` for a one-sided confidence interval with the lower bound\n",
            "            at ``-np.inf``, and ``'greater'`` for a one-sided confidence interval\n",
            "            with the upper bound at ``np.inf``. The other bound of the one-sided\n",
            "            confidence intervals is the same as that of a two-sided confidence\n",
            "            interval with `confidence_level` twice as far from 1.0; e.g. the upper\n",
            "            bound of a 95% ``'less'``  confidence interval is the same as the upper\n",
            "            bound of a 90% ``'two-sided'`` confidence interval.\n",
            "        method : {'percentile', 'basic', 'bca'}, default: ``'BCa'``\n",
            "            Whether to return the 'percentile' bootstrap confidence interval\n",
            "            (``'percentile'``), the 'basic' (AKA 'reverse') bootstrap confidence\n",
            "            interval (``'basic'``), or the bias-corrected and accelerated bootstrap\n",
            "            confidence interval (``'BCa'``).\n",
            "        bootstrap_result : BootstrapResult, optional\n",
            "            Provide the result object returned by a previous call to `bootstrap`\n",
            "            to include the previous bootstrap distribution in the new bootstrap\n",
            "            distribution. This can be used, for example, to change\n",
            "            `confidence_level`, change `method`, or see the effect of performing\n",
            "            additional resampling without repeating computations.\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                        `numpy.random.RandomState`}, optional\n",
            "        \n",
            "            Pseudorandom number generator state used to generate resamples.\n",
            "        \n",
            "            If `random_state` is ``None`` (or `np.random`), the\n",
            "            `numpy.random.RandomState` singleton is used.\n",
            "            If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `random_state`.\n",
            "            If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "            instance then that instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : BootstrapResult\n",
            "            An object with attributes:\n",
            "        \n",
            "            confidence_interval : ConfidenceInterval\n",
            "                The bootstrap confidence interval as an instance of\n",
            "                `collections.namedtuple` with attributes `low` and `high`.\n",
            "            bootstrap_distribution : ndarray\n",
            "                The bootstrap distribution, that is, the value of `statistic` for\n",
            "                each resample. The last dimension corresponds with the resamples\n",
            "                (e.g. ``res.bootstrap_distribution.shape[-1] == n_resamples``).\n",
            "            standard_error : float or ndarray\n",
            "                The bootstrap standard error, that is, the sample standard\n",
            "                deviation of the bootstrap distribution.\n",
            "        \n",
            "        Warns\n",
            "        -----\n",
            "        `~scipy.stats.DegenerateDataWarning`\n",
            "            Generated when ``method='BCa'`` and the bootstrap distribution is\n",
            "            degenerate (e.g. all elements are identical).\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Elements of the confidence interval may be NaN for ``method='BCa'`` if\n",
            "        the bootstrap distribution is degenerate (e.g. all elements are identical).\n",
            "        In this case, consider using another `method` or inspecting `data` for\n",
            "        indications that other analysis may be more appropriate (e.g. all\n",
            "        observations are identical).\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap,\n",
            "           Chapman & Hall/CRC, Boca Raton, FL, USA (1993)\n",
            "        .. [2] Nathaniel E. Helwig, \"Bootstrap Confidence Intervals\",\n",
            "           http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf\n",
            "        .. [3] Bootstrapping (statistics), Wikipedia,\n",
            "           https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we have sampled data from an unknown distribution.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> from scipy.stats import norm\n",
            "        >>> dist = norm(loc=2, scale=4)  # our \"unknown\" distribution\n",
            "        >>> data = dist.rvs(size=100, random_state=rng)\n",
            "        \n",
            "        We are interested in the standard deviation of the distribution.\n",
            "        \n",
            "        >>> std_true = dist.std()      # the true value of the statistic\n",
            "        >>> print(std_true)\n",
            "        4.0\n",
            "        >>> std_sample = np.std(data)  # the sample statistic\n",
            "        >>> print(std_sample)\n",
            "        3.9460644295563863\n",
            "        \n",
            "        The bootstrap is used to approximate the variability we would expect if we\n",
            "        were to repeatedly sample from the unknown distribution and calculate the\n",
            "        statistic of the sample each time. It does this by repeatedly resampling\n",
            "        values *from the original sample* with replacement and calculating the\n",
            "        statistic of each resample. This results in a \"bootstrap distribution\" of\n",
            "        the statistic.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import bootstrap\n",
            "        >>> data = (data,)  # samples must be in a sequence\n",
            "        >>> res = bootstrap(data, np.std, confidence_level=0.9,\n",
            "        ...                 random_state=rng)\n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(res.bootstrap_distribution, bins=25)\n",
            "        >>> ax.set_title('Bootstrap Distribution')\n",
            "        >>> ax.set_xlabel('statistic value')\n",
            "        >>> ax.set_ylabel('frequency')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The standard error quantifies this variability. It is calculated as the\n",
            "        standard deviation of the bootstrap distribution.\n",
            "        \n",
            "        >>> res.standard_error\n",
            "        0.24427002125829136\n",
            "        >>> res.standard_error == np.std(res.bootstrap_distribution, ddof=1)\n",
            "        True\n",
            "        \n",
            "        The bootstrap distribution of the statistic is often approximately normal\n",
            "        with scale equal to the standard error.\n",
            "        \n",
            "        >>> x = np.linspace(3, 5)\n",
            "        >>> pdf = norm.pdf(x, loc=std_sample, scale=res.standard_error)\n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(res.bootstrap_distribution, bins=25, density=True)\n",
            "        >>> ax.plot(x, pdf)\n",
            "        >>> ax.set_title('Normal Approximation of the Bootstrap Distribution')\n",
            "        >>> ax.set_xlabel('statistic value')\n",
            "        >>> ax.set_ylabel('pdf')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        This suggests that we could construct a 90% confidence interval on the\n",
            "        statistic based on quantiles of this normal distribution.\n",
            "        \n",
            "        >>> norm.interval(0.9, loc=std_sample, scale=res.standard_error)\n",
            "        (3.5442759991341726, 4.3478528599786)\n",
            "        \n",
            "        Due to central limit theorem, this normal approximation is accurate for a\n",
            "        variety of statistics and distributions underlying the samples; however,\n",
            "        the approximation is not reliable in all cases. Because `bootstrap` is\n",
            "        designed to work with arbitrary underlying distributions and statistics,\n",
            "        it uses more advanced techniques to generate an accurate confidence\n",
            "        interval.\n",
            "        \n",
            "        >>> print(res.confidence_interval)\n",
            "        ConfidenceInterval(low=3.57655333533867, high=4.382043696342881)\n",
            "        \n",
            "        If we sample from the original distribution 1000 times and form a bootstrap\n",
            "        confidence interval for each sample, the confidence interval\n",
            "        contains the true value of the statistic approximately 90% of the time.\n",
            "        \n",
            "        >>> n_trials = 1000\n",
            "        >>> ci_contains_true_std = 0\n",
            "        >>> for i in range(n_trials):\n",
            "        ...    data = (dist.rvs(size=100, random_state=rng),)\n",
            "        ...    ci = bootstrap(data, np.std, confidence_level=0.9, n_resamples=1000,\n",
            "        ...                   random_state=rng).confidence_interval\n",
            "        ...    if ci[0] < std_true < ci[1]:\n",
            "        ...        ci_contains_true_std += 1\n",
            "        >>> print(ci_contains_true_std)\n",
            "        875\n",
            "        \n",
            "        Rather than writing a loop, we can also determine the confidence intervals\n",
            "        for all 1000 samples at once.\n",
            "        \n",
            "        >>> data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n",
            "        >>> res = bootstrap(data, np.std, axis=-1, confidence_level=0.9,\n",
            "        ...                 n_resamples=1000, random_state=rng)\n",
            "        >>> ci_l, ci_u = res.confidence_interval\n",
            "        \n",
            "        Here, `ci_l` and `ci_u` contain the confidence interval for each of the\n",
            "        ``n_trials = 1000`` samples.\n",
            "        \n",
            "        >>> print(ci_l[995:])\n",
            "        [3.77729695 3.75090233 3.45829131 3.34078217 3.48072829]\n",
            "        >>> print(ci_u[995:])\n",
            "        [4.88316666 4.86924034 4.32032996 4.2822427  4.59360598]\n",
            "        \n",
            "        And again, approximately 90% contain the true value, ``std_true = 4``.\n",
            "        \n",
            "        >>> print(np.sum((ci_l < std_true) & (std_true < ci_u)))\n",
            "        900\n",
            "        \n",
            "        `bootstrap` can also be used to estimate confidence intervals of\n",
            "        multi-sample statistics, including those calculated by hypothesis\n",
            "        tests. `scipy.stats.mood` perform's Mood's test for equal scale parameters,\n",
            "        and it returns two outputs: a statistic, and a p-value. To get a\n",
            "        confidence interval for the test statistic, we first wrap\n",
            "        `scipy.stats.mood` in a function that accepts two sample arguments,\n",
            "        accepts an `axis` keyword argument, and returns only the statistic.\n",
            "        \n",
            "        >>> from scipy.stats import mood\n",
            "        >>> def my_statistic(sample1, sample2, axis):\n",
            "        ...     statistic, _ = mood(sample1, sample2, axis=-1)\n",
            "        ...     return statistic\n",
            "        \n",
            "        Here, we use the 'percentile' method with the default 95% confidence level.\n",
            "        \n",
            "        >>> sample1 = norm.rvs(scale=1, size=100, random_state=rng)\n",
            "        >>> sample2 = norm.rvs(scale=2, size=100, random_state=rng)\n",
            "        >>> data = (sample1, sample2)\n",
            "        >>> res = bootstrap(data, my_statistic, method='basic', random_state=rng)\n",
            "        >>> print(mood(sample1, sample2)[0])  # element 0 is the statistic\n",
            "        -5.521109549096542\n",
            "        >>> print(res.confidence_interval)\n",
            "        ConfidenceInterval(low=-7.255994487314675, high=-4.016202624747605)\n",
            "        \n",
            "        The bootstrap estimate of the standard error is also available.\n",
            "        \n",
            "        >>> print(res.standard_error)\n",
            "        0.8344963846318795\n",
            "        \n",
            "        Paired-sample statistics work, too. For example, consider the Pearson\n",
            "        correlation coefficient.\n",
            "        \n",
            "        >>> from scipy.stats import pearsonr\n",
            "        >>> n = 100\n",
            "        >>> x = np.linspace(0, 10, n)\n",
            "        >>> y = x + rng.uniform(size=n)\n",
            "        >>> print(pearsonr(x, y)[0])  # element 0 is the statistic\n",
            "        0.9962357936065914\n",
            "        \n",
            "        We wrap `pearsonr` so that it returns only the statistic.\n",
            "        \n",
            "        >>> def my_statistic(x, y):\n",
            "        ...     return pearsonr(x, y)[0]\n",
            "        \n",
            "        We call `bootstrap` using ``paired=True``.\n",
            "        Also, since ``my_statistic`` isn't vectorized to calculate the statistic\n",
            "        along a given axis, we pass in ``vectorized=False``.\n",
            "        \n",
            "        >>> res = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
            "        ...                 random_state=rng)\n",
            "        >>> print(res.confidence_interval)\n",
            "        ConfidenceInterval(low=0.9950085825848624, high=0.9971212407917498)\n",
            "        \n",
            "        The result object can be passed back into `bootstrap` to perform additional\n",
            "        resampling:\n",
            "        \n",
            "        >>> len(res.bootstrap_distribution)\n",
            "        9999\n",
            "        >>> res = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
            "        ...                 n_resamples=1001, random_state=rng,\n",
            "        ...                 bootstrap_result=res)\n",
            "        >>> len(res.bootstrap_distribution)\n",
            "        11000\n",
            "        \n",
            "        or to change the confidence interval options:\n",
            "        \n",
            "        >>> res2 = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
            "        ...                  n_resamples=0, random_state=rng, bootstrap_result=res,\n",
            "        ...                  method='percentile', confidence_level=0.9)\n",
            "        >>> np.testing.assert_equal(res2.bootstrap_distribution,\n",
            "        ...                         res.bootstrap_distribution)\n",
            "        >>> res.confidence_interval\n",
            "        ConfidenceInterval(low=0.9950035351407804, high=0.9971170323404578)\n",
            "        \n",
            "        without repeating computation of the original bootstrap distribution.\n",
            "    \n",
            "    boschloo_exact(table, alternative='two-sided', n=32)\n",
            "        Perform Boschloo's exact test on a 2x2 contingency table.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        table : array_like of ints\n",
            "            A 2x2 contingency table.  Elements should be non-negative integers.\n",
            "        \n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
            "            Please see explanations in the Notes section below.\n",
            "        \n",
            "        n : int, optional\n",
            "            Number of sampling points used in the construction of the sampling\n",
            "            method. Note that this argument will automatically be converted to\n",
            "            the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n",
            "            select sample points. Default is 32. Must be positive. In most cases,\n",
            "            32 points is enough to reach good precision. More points comes at\n",
            "            performance cost.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        ber : BoschlooExactResult\n",
            "            A result object with the following attributes.\n",
            "        \n",
            "            statistic : float\n",
            "                The statistic used in Boschloo's test; that is, the p-value\n",
            "                from Fisher's exact test.\n",
            "        \n",
            "            pvalue : float\n",
            "                P-value, the probability of obtaining a distribution at least as\n",
            "                extreme as the one that was actually observed, assuming that the\n",
            "                null hypothesis is true.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        chi2_contingency : Chi-square test of independence of variables in a\n",
            "            contingency table.\n",
            "        fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
            "        barnard_exact : Barnard's exact test, which is a more powerful alternative\n",
            "            than Fisher's exact test for 2x2 contingency tables.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Boschloo's test is an exact test used in the analysis of contingency\n",
            "        tables. It examines the association of two categorical variables, and\n",
            "        is a uniformly more powerful alternative to Fisher's exact test\n",
            "        for 2x2 contingency tables.\n",
            "        \n",
            "        Boschloo's exact test uses the p-value of Fisher's exact test as a\n",
            "        statistic, and Boschloo's p-value is the probability under the null\n",
            "        hypothesis of observing such an extreme value of this statistic.\n",
            "        \n",
            "        Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n",
            "        where each column stores the binomial experiment, as in the example\n",
            "        below. Let's also define :math:`p_1, p_2` the theoretical binomial\n",
            "        probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n",
            "        Boschloo exact test, we can assert three different alternative hypotheses:\n",
            "        \n",
            "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 < p_2`,\n",
            "          with `alternative` = \"less\"\n",
            "        \n",
            "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 > p_2`,\n",
            "          with `alternative` = \"greater\"\n",
            "        \n",
            "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n",
            "          with `alternative` = \"two-sided\" (default)\n",
            "        \n",
            "        There are multiple conventions for computing a two-sided p-value when the\n",
            "        null distribution is asymmetric. Here, we apply the convention that the\n",
            "        p-value of a two-sided test is twice the minimum of the p-values of the\n",
            "        one-sided tests (clipped to 1.0). Note that `fisher_exact` follows a\n",
            "        different convention, so for a given `table`, the statistic reported by\n",
            "        `boschloo_exact` may differ from the p-value reported by `fisher_exact`\n",
            "        when ``alternative='two-sided'``.\n",
            "        \n",
            "        .. versionadded:: 1.7.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] R.D. Boschloo. \"Raised conditional level of significance for the\n",
            "           2 x 2-table when testing the equality of two probabilities\",\n",
            "           Statistica Neerlandica, 24(1), 1970\n",
            "        \n",
            "        .. [2] \"Boschloo's test\", Wikipedia,\n",
            "           https://en.wikipedia.org/wiki/Boschloo%27s_test\n",
            "        \n",
            "        .. [3] Lise M. Saari et al. \"Employee attitudes and job satisfaction\",\n",
            "           Human Resource Management, 43(4), 395-407, 2004,\n",
            "           :doi:`10.1002/hrm.20032`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In the following example, we consider the article \"Employee\n",
            "        attitudes and job satisfaction\" [3]_\n",
            "        which reports the results of a survey from 63 scientists and 117 college\n",
            "        professors. Of the 63 scientists, 31 said they were very satisfied with\n",
            "        their jobs, whereas 74 of the college professors were very satisfied\n",
            "        with their work. Is this significant evidence that college\n",
            "        professors are happier with their work than scientists?\n",
            "        The following table summarizes the data mentioned above::\n",
            "        \n",
            "                             college professors   scientists\n",
            "            Very Satisfied   74                     31\n",
            "            Dissatisfied     43                     32\n",
            "        \n",
            "        When working with statistical hypothesis testing, we usually use a\n",
            "        threshold probability or significance level upon which we decide\n",
            "        to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n",
            "        significance level of 5%.\n",
            "        \n",
            "        Our alternative hypothesis is that college professors are truly more\n",
            "        satisfied with their work than scientists. Therefore, we expect\n",
            "        :math:`p_1` the proportion of very satisfied college professors to be\n",
            "        greater than :math:`p_2`, the proportion of very satisfied scientists.\n",
            "        We thus call `boschloo_exact` with the ``alternative=\"greater\"`` option:\n",
            "        \n",
            "        >>> import scipy.stats as stats\n",
            "        >>> res = stats.boschloo_exact([[74, 31], [43, 32]], alternative=\"greater\")\n",
            "        >>> res.statistic\n",
            "        0.0483...\n",
            "        >>> res.pvalue\n",
            "        0.0355...\n",
            "        \n",
            "        Under the null hypothesis that scientists are happier in their work than\n",
            "        college professors, the probability of obtaining test\n",
            "        results at least as extreme as the observed data is approximately 3.55%.\n",
            "        Since this p-value is less than our chosen significance level, we have\n",
            "        evidence to reject :math:`H_0` in favor of the alternative hypothesis.\n",
            "    \n",
            "    boxcox(x, lmbda=None, alpha=None, optimizer=None)\n",
            "        Return a dataset transformed by a Box-Cox power transformation.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : ndarray\n",
            "            Input array to be transformed.\n",
            "        \n",
            "            If `lmbda` is not None, this is an alias of\n",
            "            `scipy.special.boxcox`.\n",
            "            Returns nan if ``x < 0``; returns -inf if ``x == 0 and lmbda < 0``.\n",
            "        \n",
            "            If `lmbda` is None, array must be positive, 1-dimensional, and\n",
            "            non-constant.\n",
            "        \n",
            "        lmbda : scalar, optional\n",
            "            If `lmbda` is None (default), find the value of `lmbda` that maximizes\n",
            "            the log-likelihood function and return it as the second output\n",
            "            argument.\n",
            "        \n",
            "            If `lmbda` is not None, do the transformation for that value.\n",
            "        \n",
            "        alpha : float, optional\n",
            "            If `lmbda` is None and `alpha` is not None (default), return the\n",
            "            ``100 * (1-alpha)%`` confidence  interval for `lmbda` as the third\n",
            "            output argument. Must be between 0.0 and 1.0.\n",
            "        \n",
            "            If `lmbda` is not None, `alpha` is ignored.\n",
            "        optimizer : callable, optional\n",
            "            If `lmbda` is None, `optimizer` is the scalar optimizer used to find\n",
            "            the value of `lmbda` that minimizes the negative log-likelihood\n",
            "            function. `optimizer` is a callable that accepts one argument:\n",
            "        \n",
            "            fun : callable\n",
            "                The objective function, which evaluates the negative\n",
            "                log-likelihood function at a provided value of `lmbda`\n",
            "        \n",
            "            and returns an object, such as an instance of\n",
            "            `scipy.optimize.OptimizeResult`, which holds the optimal value of\n",
            "            `lmbda` in an attribute `x`.\n",
            "        \n",
            "            See the example in `boxcox_normmax` or the documentation of\n",
            "            `scipy.optimize.minimize_scalar` for more information.\n",
            "        \n",
            "            If `lmbda` is not None, `optimizer` is ignored.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        boxcox : ndarray\n",
            "            Box-Cox power transformed array.\n",
            "        maxlog : float, optional\n",
            "            If the `lmbda` parameter is None, the second returned argument is\n",
            "            the `lmbda` that maximizes the log-likelihood function.\n",
            "        (min_ci, max_ci) : tuple of float, optional\n",
            "            If `lmbda` parameter is None and `alpha` is not None, this returned\n",
            "            tuple of floats represents the minimum and maximum confidence limits\n",
            "            given `alpha`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        probplot, boxcox_normplot, boxcox_normmax, boxcox_llf\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Box-Cox transform is given by::\n",
            "        \n",
            "            y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
            "                log(x),                  for lmbda = 0\n",
            "        \n",
            "        `boxcox` requires the input data to be positive.  Sometimes a Box-Cox\n",
            "        transformation provides a shift parameter to achieve this; `boxcox` does\n",
            "        not.  Such a shift parameter is equivalent to adding a positive constant to\n",
            "        `x` before calling `boxcox`.\n",
            "        \n",
            "        The confidence limits returned when `alpha` is provided give the interval\n",
            "        where:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            llf(\\hat{\\lambda}) - llf(\\lambda) < \\frac{1}{2}\\chi^2(1 - \\alpha, 1),\n",
            "        \n",
            "        with ``llf`` the log-likelihood function and :math:`\\chi^2` the chi-squared\n",
            "        function.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the\n",
            "        Royal Statistical Society B, 26, 211-252 (1964).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        We generate some random variates from a non-normal distribution and make a\n",
            "        probability plot for it, to show it is non-normal in the tails:\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax1 = fig.add_subplot(211)\n",
            "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
            "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
            "        >>> ax1.set_xlabel('')\n",
            "        >>> ax1.set_title('Probplot against normal distribution')\n",
            "        \n",
            "        We now use `boxcox` to transform the data so it's closest to normal:\n",
            "        \n",
            "        >>> ax2 = fig.add_subplot(212)\n",
            "        >>> xt, _ = stats.boxcox(x)\n",
            "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
            "        >>> ax2.set_title('Probplot after Box-Cox transformation')\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    boxcox_llf(lmb, data)\n",
            "        The boxcox log-likelihood function.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        lmb : scalar\n",
            "            Parameter for Box-Cox transformation.  See `boxcox` for details.\n",
            "        data : array_like\n",
            "            Data to calculate Box-Cox log-likelihood for.  If `data` is\n",
            "            multi-dimensional, the log-likelihood is calculated along the first\n",
            "            axis.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        llf : float or ndarray\n",
            "            Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n",
            "            an array otherwise.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        boxcox, probplot, boxcox_normplot, boxcox_normmax\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Box-Cox log-likelihood function is defined here as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n",
            "                  N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n",
            "        \n",
            "        where ``y`` is the Box-Cox transformed input data ``x``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
            "        \n",
            "        Generate some random variates and calculate Box-Cox log-likelihood values\n",
            "        for them for a range of ``lmbda`` values:\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000, random_state=rng)\n",
            "        >>> lmbdas = np.linspace(-2, 10)\n",
            "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
            "        >>> for ii, lmbda in enumerate(lmbdas):\n",
            "        ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n",
            "        \n",
            "        Also find the optimal lmbda value with `boxcox`:\n",
            "        \n",
            "        >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n",
            "        \n",
            "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
            "        horizontal line to check that that's really the optimum:\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
            "        >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n",
            "        >>> ax.set_xlabel('lmbda parameter')\n",
            "        >>> ax.set_ylabel('Box-Cox log-likelihood')\n",
            "        \n",
            "        Now add some probability plots to show that where the log-likelihood is\n",
            "        maximized the data transformed with `boxcox` looks closest to normal:\n",
            "        \n",
            "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
            "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
            "        ...     xt = stats.boxcox(x, lmbda=lmbda)\n",
            "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
            "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
            "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
            "        ...     ax_inset.set_xticklabels([])\n",
            "        ...     ax_inset.set_yticklabels([])\n",
            "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    boxcox_normmax(x, brack=None, method='pearsonr', optimizer=None, *, ymax=BIG_FLOAT)\n",
            "        Compute optimal Box-Cox transform parameter for input data.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array. All entries must be positive, finite, real numbers.\n",
            "        brack : 2-tuple, optional, default (-2.0, 2.0)\n",
            "             The starting interval for a downhill bracket search for the default\n",
            "             `optimize.brent` solver. Note that this is in most cases not\n",
            "             critical; the final result is allowed to be outside this bracket.\n",
            "             If `optimizer` is passed, `brack` must be None.\n",
            "        method : str, optional\n",
            "            The method to determine the optimal transform parameter (`boxcox`\n",
            "            ``lmbda`` parameter). Options are:\n",
            "        \n",
            "            'pearsonr'  (default)\n",
            "                Maximizes the Pearson correlation coefficient between\n",
            "                ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be\n",
            "                normally-distributed.\n",
            "        \n",
            "            'mle'\n",
            "                Maximizes the log-likelihood `boxcox_llf`.  This is the method used\n",
            "                in `boxcox`.\n",
            "        \n",
            "            'all'\n",
            "                Use all optimization methods available, and return all results.\n",
            "                Useful to compare different methods.\n",
            "        optimizer : callable, optional\n",
            "            `optimizer` is a callable that accepts one argument:\n",
            "        \n",
            "            fun : callable\n",
            "                The objective function to be minimized. `fun` accepts one argument,\n",
            "                the Box-Cox transform parameter `lmbda`, and returns the value of\n",
            "                the function (e.g., the negative log-likelihood) at the provided\n",
            "                argument. The job of `optimizer` is to find the value of `lmbda`\n",
            "                that *minimizes* `fun`.\n",
            "        \n",
            "            and returns an object, such as an instance of\n",
            "            `scipy.optimize.OptimizeResult`, which holds the optimal value of\n",
            "            `lmbda` in an attribute `x`.\n",
            "        \n",
            "            See the example below or the documentation of\n",
            "            `scipy.optimize.minimize_scalar` for more information.\n",
            "        ymax : float, optional\n",
            "            The unconstrained optimal transform parameter may cause Box-Cox\n",
            "            transformed data to have extreme magnitude or even overflow.\n",
            "            This parameter constrains MLE optimization such that the magnitude\n",
            "            of the transformed `x` does not exceed `ymax`. The default is\n",
            "            the maximum value of the input dtype. If set to infinity,\n",
            "            `boxcox_normmax` returns the unconstrained optimal lambda.\n",
            "            Ignored when ``method='pearsonr'``.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        maxlog : float or ndarray\n",
            "            The optimal transform parameter found.  An array instead of a scalar\n",
            "            for ``method='all'``.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        boxcox, boxcox_llf, boxcox_normplot, scipy.optimize.minimize_scalar\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        We can generate some data and determine the optimal ``lmbda`` in various\n",
            "        ways:\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n",
            "        >>> y, lmax_mle = stats.boxcox(x)\n",
            "        >>> lmax_pearsonr = stats.boxcox_normmax(x)\n",
            "        \n",
            "        >>> lmax_mle\n",
            "        2.217563431465757\n",
            "        >>> lmax_pearsonr\n",
            "        2.238318660200961\n",
            "        >>> stats.boxcox_normmax(x, method='all')\n",
            "        array([2.23831866, 2.21756343])\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)\n",
            "        >>> ax.axvline(lmax_mle, color='r')\n",
            "        >>> ax.axvline(lmax_pearsonr, color='g', ls='--')\n",
            "        \n",
            "        >>> plt.show()\n",
            "        \n",
            "        Alternatively, we can define our own `optimizer` function. Suppose we\n",
            "        are only interested in values of `lmbda` on the interval [6, 7], we\n",
            "        want to use `scipy.optimize.minimize_scalar` with ``method='bounded'``,\n",
            "        and we want to use tighter tolerances when optimizing the log-likelihood\n",
            "        function. To do this, we define a function that accepts positional argument\n",
            "        `fun` and uses `scipy.optimize.minimize_scalar` to minimize `fun` subject\n",
            "        to the provided bounds and tolerances:\n",
            "        \n",
            "        >>> from scipy import optimize\n",
            "        >>> options = {'xatol': 1e-12}  # absolute tolerance on `x`\n",
            "        >>> def optimizer(fun):\n",
            "        ...     return optimize.minimize_scalar(fun, bounds=(6, 7),\n",
            "        ...                                     method=\"bounded\", options=options)\n",
            "        >>> stats.boxcox_normmax(x, optimizer=optimizer)\n",
            "        6.000...\n",
            "    \n",
            "    boxcox_normplot(x, la, lb, plot=None, N=80)\n",
            "        Compute parameters for a Box-Cox normality plot, optionally show it.\n",
            "        \n",
            "        A Box-Cox normality plot shows graphically what the best transformation\n",
            "        parameter is to use in `boxcox` to obtain a distribution that is close\n",
            "        to normal.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array.\n",
            "        la, lb : scalar\n",
            "            The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n",
            "            for Box-Cox transformations.  These are also the limits of the\n",
            "            horizontal axis of the plot if that is generated.\n",
            "        plot : object, optional\n",
            "            If given, plots the quantiles and least squares fit.\n",
            "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
            "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
            "            or a custom object with the same methods.\n",
            "            Default is None, which means that no plot is created.\n",
            "        N : int, optional\n",
            "            Number of points on the horizontal axis (equally distributed from\n",
            "            `la` to `lb`).\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        lmbdas : ndarray\n",
            "            The ``lmbda`` values for which a Box-Cox transform was done.\n",
            "        ppcc : ndarray\n",
            "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
            "            when fitting the Box-Cox transformed input `x` against a normal\n",
            "            distribution.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Even if `plot` is given, the figure is not shown or saved by\n",
            "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
            "        should be used after calling `probplot`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        Generate some non-normally distributed data, and create a Box-Cox plot:\n",
            "        \n",
            "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n",
            "        \n",
            "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
            "        the same plot:\n",
            "        \n",
            "        >>> _, maxlog = stats.boxcox(x)\n",
            "        >>> ax.axvline(maxlog, color='r')\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    brunnermunzel(x, y, alternative='two-sided', distribution='t', nan_policy='propagate', *, axis=0, keepdims=False)\n",
            "        Compute the Brunner-Munzel test on samples x and y.\n",
            "        \n",
            "        The Brunner-Munzel test is a nonparametric test of the null hypothesis that\n",
            "        when values are taken one by one from each group, the probabilities of\n",
            "        getting large values in both groups are equal.\n",
            "        Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the\n",
            "        assumption of equivariance of two groups. Note that this does not assume\n",
            "        the distributions are same. This test works on two independent samples,\n",
            "        which may have different sizes.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Array of samples, should be one-dimensional.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "            \n",
            "              * 'two-sided'\n",
            "              * 'less': one-sided\n",
            "              * 'greater': one-sided\n",
            "        distribution : {'t', 'normal'}, optional\n",
            "            Defines how to get the p-value.\n",
            "            The following options are available (default is 't'):\n",
            "            \n",
            "              * 't': get the p-value by t-distribution\n",
            "              * 'normal': get the p-value by standard normal distribution.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The Brunner-Munzer W statistic.\n",
            "        pvalue : float\n",
            "            p-value assuming an t distribution. One-sided or\n",
            "            two-sided, depending on the choice of `alternative` and `distribution`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`mannwhitneyu`\n",
            "            Mann-Whitney rank test on two samples.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Brunner and Munzel recommended to estimate the p-value by t-distribution\n",
            "        when the size of data is 50 or less. If the size is lower than 10, it would\n",
            "        be better to use permuted Brunner Munzel test (see [2]_).\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Brunner, E. and Munzel, U. \"The nonparametric Benhrens-Fisher\n",
            "               problem: Asymptotic theory and a small-sample approximation\".\n",
            "               Biometrical Journal. Vol. 42(2000): 17-25.\n",
            "        .. [2] Neubert, K. and Brunner, E. \"A studentized permutation test for the\n",
            "               non-parametric Behrens-Fisher problem\". Computational Statistics and\n",
            "               Data Analysis. Vol. 51(2007): 5192-5204.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]\n",
            "        >>> x2 = [3,3,4,3,1,2,3,1,1,5,4]\n",
            "        >>> w, p_value = stats.brunnermunzel(x1, x2)\n",
            "        >>> w\n",
            "        3.1374674823029505\n",
            "        >>> p_value\n",
            "        0.0057862086661515377\n",
            "    \n",
            "    bws_test(x, y, *, alternative='two-sided', method=None)\n",
            "        Perform the Baumgartner-Weiss-Schindler test on two independent samples.\n",
            "        \n",
            "        The Baumgartner-Weiss-Schindler (BWS) test is a nonparametric test of \n",
            "        the null hypothesis that the distribution underlying sample `x` \n",
            "        is the same as the distribution underlying sample `y`. Unlike \n",
            "        the Kolmogorov-Smirnov, Wilcoxon, and Cramer-Von Mises tests, \n",
            "        the BWS test weights the integral by the variance of the difference\n",
            "        in cumulative distribution functions (CDFs), emphasizing the tails of the\n",
            "        distributions, which increases the power of the test in many applications.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array-like\n",
            "            1-d arrays of samples.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            Let *F(u)* and *G(u)* be the cumulative distribution functions of the\n",
            "            distributions underlying `x` and `y`, respectively. Then the following\n",
            "            alternative hypotheses are available:\n",
            "        \n",
            "            * 'two-sided': the distributions are not equal, i.e. *F(u)  G(u)* for\n",
            "              at least one *u*.\n",
            "            * 'less': the distribution underlying `x` is stochastically less than\n",
            "              the distribution underlying `y`, i.e. *F(u) >= G(u)* for all *u*.\n",
            "            * 'greater': the distribution underlying `x` is stochastically greater\n",
            "              than the distribution underlying `y`, i.e. *F(u) <= G(u)* for all\n",
            "              *u*.\n",
            "        \n",
            "            Under a more restrictive set of assumptions, the alternative hypotheses\n",
            "            can be expressed in terms of the locations of the distributions;\n",
            "            see [2] section 5.1.\n",
            "        method : PermutationMethod, optional\n",
            "            Configures the method used to compute the p-value. The default is\n",
            "            the default `PermutationMethod` object.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : PermutationTestResult\n",
            "        An object with attributes:\n",
            "        \n",
            "        statistic : float\n",
            "            The observed test statistic of the data.\n",
            "        pvalue : float\n",
            "            The p-value for the given alternative.\n",
            "        null_distribution : ndarray\n",
            "            The values of the test statistic generated under the null hypothesis.\n",
            "        \n",
            "        See also\n",
            "        --------\n",
            "        scipy.stats.wilcoxon, scipy.stats.mannwhitneyu, scipy.stats.ttest_ind\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        When ``alternative=='two-sided'``, the statistic is defined by the\n",
            "        equations given in [1]_ Section 2. This statistic is not appropriate for\n",
            "        one-sided alternatives; in that case, the statistic is the *negative* of\n",
            "        that given by the equations in [1]_ Section 2. Consequently, when the\n",
            "        distribution of the first sample is stochastically greater than that of the\n",
            "        second sample, the statistic will tend to be positive.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Neuhuser, M. (2005). Exact Tests Based on the\n",
            "               Baumgartner-Weiss-Schindler Statistic: A Survey. Statistical Papers,\n",
            "               46(1), 1-29.\n",
            "        .. [2] Fay, M. P., & Proschan, M. A. (2010). Wilcoxon-Mann-Whitney or t-test?\n",
            "               On assumptions for hypothesis tests and multiple interpretations of \n",
            "               decision rules. Statistics surveys, 4, 1.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We follow the example of table 3 in [1]_: Fourteen children were divided\n",
            "        randomly into two groups. Their ranks at performing a specific tests are\n",
            "        as follows.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = [1, 2, 3, 4, 6, 7, 8]\n",
            "        >>> y = [5, 9, 10, 11, 12, 13, 14]\n",
            "        \n",
            "        We use the BWS test to assess whether there is a statistically significant\n",
            "        difference between the two groups.\n",
            "        The null hypothesis is that there is no difference in the distributions of\n",
            "        performance between the two groups. We decide that a significance level of\n",
            "        1% is required to reject the null hypothesis in favor of the alternative\n",
            "        that the distributions are different.\n",
            "        Since the number of samples is very small, we can compare the observed test\n",
            "        statistic against the *exact* distribution of the test statistic under the\n",
            "        null hypothesis.\n",
            "        \n",
            "        >>> from scipy.stats import bws_test\n",
            "        >>> res = bws_test(x, y)\n",
            "        >>> print(res.statistic)\n",
            "        5.132167152575315\n",
            "        \n",
            "        This agrees with :math:`B = 5.132` reported in [1]_. The *p*-value produced\n",
            "        by `bws_test` also agrees with :math:`p = 0.0029` reported in [1]_.\n",
            "        \n",
            "        >>> print(res.pvalue)\n",
            "        0.002913752913752914\n",
            "        \n",
            "        Because the p-value is below our threshold of 1%, we take this as evidence\n",
            "        against the null hypothesis in favor of the alternative that there is a\n",
            "        difference in performance between the two groups.\n",
            "    \n",
            "    chi2_contingency(observed, correction=True, lambda_=None)\n",
            "        Chi-square test of independence of variables in a contingency table.\n",
            "        \n",
            "        This function computes the chi-square statistic and p-value for the\n",
            "        hypothesis test of independence of the observed frequencies in the\n",
            "        contingency table [1]_ `observed`.  The expected frequencies are computed\n",
            "        based on the marginal sums under the assumption of independence; see\n",
            "        `scipy.stats.contingency.expected_freq`.  The number of degrees of\n",
            "        freedom is (expressed using numpy functions and attributes)::\n",
            "        \n",
            "            dof = observed.size - sum(observed.shape) + observed.ndim - 1\n",
            "        \n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        observed : array_like\n",
            "            The contingency table. The table contains the observed frequencies\n",
            "            (i.e. number of occurrences) in each category.  In the two-dimensional\n",
            "            case, the table is often described as an \"R x C table\".\n",
            "        correction : bool, optional\n",
            "            If True, *and* the degrees of freedom is 1, apply Yates' correction\n",
            "            for continuity.  The effect of the correction is to adjust each\n",
            "            observed value by 0.5 towards the corresponding expected value.\n",
            "        lambda_ : float or str, optional\n",
            "            By default, the statistic computed in this test is Pearson's\n",
            "            chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n",
            "            Cressie-Read power divergence family [3]_ to be used instead.  See\n",
            "            `scipy.stats.power_divergence` for details.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : Chi2ContingencyResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                The test statistic.\n",
            "            pvalue : float\n",
            "                The p-value of the test.\n",
            "            dof : int\n",
            "                The degrees of freedom.\n",
            "            expected_freq : ndarray, same shape as `observed`\n",
            "                The expected frequencies, based on the marginal sums of the table.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.contingency.expected_freq\n",
            "        scipy.stats.fisher_exact\n",
            "        scipy.stats.chisquare\n",
            "        scipy.stats.power_divergence\n",
            "        scipy.stats.barnard_exact\n",
            "        scipy.stats.boschloo_exact\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        An often quoted guideline for the validity of this calculation is that\n",
            "        the test should be used only if the observed and expected frequencies\n",
            "        in each cell are at least 5.\n",
            "        \n",
            "        This is a test for the independence of different categories of a\n",
            "        population. The test is only meaningful when the dimension of\n",
            "        `observed` is two or more.  Applying the test to a one-dimensional\n",
            "        table will always result in `expected` equal to `observed` and a\n",
            "        chi-square statistic equal to 0.\n",
            "        \n",
            "        This function does not handle masked arrays, because the calculation\n",
            "        does not make sense with missing values.\n",
            "        \n",
            "        Like `scipy.stats.chisquare`, this function computes a chi-square\n",
            "        statistic; the convenience this function provides is to figure out the\n",
            "        expected frequencies and degrees of freedom from the given contingency\n",
            "        table. If these were already known, and if the Yates' correction was not\n",
            "        required, one could use `scipy.stats.chisquare`.  That is, if one calls::\n",
            "        \n",
            "            res = chi2_contingency(obs, correction=False)\n",
            "        \n",
            "        then the following is true::\n",
            "        \n",
            "            (res.statistic, res.pvalue) == stats.chisquare(obs.ravel(),\n",
            "                                                           f_exp=ex.ravel(),\n",
            "                                                           ddof=obs.size - 1 - dof)\n",
            "        \n",
            "        The `lambda_` argument was added in version 0.13.0 of scipy.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Contingency table\",\n",
            "               https://en.wikipedia.org/wiki/Contingency_table\n",
            "        .. [2] \"Pearson's chi-squared test\",\n",
            "               https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
            "        .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
            "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
            "               pp. 440-464.\n",
            "        .. [4] Berger, Jeffrey S. et al. \"Aspirin for the Primary Prevention of\n",
            "               Cardiovascular Events in Women and Men: A Sex-Specific\n",
            "               Meta-analysis of Randomized Controlled Trials.\"\n",
            "               JAMA, 295(3):306-313, :doi:`10.1001/jama.295.3.306`, 2006.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [4]_, the use of aspirin to prevent cardiovascular events in women\n",
            "        and men was investigated. The study notably concluded:\n",
            "        \n",
            "            ...aspirin therapy reduced the risk of a composite of\n",
            "            cardiovascular events due to its effect on reducing the risk of\n",
            "            ischemic stroke in women [...]\n",
            "        \n",
            "        The article lists studies of various cardiovascular events. Let's\n",
            "        focus on the ischemic stoke in women.\n",
            "        \n",
            "        The following table summarizes the results of the experiment in which\n",
            "        participants took aspirin or a placebo on a regular basis for several\n",
            "        years. Cases of ischemic stroke were recorded::\n",
            "        \n",
            "                              Aspirin   Control/Placebo\n",
            "            Ischemic stroke     176           230\n",
            "            No stroke         21035         21018\n",
            "        \n",
            "        Is there evidence that the aspirin reduces the risk of ischemic stroke?\n",
            "        We begin by formulating a null hypothesis :math:`H_0`:\n",
            "        \n",
            "            The effect of aspirin is equivalent to that of placebo.\n",
            "        \n",
            "        Let's assess the plausibility of this hypothesis with\n",
            "        a chi-square test.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import chi2_contingency\n",
            "        >>> table = np.array([[176, 230], [21035, 21018]])\n",
            "        >>> res = chi2_contingency(table)\n",
            "        >>> res.statistic\n",
            "        6.892569132546561\n",
            "        >>> res.pvalue\n",
            "        0.008655478161175739\n",
            "        \n",
            "        Using a significance level of 5%, we would reject the null hypothesis in\n",
            "        favor of the alternative hypothesis: \"the effect of aspirin\n",
            "        is not equivalent to the effect of placebo\".\n",
            "        Because `scipy.stats.contingency.chi2_contingency` performs a two-sided\n",
            "        test, the alternative hypothesis does not indicate the direction of the\n",
            "        effect. We can use `stats.contingency.odds_ratio` to support the\n",
            "        conclusion that aspirin *reduces* the risk of ischemic stroke.\n",
            "        \n",
            "        Below are further examples showing how larger contingency tables can be\n",
            "        tested.\n",
            "        \n",
            "        A two-way example (2 x 3):\n",
            "        \n",
            "        >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n",
            "        >>> res = chi2_contingency(obs)\n",
            "        >>> res.statistic\n",
            "        2.7777777777777777\n",
            "        >>> res.pvalue\n",
            "        0.24935220877729619\n",
            "        >>> res.dof\n",
            "        2\n",
            "        >>> res.expected_freq\n",
            "        array([[ 12.,  12.,  16.],\n",
            "               [ 18.,  18.,  24.]])\n",
            "        \n",
            "        Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n",
            "        instead of Pearson's chi-squared statistic.\n",
            "        \n",
            "        >>> res = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
            "        >>> res.statistic\n",
            "        2.7688587616781319\n",
            "        >>> res.pvalue\n",
            "        0.25046668010954165\n",
            "        \n",
            "        A four-way example (2 x 2 x 2 x 2):\n",
            "        \n",
            "        >>> obs = np.array(\n",
            "        ...     [[[[12, 17],\n",
            "        ...        [11, 16]],\n",
            "        ...       [[11, 12],\n",
            "        ...        [15, 16]]],\n",
            "        ...      [[[23, 15],\n",
            "        ...        [30, 22]],\n",
            "        ...       [[14, 17],\n",
            "        ...        [15, 16]]]])\n",
            "        >>> res = chi2_contingency(obs)\n",
            "        >>> res.statistic\n",
            "        8.7584514426741897\n",
            "        >>> res.pvalue\n",
            "        0.64417725029295503\n",
            "    \n",
            "    chisquare(f_obs, f_exp=None, ddof=0, axis=0)\n",
            "        Calculate a one-way chi-square test.\n",
            "        \n",
            "        The chi-square test tests the null hypothesis that the categorical data\n",
            "        has the given frequencies.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        f_obs : array_like\n",
            "            Observed frequencies in each category.\n",
            "        f_exp : array_like, optional\n",
            "            Expected frequencies in each category.  By default the categories are\n",
            "            assumed to be equally likely.\n",
            "        ddof : int, optional\n",
            "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
            "            for the p-value.  The p-value is computed using a chi-squared\n",
            "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
            "            is the number of observed frequencies.  The default value of `ddof`\n",
            "            is 0.\n",
            "        axis : int or None, optional\n",
            "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
            "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
            "            as a single data set.  Default is 0.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: Power_divergenceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float or ndarray\n",
            "                The chi-squared test statistic.  The value is a float if `axis` is\n",
            "                None or `f_obs` and `f_exp` are 1-D.\n",
            "            pvalue : float or ndarray\n",
            "                The p-value of the test.  The value is a float if `ddof` and the\n",
            "                result attribute `statistic` are scalars.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.power_divergence\n",
            "        scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
            "        scipy.stats.barnard_exact : An unconditional exact test. An alternative\n",
            "            to chi-squared test for small sample sizes.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This test is invalid when the observed or expected frequencies in each\n",
            "        category are too small.  A typical rule is that all of the observed\n",
            "        and expected frequencies should be at least 5. According to [3]_, the\n",
            "        total number of samples is recommended to be greater than 13,\n",
            "        otherwise exact tests (such as Barnard's Exact test) should be used\n",
            "        because they do not overreject.\n",
            "        \n",
            "        Also, the sum of the observed and expected frequencies must be the same\n",
            "        for the test to be valid; `chisquare` raises an error if the sums do not\n",
            "        agree within a relative tolerance of ``1e-8``.\n",
            "        \n",
            "        The default degrees of freedom, k-1, are for the case when no parameters\n",
            "        of the distribution are estimated. If p parameters are estimated by\n",
            "        efficient maximum likelihood then the correct degrees of freedom are\n",
            "        k-1-p. If the parameters are estimated in a different way, then the\n",
            "        dof can be between k-1-p and k-1. However, it is also possible that\n",
            "        the asymptotic distribution is not chi-square, in which case this test\n",
            "        is not appropriate.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
            "               Statistics\". Chapter 8.\n",
            "               https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n",
            "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
            "        .. [3] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n",
            "               in the case of a correlated system of variables is such that it can be reasonably\n",
            "               supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n",
            "               (1900), pp. 157-175.\n",
            "        .. [4] Mannan, R. William and E. Charles. Meslow. \"Bird populations and\n",
            "               vegetation characteristics in managed and old-growth forests,\n",
            "               northeastern Oregon.\" Journal of Wildlife Management\n",
            "               48, 1219-1238, :doi:`10.2307/3801783`, 1984.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [4]_, bird foraging behavior was investigated in an old-growth forest\n",
            "        of Oregon.\n",
            "        In the forest, 44% of the canopy volume was Douglas fir,\n",
            "        24% was ponderosa pine, 29% was grand fir, and 3% was western larch.\n",
            "        The authors observed the behavior of several species of birds, one of\n",
            "        which was the red-breasted nuthatch. They made 189 observations of this\n",
            "        species foraging, recording 43 (\"23%\") of observations in Douglas fir,\n",
            "        52 (\"28%\") in ponderosa pine, 54 (\"29%\") in grand fir, and 40 (\"21%\") in\n",
            "        western larch.\n",
            "        \n",
            "        Using a chi-square test, we can test the null hypothesis that the\n",
            "        proportions of foraging events are equal to the proportions of canopy\n",
            "        volume. The authors of the paper considered a p-value less than 1% to be\n",
            "        significant.\n",
            "        \n",
            "        Using the above proportions of canopy volume and observed events, we can\n",
            "        infer expected frequencies.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> f_exp = np.array([44, 24, 29, 3]) / 100 * 189\n",
            "        \n",
            "        The observed frequencies of foraging were:\n",
            "        \n",
            "        >>> f_obs = np.array([43, 52, 54, 40])\n",
            "        \n",
            "        We can now compare the observed frequencies with the expected frequencies.\n",
            "        \n",
            "        >>> from scipy.stats import chisquare\n",
            "        >>> chisquare(f_obs=f_obs, f_exp=f_exp)\n",
            "        Power_divergenceResult(statistic=228.23515947653874, pvalue=3.3295585338846486e-49)\n",
            "        \n",
            "        The p-value is well below the chosen significance level. Hence, the\n",
            "        authors considered the difference to be significant and concluded\n",
            "        that the relative proportions of foraging events were not the same\n",
            "        as the relative proportions of tree canopy volume.\n",
            "        \n",
            "        Following are other generic examples to demonstrate how the other\n",
            "        parameters can be used.\n",
            "        \n",
            "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
            "        are uniform and given by the mean of the observed frequencies.\n",
            "        \n",
            "        >>> chisquare([16, 18, 16, 14, 12, 12])\n",
            "        Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n",
            "        \n",
            "        With `f_exp` the expected frequencies can be given.\n",
            "        \n",
            "        >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n",
            "        Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n",
            "        \n",
            "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
            "        \n",
            "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
            "        >>> obs.shape\n",
            "        (6, 2)\n",
            "        >>> chisquare(obs)\n",
            "        Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n",
            "        \n",
            "        By setting ``axis=None``, the test is applied to all data in the array,\n",
            "        which is equivalent to applying the test to the flattened array.\n",
            "        \n",
            "        >>> chisquare(obs, axis=None)\n",
            "        Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n",
            "        >>> chisquare(obs.ravel())\n",
            "        Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n",
            "        \n",
            "        `ddof` is the change to make to the default degrees of freedom.\n",
            "        \n",
            "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n",
            "        Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n",
            "        \n",
            "        The calculation of the p-values is done by broadcasting the\n",
            "        chi-squared statistic with `ddof`.\n",
            "        \n",
            "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
            "        Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n",
            "        \n",
            "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
            "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
            "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
            "        statistics, we use ``axis=1``:\n",
            "        \n",
            "        >>> chisquare([16, 18, 16, 14, 12, 12],\n",
            "        ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n",
            "        ...           axis=1)\n",
            "        Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n",
            "    \n",
            "    circmean(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the circular mean for samples in a range.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        samples : array_like\n",
            "            Input array.\n",
            "        high : float or int, optional\n",
            "            High boundary for the sample range. Default is ``2*pi``.\n",
            "        low : float or int, optional\n",
            "            Low boundary for the sample range. Default is 0.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        circmean : float\n",
            "            Circular mean.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`circstd`\n",
            "            Circular standard deviation.\n",
            "        :func:`circvar`\n",
            "            Circular variance.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        For simplicity, all angles are printed out in degrees.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import circmean\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> angles = np.deg2rad(np.array([20, 30, 330]))\n",
            "        >>> circmean = circmean(angles)\n",
            "        >>> np.rad2deg(circmean)\n",
            "        7.294976657784009\n",
            "        \n",
            "        >>> mean = angles.mean()\n",
            "        >>> np.rad2deg(mean)\n",
            "        126.66666666666666\n",
            "        \n",
            "        Plot and compare the circular mean against the arithmetic mean.\n",
            "        \n",
            "        >>> plt.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...          np.sin(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...          c='k')\n",
            "        >>> plt.scatter(np.cos(angles), np.sin(angles), c='k')\n",
            "        >>> plt.scatter(np.cos(circmean), np.sin(circmean), c='b',\n",
            "        ...             label='circmean')\n",
            "        >>> plt.scatter(np.cos(mean), np.sin(mean), c='r', label='mean')\n",
            "        >>> plt.legend()\n",
            "        >>> plt.axis('equal')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    circstd(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate', *, normalize=False, keepdims=False)\n",
            "        Compute the circular standard deviation for samples assumed to be in the\n",
            "        range [low to high].\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        samples : array_like\n",
            "            Input array.\n",
            "        high : float or int, optional\n",
            "            High boundary for the sample range. Default is ``2*pi``.\n",
            "        low : float or int, optional\n",
            "            Low boundary for the sample range. Default is 0.\n",
            "        normalize : boolean, optional\n",
            "            If True, the returned value is equal to ``sqrt(-2*log(R))`` and does\n",
            "            not depend on the variable units. If False (default), the returned\n",
            "            value is scaled by ``((high-low)/(2*pi))``.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        circstd : float\n",
            "            Circular standard deviation.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`circmean`\n",
            "            Circular mean.\n",
            "        :func:`circvar`\n",
            "            Circular variance.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This uses a definition of circular standard deviation from [1]_.\n",
            "        Essentially, the calculation is as follows.\n",
            "        \n",
            "        .. code-block:: python\n",
            "        \n",
            "            import numpy as np\n",
            "            C = np.cos(samples).mean()\n",
            "            S = np.sin(samples).mean()\n",
            "            R = np.sqrt(C**2 + S**2)\n",
            "            l = 2*np.pi / (high-low)\n",
            "            circstd = np.sqrt(-2*np.log(R)) / l\n",
            "        \n",
            "        In the limit of small angles, it returns a number close to the 'linear'\n",
            "        standard deviation.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Mardia, K. V. (1972). 2. In *Statistics of Directional Data*\n",
            "           (pp. 18-24). Academic Press. :doi:`10.1016/C2013-0-07425-7`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import circstd\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286,\n",
            "        ...                       0.133, -0.473, -0.001, -0.348, 0.131])\n",
            "        >>> samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421,\n",
            "        ...                       0.104, -0.136, -0.867,  0.012,  0.105])\n",
            "        >>> circstd_1 = circstd(samples_1)\n",
            "        >>> circstd_2 = circstd(samples_2)\n",
            "        \n",
            "        Plot the samples.\n",
            "        \n",
            "        >>> fig, (left, right) = plt.subplots(ncols=2)\n",
            "        >>> for image in (left, right):\n",
            "        ...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...                np.sin(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...                c='k')\n",
            "        ...     image.axis('equal')\n",
            "        ...     image.axis('off')\n",
            "        >>> left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15)\n",
            "        >>> left.set_title(f\"circular std: {np.round(circstd_1, 2)!r}\")\n",
            "        >>> right.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...            np.sin(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...            c='k')\n",
            "        >>> right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15)\n",
            "        >>> right.set_title(f\"circular std: {np.round(circstd_2, 2)!r}\")\n",
            "        >>> plt.show()\n",
            "    \n",
            "    circvar(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the circular variance for samples assumed to be in a range.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        samples : array_like\n",
            "            Input array.\n",
            "        high : float or int, optional\n",
            "            High boundary for the sample range. Default is ``2*pi``.\n",
            "        low : float or int, optional\n",
            "            Low boundary for the sample range. Default is 0.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        circvar : float\n",
            "            Circular variance.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`circmean`\n",
            "            Circular mean.\n",
            "        :func:`circstd`\n",
            "            Circular standard deviation.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This uses the following definition of circular variance: ``1-R``, where\n",
            "        ``R`` is the mean resultant vector. The\n",
            "        returned value is in the range [0, 1], 0 standing for no variance, and 1\n",
            "        for a large variance. In the limit of small angles, this value is similar\n",
            "        to half the 'linear' variance.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Fisher, N.I. *Statistical analysis of circular data*. Cambridge\n",
            "              University Press, 1993.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import circvar\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286,\n",
            "        ...                       0.133, -0.473, -0.001, -0.348, 0.131])\n",
            "        >>> samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421,\n",
            "        ...                       0.104, -0.136, -0.867,  0.012,  0.105])\n",
            "        >>> circvar_1 = circvar(samples_1)\n",
            "        >>> circvar_2 = circvar(samples_2)\n",
            "        \n",
            "        Plot the samples.\n",
            "        \n",
            "        >>> fig, (left, right) = plt.subplots(ncols=2)\n",
            "        >>> for image in (left, right):\n",
            "        ...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...                np.sin(np.linspace(0, 2*np.pi, 500)),\n",
            "        ...                c='k')\n",
            "        ...     image.axis('equal')\n",
            "        ...     image.axis('off')\n",
            "        >>> left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15)\n",
            "        >>> left.set_title(f\"circular variance: {np.round(circvar_1, 2)!r}\")\n",
            "        >>> right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15)\n",
            "        >>> right.set_title(f\"circular variance: {np.round(circvar_2, 2)!r}\")\n",
            "        >>> plt.show()\n",
            "    \n",
            "    combine_pvalues(pvalues, method='fisher', weights=None, *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Combine p-values from independent tests that bear upon the same hypothesis.\n",
            "        \n",
            "        These methods are intended only for combining p-values from hypothesis\n",
            "        tests based upon continuous distributions.\n",
            "        \n",
            "        Each method assumes that under the null hypothesis, the p-values are\n",
            "        sampled independently and uniformly from the interval [0, 1]. A test\n",
            "        statistic (different for each method) is computed and a combined\n",
            "        p-value is calculated based upon the distribution of this test statistic\n",
            "        under the null hypothesis.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        pvalues : array_like\n",
            "            Array of p-values assumed to come from independent tests based on\n",
            "            continuous distributions.\n",
            "        method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n",
            "            Name of method to use to combine p-values.\n",
            "            \n",
            "            The available methods are (see Notes for details):\n",
            "            \n",
            "            * 'fisher': Fisher's method (Fisher's combined probability test)\n",
            "            * 'pearson': Pearson's method\n",
            "            * 'mudholkar_george': Mudholkar's and George's method\n",
            "            * 'tippett': Tippett's method\n",
            "            * 'stouffer': Stouffer's Z-score method\n",
            "        weights : array_like, optional\n",
            "            Optional array of weights used only for Stouffer's Z-score method.\n",
            "            Ignored by other methods.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SignificanceResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                The statistic calculated by the specified method.\n",
            "            pvalue : float\n",
            "                The combined p-value.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        If this function is applied to tests with a discrete statistics such as\n",
            "        any rank test or contingency-table test, it will yield systematically\n",
            "        wrong results, e.g. Fisher's method will systematically overestimate the\n",
            "        p-value [1]_. This problem becomes less severe for large sample sizes\n",
            "        when the discrete distributions become approximately continuous.\n",
            "        \n",
            "        The differences between the methods can be best illustrated by their\n",
            "        statistics and what aspects of a combination of p-values they emphasise\n",
            "        when considering significance [2]_. For example, methods emphasising large\n",
            "        p-values are more sensitive to strong false and true negatives; conversely\n",
            "        methods focussing on small p-values are sensitive to positives.\n",
            "        \n",
            "        * The statistics of Fisher's method (also known as Fisher's combined\n",
            "          probability test) [3]_ is :math:`-2\\sum_i \\log(p_i)`, which is\n",
            "          equivalent (as a test statistics) to the product of individual p-values:\n",
            "          :math:`\\prod_i p_i`. Under the null hypothesis, this statistics follows\n",
            "          a :math:`\\chi^2` distribution. This method emphasises small p-values.\n",
            "        * Pearson's method uses :math:`-2\\sum_i\\log(1-p_i)`, which is equivalent\n",
            "          to :math:`\\prod_i \\frac{1}{1-p_i}` [2]_.\n",
            "          It thus emphasises large p-values.\n",
            "        * Mudholkar and George compromise between Fisher's and Pearson's method by\n",
            "          averaging their statistics [4]_. Their method emphasises extreme\n",
            "          p-values, both close to 1 and 0.\n",
            "        * Stouffer's method [5]_ uses Z-scores and the statistic:\n",
            "          :math:`\\sum_i \\Phi^{-1} (p_i)`, where :math:`\\Phi` is the CDF of the\n",
            "          standard normal distribution. The advantage of this method is that it is\n",
            "          straightforward to introduce weights, which can make Stouffer's method\n",
            "          more powerful than Fisher's method when the p-values are from studies\n",
            "          of different size [6]_ [7]_.\n",
            "        * Tippett's method uses the smallest p-value as a statistic.\n",
            "          (Mind that this minimum is not the combined p-value.)\n",
            "        \n",
            "        Fisher's method may be extended to combine p-values from dependent tests\n",
            "        [8]_. Extensions such as Brown's method and Kost's method are not currently\n",
            "        implemented.\n",
            "        \n",
            "        .. versionadded:: 0.15.0\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n",
            "               Distributions.\" Journal of the American Statistical Association 57,\n",
            "               no. 297 (1962), 10-19.\n",
            "        .. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n",
            "               combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n",
            "        .. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n",
            "        .. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n",
            "               random variables.\" Metrika 30.1 (1983): 1-13.\n",
            "        .. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n",
            "        .. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n",
            "               weighted Z-method is superior to Fisher's approach.\" Journal of\n",
            "               Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n",
            "        .. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n",
            "               for combining probabilities in meta-analysis.\" Journal of\n",
            "               Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n",
            "        .. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to combine p-values from four independent tests\n",
            "        of the same null hypothesis using Fisher's method (default).\n",
            "        \n",
            "        >>> from scipy.stats import combine_pvalues\n",
            "        >>> pvalues = [0.1, 0.05, 0.02, 0.3]\n",
            "        >>> combine_pvalues(pvalues)\n",
            "        SignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n",
            "        \n",
            "        When the individual p-values carry different weights, consider Stouffer's\n",
            "        method.\n",
            "        \n",
            "        >>> weights = [1, 2, 3, 4]\n",
            "        >>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n",
            "        >>> res.pvalue\n",
            "        0.009578891494533616\n",
            "    \n",
            "    cramervonmises(rvs, cdf, args=(), *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform the one-sample Cramr-von Mises test for goodness of fit.\n",
            "        \n",
            "        This performs a test of the goodness of fit of a cumulative distribution\n",
            "        function (cdf) :math:`F` compared to the empirical distribution function\n",
            "        :math:`F_n` of observed random variates :math:`X_1, ..., X_n` that are\n",
            "        assumed to be independent and identically distributed ([1]_).\n",
            "        The null hypothesis is that the :math:`X_i` have cumulative distribution\n",
            "        :math:`F`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        rvs : array_like\n",
            "            A 1-D array of observed values of the random variables :math:`X_i`.\n",
            "        cdf : str or callable\n",
            "            The cumulative distribution function :math:`F` to test the\n",
            "            observations against. If a string, it should be the name of a\n",
            "            distribution in `scipy.stats`. If a callable, that callable is used\n",
            "            to calculate the cdf: ``cdf(x, *args) -> float``.\n",
            "        args : tuple, optional\n",
            "            Distribution parameters. These are assumed to be known; see Notes.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : object with attributes\n",
            "            statistic : float\n",
            "                Cramr-von Mises statistic.\n",
            "            pvalue : float\n",
            "                The p-value.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kstest`, :func:`cramervonmises_2samp`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        .. versionadded:: 1.6.0\n",
            "        \n",
            "        The p-value relies on the approximation given by equation 1.8 in [2]_.\n",
            "        It is important to keep in mind that the p-value is only accurate if\n",
            "        one tests a simple hypothesis, i.e. the parameters of the reference\n",
            "        distribution are known. If the parameters are estimated from the data\n",
            "        (composite hypothesis), the computed p-value is not reliable.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Cramr-von Mises criterion, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n",
            "        .. [2] Csrg, S. and Faraway, J. (1996). The Exact and Asymptotic\n",
            "               Distribution of Cramr-von Mises Statistics. Journal of the\n",
            "               Royal Statistical Society, pp. 221-234.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test whether data generated by ``scipy.stats.norm.rvs``\n",
            "        were, in fact, drawn from the standard normal distribution. We choose a\n",
            "        significance level of ``alpha=0.05``.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng(165417232101553420507139617764912913465)\n",
            "        >>> x = stats.norm.rvs(size=500, random_state=rng)\n",
            "        >>> res = stats.cramervonmises(x, 'norm')\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.1072085112565724, 0.5508482238203407)\n",
            "        \n",
            "        The p-value exceeds our chosen significance level, so we do not\n",
            "        reject the null hypothesis that the observed sample is drawn from the\n",
            "        standard normal distribution.\n",
            "        \n",
            "        Now suppose we wish to check whether the same samples shifted by 2.1 is\n",
            "        consistent with being drawn from a normal distribution with a mean of 2.\n",
            "        \n",
            "        >>> y = x + 2.1\n",
            "        >>> res = stats.cramervonmises(y, 'norm', args=(2,))\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.8364446265294695, 0.00596286797008283)\n",
            "        \n",
            "        Here we have used the `args` keyword to specify the mean (``loc``)\n",
            "        of the normal distribution to test the data against. This is equivalent\n",
            "        to the following, in which we create a frozen normal distribution with\n",
            "        mean 2.1, then pass its ``cdf`` method as an argument.\n",
            "        \n",
            "        >>> frozen_dist = stats.norm(loc=2)\n",
            "        >>> res = stats.cramervonmises(y, frozen_dist.cdf)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.8364446265294695, 0.00596286797008283)\n",
            "        \n",
            "        In either case, we would reject the null hypothesis that the observed\n",
            "        sample is drawn from a normal distribution with a mean of 2 (and default\n",
            "        variance of 1) because the p-value is less than our chosen\n",
            "        significance level.\n",
            "    \n",
            "    cramervonmises_2samp(x, y, method='auto', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform the two-sample Cramr-von Mises test for goodness of fit.\n",
            "        \n",
            "        This is the two-sample version of the Cramr-von Mises test ([1]_):\n",
            "        for two independent samples :math:`X_1, ..., X_n` and\n",
            "        :math:`Y_1, ..., Y_m`, the null hypothesis is that the samples\n",
            "        come from the same (unspecified) continuous distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            A 1-D array of observed values of the random variables :math:`X_i`.\n",
            "        y : array_like\n",
            "            A 1-D array of observed values of the random variables :math:`Y_i`.\n",
            "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
            "            The method used to compute the p-value, see Notes for details.\n",
            "            The default is 'auto'.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : object with attributes\n",
            "            statistic : float\n",
            "                Cramr-von Mises statistic.\n",
            "            pvalue : float\n",
            "                The p-value.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`cramervonmises`, :func:`anderson_ksamp`, :func:`epps_singleton_2samp`, :func:`ks_2samp`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        .. versionadded:: 1.7.0\n",
            "        \n",
            "        The statistic is computed according to equation 9 in [2]_. The\n",
            "        calculation of the p-value depends on the keyword `method`:\n",
            "        \n",
            "        - ``asymptotic``: The p-value is approximated by using the limiting\n",
            "          distribution of the test statistic.\n",
            "        - ``exact``: The exact p-value is computed by enumerating all\n",
            "          possible combinations of the test statistic, see [2]_.\n",
            "        \n",
            "        If ``method='auto'``, the exact approach is used\n",
            "        if both samples contain equal to or less than 20 observations,\n",
            "        otherwise the asymptotic distribution is used.\n",
            "        \n",
            "        If the underlying distribution is not continuous, the p-value is likely to\n",
            "        be conservative (Section 6.2 in [3]_). When ranking the data to compute\n",
            "        the test statistic, midranks are used if there are ties.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Cramer-von_Mises_criterion\n",
            "        .. [2] Anderson, T.W. (1962). On the distribution of the two-sample\n",
            "               Cramer-von-Mises criterion. The Annals of Mathematical\n",
            "               Statistics, pp. 1148-1159.\n",
            "        .. [3] Conover, W.J., Practical Nonparametric Statistics, 1971.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test whether two samples generated by\n",
            "        ``scipy.stats.norm.rvs`` have the same distribution. We choose a\n",
            "        significance level of alpha=0.05.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.norm.rvs(size=100, random_state=rng)\n",
            "        >>> y = stats.norm.rvs(size=70, random_state=rng)\n",
            "        >>> res = stats.cramervonmises_2samp(x, y)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.29376470588235293, 0.1412873014573014)\n",
            "        \n",
            "        The p-value exceeds our chosen significance level, so we do not\n",
            "        reject the null hypothesis that the observed samples are drawn from the\n",
            "        same distribution.\n",
            "        \n",
            "        For small sample sizes, one can compute the exact p-values:\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=7, random_state=rng)\n",
            "        >>> y = stats.t.rvs(df=2, size=6, random_state=rng)\n",
            "        >>> res = stats.cramervonmises_2samp(x, y, method='exact')\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.197802197802198, 0.31643356643356646)\n",
            "        \n",
            "        The p-value based on the asymptotic distribution is a good approximation\n",
            "        even though the sample size is small.\n",
            "        \n",
            "        >>> res = stats.cramervonmises_2samp(x, y, method='asymptotic')\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.197802197802198, 0.2966041181527128)\n",
            "        \n",
            "        Independent of the method, one would not reject the null hypothesis at the\n",
            "        chosen significance level in this example.\n",
            "    \n",
            "    cumfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
            "        Return a cumulative frequency histogram, using the histogram function.\n",
            "        \n",
            "        A cumulative histogram is a mapping that counts the cumulative number of\n",
            "        observations in all of the bins up to the specified bin.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        numbins : int, optional\n",
            "            The number of bins to use for the histogram. Default is 10.\n",
            "        defaultreallimits : tuple (lower, upper), optional\n",
            "            The lower and upper values for the range of the histogram.\n",
            "            If no value is given, a range slightly larger than the range of the\n",
            "            values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
            "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
            "        weights : array_like, optional\n",
            "            The weights for each value in `a`. Default is None, which gives each\n",
            "            value a weight of 1.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        cumcount : ndarray\n",
            "            Binned values of cumulative frequency.\n",
            "        lowerlimit : float\n",
            "            Lower real limit\n",
            "        binsize : float\n",
            "            Width of each bin.\n",
            "        extrapoints : int\n",
            "            Extra points.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = [1, 4, 2, 1, 3, 1]\n",
            "        >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n",
            "        >>> res.cumcount\n",
            "        array([ 1.,  2.,  3.,  3.])\n",
            "        >>> res.extrapoints\n",
            "        3\n",
            "        \n",
            "        Create a normal distribution with 1000 random values\n",
            "        \n",
            "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
            "        \n",
            "        Calculate cumulative frequencies\n",
            "        \n",
            "        >>> res = stats.cumfreq(samples, numbins=25)\n",
            "        \n",
            "        Calculate space of values for x\n",
            "        \n",
            "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n",
            "        ...                                  res.cumcount.size)\n",
            "        \n",
            "        Plot histogram and cumulative histogram\n",
            "        \n",
            "        >>> fig = plt.figure(figsize=(10, 4))\n",
            "        >>> ax1 = fig.add_subplot(1, 2, 1)\n",
            "        >>> ax2 = fig.add_subplot(1, 2, 2)\n",
            "        >>> ax1.hist(samples, bins=25)\n",
            "        >>> ax1.set_title('Histogram')\n",
            "        >>> ax2.bar(x, res.cumcount, width=res.binsize)\n",
            "        >>> ax2.set_title('Cumulative histogram')\n",
            "        >>> ax2.set_xlim([x.min(), x.max()])\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate')\n",
            "        Compute several descriptive statistics of the passed array.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input data.\n",
            "        axis : int or None, optional\n",
            "            Axis along which statistics are calculated. Default is 0.\n",
            "            If None, compute over the whole array `a`.\n",
            "        ddof : int, optional\n",
            "            Delta degrees of freedom (only for variance).  Default is 1.\n",
            "        bias : bool, optional\n",
            "            If False, then the skewness and kurtosis calculations are corrected\n",
            "            for statistical bias.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "            * 'propagate': returns nan\n",
            "            * 'raise': throws an error\n",
            "            * 'omit': performs the calculations ignoring nan values\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        nobs : int or ndarray of ints\n",
            "            Number of observations (length of data along `axis`).\n",
            "            When 'omit' is chosen as nan_policy, the length along each axis\n",
            "            slice is counted separately.\n",
            "        minmax: tuple of ndarrays or floats\n",
            "            Minimum and maximum value of `a` along the given axis.\n",
            "        mean : ndarray or float\n",
            "            Arithmetic mean of `a` along the given axis.\n",
            "        variance : ndarray or float\n",
            "            Unbiased variance of `a` along the given axis; denominator is number\n",
            "            of observations minus one.\n",
            "        skewness : ndarray or float\n",
            "            Skewness of `a` along the given axis, based on moment calculations\n",
            "            with denominator equal to the number of observations, i.e. no degrees\n",
            "            of freedom correction.\n",
            "        kurtosis : ndarray or float\n",
            "            Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n",
            "            normalized so that it is zero for the normal distribution.  No\n",
            "            degrees of freedom are used.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        skew, kurtosis\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = np.arange(10)\n",
            "        >>> stats.describe(a)\n",
            "        DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n",
            "                       variance=9.166666666666666, skewness=0.0,\n",
            "                       kurtosis=-1.2242424242424244)\n",
            "        >>> b = [[1, 2], [3, 4]]\n",
            "        >>> stats.describe(b)\n",
            "        DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n",
            "                       mean=array([2., 3.]), variance=array([2., 2.]),\n",
            "                       skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n",
            "    \n",
            "    differential_entropy(values: 'np.typing.ArrayLike', *, window_length: 'int | None' = None, base: 'float | None' = None, axis: 'int' = 0, method: 'str' = 'auto', nan_policy='propagate', keepdims=False) -> 'np.number | np.ndarray'\n",
            "        Given a sample of a distribution, estimate the differential entropy.\n",
            "        \n",
            "        Several estimation methods are available using the `method` parameter. By\n",
            "        default, a method is selected based the size of the sample.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        values : sequence\n",
            "            Sample from a continuous distribution.\n",
            "        window_length : int, optional\n",
            "            Window length for computing Vasicek estimate. Must be an integer\n",
            "            between 1 and half of the sample size. If ``None`` (the default), it\n",
            "            uses the heuristic value\n",
            "            \n",
            "            .. math::\n",
            "                \\left \\lfloor \\sqrt{n} + 0.5 \\right \\rfloor\n",
            "            \n",
            "            where :math:`n` is the sample size. This heuristic was originally\n",
            "            proposed in [2]_ and has become common in the literature.\n",
            "        base : float, optional\n",
            "            The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\n",
            "            The method used to estimate the differential entropy from the sample.\n",
            "            Default is ``'auto'``.  See Notes for more information.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        entropy : float\n",
            "            The calculated differential entropy.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function will converge to the true differential entropy in the limit\n",
            "        \n",
            "        .. math::\n",
            "            n \\to \\infty, \\quad m \\to \\infty, \\quad \\frac{m}{n} \\to 0\n",
            "        \n",
            "        The optimal choice of ``window_length`` for a given sample size depends on\n",
            "        the (unknown) distribution. Typically, the smoother the density of the\n",
            "        distribution, the larger the optimal value of ``window_length`` [1]_.\n",
            "        \n",
            "        The following options are available for the `method` parameter.\n",
            "        \n",
            "        * ``'vasicek'`` uses the estimator presented in [1]_. This is\n",
            "          one of the first and most influential estimators of differential entropy.\n",
            "        * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\n",
            "          is not only consistent but, under some conditions, asymptotically normal.\n",
            "        * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\n",
            "          in simulation to have smaller bias and mean squared error than\n",
            "          the Vasicek estimator.\n",
            "        * ``'correa'`` uses the estimator presented in [5]_ based on local linear\n",
            "          regression. In a simulation study, it had consistently smaller mean\n",
            "          square error than the Vasiceck estimator, but it is more expensive to\n",
            "          compute.\n",
            "        * ``'auto'`` selects the method automatically (default). Currently,\n",
            "          this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\n",
            "          for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\n",
            "          samples, but this behavior is subject to change in future versions.\n",
            "        \n",
            "        All estimators are implemented as described in [6]_.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\n",
            "               Journal of the Royal Statistical Society:\n",
            "               Series B (Methodological), 38(1), 54-59.\n",
            "        .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\n",
            "               goodness-of-fit test for exponentiality. Communications in\n",
            "               Statistics-Theory and Methods, 28(5), 1183-1202.\n",
            "        .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\n",
            "               class of statistics based on spacings. Scandinavian Journal of\n",
            "               Statistics, 61-72.\n",
            "        .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\n",
            "               of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n",
            "        .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\n",
            "               in Statistics-Theory and Methods, 24(10), 2439-2449.\n",
            "        .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\n",
            "               Annals of Data Science, 2(2), 231-241.\n",
            "               https://link.springer.com/article/10.1007/s40745-015-0045-9\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import differential_entropy, norm\n",
            "        \n",
            "        Entropy of a standard normal distribution:\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> values = rng.standard_normal(100)\n",
            "        >>> differential_entropy(values)\n",
            "        1.3407817436640392\n",
            "        \n",
            "        Compare with the true entropy:\n",
            "        \n",
            "        >>> float(norm.entropy())\n",
            "        1.4189385332046727\n",
            "        \n",
            "        For several sample sizes between 5 and 1000, compare the accuracy of\n",
            "        the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\n",
            "        compare the root mean squared error (over 1000 trials) between the estimate\n",
            "        and the true differential entropy of the distribution.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>>\n",
            "        >>>\n",
            "        >>> def rmse(res, expected):\n",
            "        ...     '''Root mean squared error'''\n",
            "        ...     return np.sqrt(np.mean((res - expected)**2))\n",
            "        >>>\n",
            "        >>>\n",
            "        >>> a, b = np.log10(5), np.log10(1000)\n",
            "        >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\n",
            "        >>> reps = 1000  # number of repetitions for each sample size\n",
            "        >>> expected = stats.expon.entropy()\n",
            "        >>>\n",
            "        >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\n",
            "        >>> for method in method_errors:\n",
            "        ...     for n in ns:\n",
            "        ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\n",
            "        ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\n",
            "        ...        error = rmse(res, expected)\n",
            "        ...        method_errors[method].append(error)\n",
            "        >>>\n",
            "        >>> for method, errors in method_errors.items():\n",
            "        ...     plt.loglog(ns, errors, label=method)\n",
            "        >>>\n",
            "        >>> plt.legend()\n",
            "        >>> plt.xlabel('sample size')\n",
            "        >>> plt.ylabel('RMSE (1000 trials)')\n",
            "        >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\n",
            "    \n",
            "    directional_stats(samples, *, axis=0, normalize=True)\n",
            "        Computes sample statistics for directional data.\n",
            "        \n",
            "        Computes the directional mean (also called the mean direction vector) and\n",
            "        mean resultant length of a sample of vectors.\n",
            "        \n",
            "        The directional mean is a measure of \"preferred direction\" of vector data.\n",
            "        It is analogous to the sample mean, but it is for use when the length of\n",
            "        the data is irrelevant (e.g. unit vectors).\n",
            "        \n",
            "        The mean resultant length is a value between 0 and 1 used to quantify the\n",
            "        dispersion of directional data: the smaller the mean resultant length, the\n",
            "        greater the dispersion. Several definitions of directional variance\n",
            "        involving the mean resultant length are given in [1]_ and [2]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        samples : array_like\n",
            "            Input array. Must be at least two-dimensional, and the last axis of the\n",
            "            input must correspond with the dimensionality of the vector space.\n",
            "            When the input is exactly two dimensional, this means that each row\n",
            "            of the data is a vector observation.\n",
            "        axis : int, default: 0\n",
            "            Axis along which the directional mean is computed.\n",
            "        normalize: boolean, default: True\n",
            "            If True, normalize the input to ensure that each observation is a\n",
            "            unit vector. It the observations are already unit vectors, consider\n",
            "            setting this to False to avoid unnecessary computation.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : DirectionalStats\n",
            "            An object containing attributes:\n",
            "        \n",
            "            mean_direction : ndarray\n",
            "                Directional mean.\n",
            "            mean_resultant_length : ndarray\n",
            "                The mean resultant length [1]_.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        circmean: circular mean; i.e. directional mean for 2D *angles*\n",
            "        circvar: circular variance; i.e. directional variance for 2D *angles*\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This uses a definition of directional mean from [1]_.\n",
            "        Assuming the observations are unit vectors, the calculation is as follows.\n",
            "        \n",
            "        .. code-block:: python\n",
            "        \n",
            "            mean = samples.mean(axis=0)\n",
            "            mean_resultant_length = np.linalg.norm(mean)\n",
            "            mean_direction = mean / mean_resultant_length\n",
            "        \n",
            "        This definition is appropriate for *directional* data (i.e. vector data\n",
            "        for which the magnitude of each observation is irrelevant) but not\n",
            "        for *axial* data (i.e. vector data for which the magnitude and *sign* of\n",
            "        each observation is irrelevant).\n",
            "        \n",
            "        Several definitions of directional variance involving the mean resultant\n",
            "        length ``R`` have been proposed, including ``1 - R`` [1]_, ``1 - R**2``\n",
            "        [2]_, and ``2 * (1 - R)`` [2]_. Rather than choosing one, this function\n",
            "        returns ``R`` as attribute `mean_resultant_length` so the user can compute\n",
            "        their preferred measure of dispersion.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Mardia, Jupp. (2000). *Directional Statistics*\n",
            "           (p. 163). Wiley.\n",
            "        \n",
            "        .. [2] https://en.wikipedia.org/wiki/Directional_statistics\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import directional_stats\n",
            "        >>> data = np.array([[3, 4],    # first observation, 2D vector space\n",
            "        ...                  [6, -8]])  # second observation\n",
            "        >>> dirstats = directional_stats(data)\n",
            "        >>> dirstats.mean_direction\n",
            "        array([1., 0.])\n",
            "        \n",
            "        In contrast, the regular sample mean of the vectors would be influenced\n",
            "        by the magnitude of each observation. Furthermore, the result would not be\n",
            "        a unit vector.\n",
            "        \n",
            "        >>> data.mean(axis=0)\n",
            "        array([4.5, -2.])\n",
            "        \n",
            "        An exemplary use case for `directional_stats` is to find a *meaningful*\n",
            "        center for a set of observations on a sphere, e.g. geographical locations.\n",
            "        \n",
            "        >>> data = np.array([[0.8660254, 0.5, 0.],\n",
            "        ...                  [0.8660254, -0.5, 0.]])\n",
            "        >>> dirstats = directional_stats(data)\n",
            "        >>> dirstats.mean_direction\n",
            "        array([1., 0., 0.])\n",
            "        \n",
            "        The regular sample mean on the other hand yields a result which does not\n",
            "        lie on the surface of the sphere.\n",
            "        \n",
            "        >>> data.mean(axis=0)\n",
            "        array([0.8660254, 0., 0.])\n",
            "        \n",
            "        The function also returns the mean resultant length, which\n",
            "        can be used to calculate a directional variance. For example, using the\n",
            "        definition ``Var(z) = 1 - R`` from [2]_ where ``R`` is the\n",
            "        mean resultant length, we can calculate the directional variance of the\n",
            "        vectors in the above example as:\n",
            "        \n",
            "        >>> 1 - dirstats.mean_resultant_length\n",
            "        0.13397459716167093\n",
            "    \n",
            "    dunnett(*samples: 'npt.ArrayLike', control: 'npt.ArrayLike', alternative: \"Literal['two-sided', 'less', 'greater']\" = 'two-sided', random_state: 'SeedType' = None) -> 'DunnettResult'\n",
            "        Dunnett's test: multiple comparisons of means against a control group.\n",
            "        \n",
            "        This is an implementation of Dunnett's original, single-step test as\n",
            "        described in [1]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : 1D array_like\n",
            "            The sample measurements for each experimental group.\n",
            "        control : 1D array_like\n",
            "            The sample measurements for the control group.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "        \n",
            "            The null hypothesis is that the means of the distributions underlying\n",
            "            the samples and control are equal. The following alternative\n",
            "            hypotheses are available (default is 'two-sided'):\n",
            "        \n",
            "            * 'two-sided': the means of the distributions underlying the samples\n",
            "              and control are unequal.\n",
            "            * 'less': the means of the distributions underlying the samples\n",
            "              are less than the mean of the distribution underlying the control.\n",
            "            * 'greater': the means of the distributions underlying the\n",
            "              samples are greater than the mean of the distribution underlying\n",
            "              the control.\n",
            "        random_state : {None, int, `numpy.random.Generator`}, optional\n",
            "            If `random_state` is an int or None, a new `numpy.random.Generator` is\n",
            "            created using ``np.random.default_rng(random_state)``.\n",
            "            If `random_state` is already a ``Generator`` instance, then the\n",
            "            provided instance is used.\n",
            "        \n",
            "            The random number generator is used to control the randomized\n",
            "            Quasi-Monte Carlo integration of the multivariate-t distribution.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : `~scipy.stats._result_classes.DunnettResult`\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float ndarray\n",
            "                The computed statistic of the test for each comparison. The element\n",
            "                at index ``i`` is the statistic for the comparison between\n",
            "                groups ``i`` and the control.\n",
            "            pvalue : float ndarray\n",
            "                The computed p-value of the test for each comparison. The element\n",
            "                at index ``i`` is the p-value for the comparison between\n",
            "                group ``i`` and the control.\n",
            "        \n",
            "            And the following method:\n",
            "        \n",
            "            confidence_interval(confidence_level=0.95) :\n",
            "                Compute the difference in means of the groups\n",
            "                with the control +- the allowance.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        tukey_hsd : performs pairwise comparison of means.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Like the independent-sample t-test, Dunnett's test [1]_ is used to make\n",
            "        inferences about the means of distributions from which samples were drawn.\n",
            "        However, when multiple t-tests are performed at a fixed significance level,\n",
            "        the \"family-wise error rate\" - the probability of incorrectly rejecting the\n",
            "        null hypothesis in at least one test - will exceed the significance level.\n",
            "        Dunnett's test is designed to perform multiple comparisons while\n",
            "        controlling the family-wise error rate.\n",
            "        \n",
            "        Dunnett's test compares the means of multiple experimental groups\n",
            "        against a single control group. Tukey's Honestly Significant Difference Test\n",
            "        is another multiple-comparison test that controls the family-wise error\n",
            "        rate, but `tukey_hsd` performs *all* pairwise comparisons between groups.\n",
            "        When pairwise comparisons between experimental groups are not needed,\n",
            "        Dunnett's test is preferable due to its higher power.\n",
            "        \n",
            "        \n",
            "        The use of this test relies on several assumptions.\n",
            "        \n",
            "        1. The observations are independent within and among groups.\n",
            "        2. The observations within each group are normally distributed.\n",
            "        3. The distributions from which the samples are drawn have the same finite\n",
            "           variance.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Charles W. Dunnett. \"A Multiple Comparison Procedure for Comparing\n",
            "           Several Treatments with a Control.\"\n",
            "           Journal of the American Statistical Association, 50:272, 1096-1121,\n",
            "           :doi:`10.1080/01621459.1955.10501294`, 1955.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [1]_, the influence of drugs on blood count measurements on three groups\n",
            "        of animal is investigated.\n",
            "        \n",
            "        The following table summarizes the results of the experiment in which\n",
            "        two groups received different drugs, and one group acted as a control.\n",
            "        Blood counts (in millions of cells per cubic millimeter) were recorded::\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> control = np.array([7.40, 8.50, 7.20, 8.24, 9.84, 8.32])\n",
            "        >>> drug_a = np.array([9.76, 8.80, 7.68, 9.36])\n",
            "        >>> drug_b = np.array([12.80, 9.68, 12.16, 9.20, 10.55])\n",
            "        \n",
            "        We would like to see if the means between any of the groups are\n",
            "        significantly different. First, visually examine a box and whisker plot.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.boxplot([control, drug_a, drug_b])\n",
            "        >>> ax.set_xticklabels([\"Control\", \"Drug A\", \"Drug B\"])  # doctest: +SKIP\n",
            "        >>> ax.set_ylabel(\"mean\")  # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Note the overlapping interquartile ranges of the drug A group and control\n",
            "        group and the apparent separation between the drug B group and control\n",
            "        group.\n",
            "        \n",
            "        Next, we will use Dunnett's test to assess whether the difference\n",
            "        between group means is significant while controlling the family-wise error\n",
            "        rate: the probability of making any false discoveries.\n",
            "        Let the null hypothesis be that the experimental groups have the same\n",
            "        mean as the control and the alternative be that an experimental group does\n",
            "        not have the same mean as the control. We will consider a 5% family-wise\n",
            "        error rate to be acceptable, and therefore we choose 0.05 as the threshold\n",
            "        for significance.\n",
            "        \n",
            "        >>> from scipy.stats import dunnett\n",
            "        >>> res = dunnett(drug_a, drug_b, control=control)\n",
            "        >>> res.pvalue\n",
            "        array([0.62004941, 0.0059035 ])  # may vary\n",
            "        \n",
            "        The p-value corresponding with the comparison between group A and control\n",
            "        exceeds 0.05, so we do not reject the null hypothesis for that comparison.\n",
            "        However, the p-value corresponding with the comparison between group B\n",
            "        and control is less than 0.05, so we consider the experimental results\n",
            "        to be evidence against the null hypothesis in favor of the alternative:\n",
            "        group B has a different mean than the control group.\n",
            "    \n",
            "    ecdf(sample: 'npt.ArrayLike | CensoredData') -> 'ECDFResult'\n",
            "        Empirical cumulative distribution function of a sample.\n",
            "        \n",
            "        The empirical cumulative distribution function (ECDF) is a step function\n",
            "        estimate of the CDF of the distribution underlying a sample. This function\n",
            "        returns objects representing both the empirical distribution function and\n",
            "        its complement, the empirical survival function.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample : 1D array_like or `scipy.stats.CensoredData`\n",
            "            Besides array_like, instances of `scipy.stats.CensoredData` containing\n",
            "            uncensored and right-censored observations are supported. Currently,\n",
            "            other instances of `scipy.stats.CensoredData` will result in a\n",
            "            ``NotImplementedError``.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : `~scipy.stats._result_classes.ECDFResult`\n",
            "            An object with the following attributes.\n",
            "        \n",
            "            cdf : `~scipy.stats._result_classes.EmpiricalDistributionFunction`\n",
            "                An object representing the empirical cumulative distribution\n",
            "                function.\n",
            "            sf : `~scipy.stats._result_classes.EmpiricalDistributionFunction`\n",
            "                An object representing the empirical survival function.\n",
            "        \n",
            "            The `cdf` and `sf` attributes themselves have the following attributes.\n",
            "        \n",
            "            quantiles : ndarray\n",
            "                The unique values in the sample that defines the empirical CDF/SF.\n",
            "            probabilities : ndarray\n",
            "                The point estimates of the probabilities corresponding with\n",
            "                `quantiles`.\n",
            "        \n",
            "            And the following methods:\n",
            "        \n",
            "            evaluate(x) :\n",
            "                Evaluate the CDF/SF at the argument.\n",
            "        \n",
            "            plot(ax) :\n",
            "                Plot the CDF/SF on the provided axes.\n",
            "        \n",
            "            confidence_interval(confidence_level=0.95) :\n",
            "                Compute the confidence interval around the CDF/SF at the values in\n",
            "                `quantiles`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        When each observation of the sample is a precise measurement, the ECDF\n",
            "        steps up by ``1/len(sample)`` at each of the observations [1]_.\n",
            "        \n",
            "        When observations are lower bounds, upper bounds, or both upper and lower\n",
            "        bounds, the data is said to be \"censored\", and `sample` may be provided as\n",
            "        an instance of `scipy.stats.CensoredData`.\n",
            "        \n",
            "        For right-censored data, the ECDF is given by the Kaplan-Meier estimator\n",
            "        [2]_; other forms of censoring are not supported at this time.\n",
            "        \n",
            "        Confidence intervals are computed according to the Greenwood formula or the\n",
            "        more recent \"Exponential Greenwood\" formula as described in [4]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Conover, William Jay. Practical nonparametric statistics. Vol. 350.\n",
            "               John Wiley & Sons, 1999.\n",
            "        \n",
            "        .. [2] Kaplan, Edward L., and Paul Meier. \"Nonparametric estimation from\n",
            "               incomplete observations.\" Journal of the American statistical\n",
            "               association 53.282 (1958): 457-481.\n",
            "        \n",
            "        .. [3] Goel, Manish Kumar, Pardeep Khanna, and Jugal Kishore.\n",
            "               \"Understanding survival analysis: Kaplan-Meier estimate.\"\n",
            "               International journal of Ayurveda research 1.4 (2010): 274.\n",
            "        \n",
            "        .. [4] Sawyer, Stanley. \"The Greenwood and Exponential Greenwood Confidence\n",
            "               Intervals in Survival Analysis.\"\n",
            "               https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        **Uncensored Data**\n",
            "        \n",
            "        As in the example from [1]_ page 79, five boys were selected at random from\n",
            "        those in a single high school. Their one-mile run times were recorded as\n",
            "        follows.\n",
            "        \n",
            "        >>> sample = [6.23, 5.58, 7.06, 6.42, 5.20]  # one-mile run times (minutes)\n",
            "        \n",
            "        The empirical distribution function, which approximates the distribution\n",
            "        function of one-mile run times of the population from which the boys were\n",
            "        sampled, is calculated as follows.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.ecdf(sample)\n",
            "        >>> res.cdf.quantiles\n",
            "        array([5.2 , 5.58, 6.23, 6.42, 7.06])\n",
            "        >>> res.cdf.probabilities\n",
            "        array([0.2, 0.4, 0.6, 0.8, 1. ])\n",
            "        \n",
            "        To plot the result as a step function:\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> ax = plt.subplot()\n",
            "        >>> res.cdf.plot(ax)\n",
            "        >>> ax.set_xlabel('One-Mile Run Time (minutes)')\n",
            "        >>> ax.set_ylabel('Empirical CDF')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        **Right-censored Data**\n",
            "        \n",
            "        As in the example from [1]_ page 91, the lives of ten car fanbelts were\n",
            "        tested. Five tests concluded because the fanbelt being tested broke, but\n",
            "        the remaining tests concluded for other reasons (e.g. the study ran out of\n",
            "        funding, but the fanbelt was still functional). The mileage driven\n",
            "        with the fanbelts were recorded as follows.\n",
            "        \n",
            "        >>> broken = [77, 47, 81, 56, 80]  # in thousands of miles driven\n",
            "        >>> unbroken = [62, 60, 43, 71, 37]\n",
            "        \n",
            "        Precise survival times of the fanbelts that were still functional at the\n",
            "        end of the tests are unknown, but they are known to exceed the values\n",
            "        recorded in ``unbroken``. Therefore, these observations are said to be\n",
            "        \"right-censored\", and the data is represented using\n",
            "        `scipy.stats.CensoredData`.\n",
            "        \n",
            "        >>> sample = stats.CensoredData(uncensored=broken, right=unbroken)\n",
            "        \n",
            "        The empirical survival function is calculated as follows.\n",
            "        \n",
            "        >>> res = stats.ecdf(sample)\n",
            "        >>> res.sf.quantiles\n",
            "        array([37., 43., 47., 56., 60., 62., 71., 77., 80., 81.])\n",
            "        >>> res.sf.probabilities\n",
            "        array([1.   , 1.   , 0.875, 0.75 , 0.75 , 0.75 , 0.75 , 0.5  , 0.25 , 0.   ])\n",
            "        \n",
            "        To plot the result as a step function:\n",
            "        \n",
            "        >>> ax = plt.subplot()\n",
            "        >>> res.cdf.plot(ax)\n",
            "        >>> ax.set_xlabel('Fanbelt Survival Time (thousands of miles)')\n",
            "        >>> ax.set_ylabel('Empirical SF')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    energy_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
            "        Compute the energy distance between two 1D distributions.\n",
            "        \n",
            "        .. versionadded:: 1.0.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        u_values, v_values : array_like\n",
            "            Values observed in the (empirical) distribution.\n",
            "        u_weights, v_weights : array_like, optional\n",
            "            Weight for each value. If unspecified, each value is assigned the same\n",
            "            weight.\n",
            "            `u_weights` (resp. `v_weights`) must have the same length as\n",
            "            `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
            "            must still be positive and finite so that the weights can be normalized\n",
            "            to sum to 1.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        distance : float\n",
            "            The computed distance between the distributions.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The energy distance between two distributions :math:`u` and :math:`v`, whose\n",
            "        respective CDFs are :math:`U` and :math:`V`, equals to:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D(u, v) = \\left( 2\\mathbb E|X - Y| - \\mathbb E|X - X'| -\n",
            "            \\mathbb E|Y - Y'| \\right)^{1/2}\n",
            "        \n",
            "        where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are\n",
            "        independent random variables whose probability distribution is :math:`u`\n",
            "        (resp. :math:`v`).\n",
            "        \n",
            "        Sometimes the square of this quantity is referred to as the \"energy\n",
            "        distance\" (e.g. in [2]_, [4]_), but as noted in [1]_ and [3]_, only the\n",
            "        definition above satisfies the axioms of a distance function (metric).\n",
            "        \n",
            "        As shown in [2]_, for one-dimensional real-valued variables, the energy\n",
            "        distance is linked to the non-distribution-free version of the Cramr-von\n",
            "        Mises distance:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D(u, v) = \\sqrt{2} l_2(u, v) = \\left( 2 \\int_{-\\infty}^{+\\infty} (U-V)^2\n",
            "            \\right)^{1/2}\n",
            "        \n",
            "        Note that the common Cramr-von Mises criterion uses the distribution-free\n",
            "        version of the distance. See [2]_ (section 2), for more details about both\n",
            "        versions of the distance.\n",
            "        \n",
            "        The input distributions can be empirical, therefore coming from samples\n",
            "        whose values are effectively inputs of the function, or they can be seen as\n",
            "        generalized functions, in which case they are weighted sums of Dirac delta\n",
            "        functions located at the specified values.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Rizzo, Szekely \"Energy distance.\" Wiley Interdisciplinary Reviews:\n",
            "               Computational Statistics, 8(1):27-38 (2015).\n",
            "        .. [2] Szekely \"E-statistics: The energy of statistical samples.\" Bowling\n",
            "               Green State University, Department of Mathematics and Statistics,\n",
            "               Technical Report 02-16 (2002).\n",
            "        .. [3] \"Energy distance\", https://en.wikipedia.org/wiki/Energy_distance\n",
            "        .. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
            "               Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
            "               Gradients\" (2017). :arXiv:`1705.10743`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import energy_distance\n",
            "        >>> energy_distance([0], [2])\n",
            "        2.0000000000000004\n",
            "        >>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])\n",
            "        1.0000000000000002\n",
            "        >>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],\n",
            "        ...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])\n",
            "        0.88003340976158217\n",
            "    \n",
            "    entropy(pk: 'np.typing.ArrayLike', qk: 'np.typing.ArrayLike | None' = None, base: 'float | None' = None, axis: 'int' = 0, *, nan_policy='propagate', keepdims=False) -> 'np.number | np.ndarray'\n",
            "        Calculate the Shannon entropy/relative entropy of given distribution(s).\n",
            "        \n",
            "        If only probabilities `pk` are given, the Shannon entropy is calculated as\n",
            "        ``H = -sum(pk * log(pk))``.\n",
            "        \n",
            "        If `qk` is not None, then compute the relative entropy\n",
            "        ``D = sum(pk * log(pk / qk))``. This quantity is also known\n",
            "        as the Kullback-Leibler divergence.\n",
            "        \n",
            "        This routine will normalize `pk` and `qk` if they don't sum to 1.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        pk : array_like\n",
            "            Defines the (discrete) distribution. Along each axis-slice of ``pk``,\n",
            "            element ``i`` is the  (possibly unnormalized) probability of event\n",
            "            ``i``.\n",
            "        qk : array_like, optional\n",
            "            Sequence against which the relative entropy is computed. Should be in\n",
            "            the same format as `pk`.\n",
            "        base : float, optional\n",
            "            The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        S : {float, array_like}\n",
            "            The calculated entropy.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Informally, the Shannon entropy quantifies the expected uncertainty\n",
            "        inherent in the possible outcomes of a discrete random variable.\n",
            "        For example,\n",
            "        if messages consisting of sequences of symbols from a set are to be\n",
            "        encoded and transmitted over a noiseless channel, then the Shannon entropy\n",
            "        ``H(pk)`` gives a tight lower bound for the average number of units of\n",
            "        information needed per symbol if the symbols occur with frequencies\n",
            "        governed by the discrete distribution `pk` [1]_. The choice of base\n",
            "        determines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\n",
            "        \n",
            "        The relative entropy, ``D(pk|qk)``, quantifies the increase in the average\n",
            "        number of units of information needed per symbol if the encoding is\n",
            "        optimized for the probability distribution `qk` instead of the true\n",
            "        distribution `pk`. Informally, the relative entropy quantifies the expected\n",
            "        excess in surprise experienced if one believes the true distribution is\n",
            "        `qk` when it is actually `pk`.\n",
            "        \n",
            "        A related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\n",
            "        equation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\n",
            "        the formula ``CE = -sum(pk * log(qk))``. It gives the average\n",
            "        number of units of information needed per symbol if an encoding is\n",
            "        optimized for the probability distribution `qk` when the true distribution\n",
            "        is `pk`. It is not computed directly by `entropy`, but it can be computed\n",
            "        using two calls to the function (see Examples).\n",
            "        \n",
            "        See [2]_ for more information.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\n",
            "               Bell System Technical Journal, 27: 379-423.\n",
            "               https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n",
            "        .. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\n",
            "               Theory (Wiley Series in Telecommunications and Signal Processing).\n",
            "               Wiley-Interscience, USA.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        The outcome of a fair coin is the most uncertain:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import entropy\n",
            "        >>> base = 2  # work in units of bits\n",
            "        >>> pk = np.array([1/2, 1/2])  # fair coin\n",
            "        >>> H = entropy(pk, base=base)\n",
            "        >>> H\n",
            "        1.0\n",
            "        >>> H == -np.sum(pk * np.log(pk)) / np.log(base)\n",
            "        True\n",
            "        \n",
            "        The outcome of a biased coin is less uncertain:\n",
            "        \n",
            "        >>> qk = np.array([9/10, 1/10])  # biased coin\n",
            "        >>> entropy(qk, base=base)\n",
            "        0.46899559358928117\n",
            "        \n",
            "        The relative entropy between the fair coin and biased coin is calculated\n",
            "        as:\n",
            "        \n",
            "        >>> D = entropy(pk, qk, base=base)\n",
            "        >>> D\n",
            "        0.7369655941662062\n",
            "        >>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\n",
            "        True\n",
            "        \n",
            "        The cross entropy can be calculated as the sum of the entropy and\n",
            "        relative entropy`:\n",
            "        \n",
            "        >>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\n",
            "        >>> CE\n",
            "        1.736965594166206\n",
            "        >>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\n",
            "        True\n",
            "    \n",
            "    epps_singleton_2samp(x, y, t=(0.4, 0.8), *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Compute the Epps-Singleton (ES) test statistic.\n",
            "        \n",
            "        Test the null hypothesis that two samples have the same underlying\n",
            "        probability distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array-like\n",
            "            The two samples of observations to be tested. Input must not have more\n",
            "            than one dimension. Samples can have different lengths.\n",
            "        t : array-like, optional\n",
            "            The points (t1, ..., tn) where the empirical characteristic function is\n",
            "            to be evaluated. It should be positive distinct numbers. The default\n",
            "            value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n",
            "            one dimension.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic.\n",
            "        pvalue : float\n",
            "            The associated p-value based on the asymptotic chi2-distribution.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`ks_2samp`, :func:`anderson_ksamp`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Testing whether two samples are generated by the same underlying\n",
            "        distribution is a classical question in statistics. A widely used test is\n",
            "        the Kolmogorov-Smirnov (KS) test which relies on the empirical\n",
            "        distribution function. Epps and Singleton introduce a test based on the\n",
            "        empirical characteristic function in [1]_.\n",
            "        \n",
            "        One advantage of the ES test compared to the KS test is that is does\n",
            "        not assume a continuous distribution. In [1]_, the authors conclude\n",
            "        that the test also has a higher power than the KS test in many\n",
            "        examples. They recommend the use of the ES test for discrete samples as\n",
            "        well as continuous samples with at least 25 observations each, whereas\n",
            "        `anderson_ksamp` is recommended for smaller sample sizes in the\n",
            "        continuous case.\n",
            "        \n",
            "        The p-value is computed from the asymptotic distribution of the test\n",
            "        statistic which follows a `chi2` distribution. If the sample size of both\n",
            "        `x` and `y` is below 25, the small sample correction proposed in [1]_ is\n",
            "        applied to the test statistic.\n",
            "        \n",
            "        The default values of `t` are determined in [1]_ by considering\n",
            "        various distributions and finding good values that lead to a high power\n",
            "        of the test in general. Table III in [1]_ gives the optimal values for\n",
            "        the distributions tested in that study. The values of `t` are scaled by\n",
            "        the semi-interquartile range in the implementation, see [1]_.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n",
            "           problem using the empirical characteristic function\", Journal of\n",
            "           Statistical Computation and Simulation 26, p. 177--203, 1986.\n",
            "        \n",
            "        .. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n",
            "           - the Epps-Singleton two-sample test using the empirical characteristic\n",
            "           function\", The Stata Journal 9(3), p. 454--465, 2009.\n",
            "    \n",
            "    expectile(a, alpha=0.5, *, weights=None)\n",
            "        Compute the expectile at the specified level.\n",
            "        \n",
            "        Expectiles are a generalization of the expectation in the same way as\n",
            "        quantiles are a generalization of the median. The expectile at level\n",
            "        `alpha = 0.5` is the mean (average). See Notes for more details.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array containing numbers whose expectile is desired.\n",
            "        alpha : float, default: 0.5\n",
            "            The level of the expectile; e.g., `alpha=0.5` gives the mean.\n",
            "        weights : array_like, optional\n",
            "            An array of weights associated with the values in `a`.\n",
            "            The `weights` must be broadcastable to the same shape as `a`.\n",
            "            Default is None, which gives each value a weight of 1.0.\n",
            "            An integer valued weight element acts like repeating the corresponding\n",
            "            observation in `a` that many times. See Notes for more details.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        expectile : ndarray\n",
            "            The empirical expectile at level `alpha`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.mean : Arithmetic average\n",
            "        numpy.quantile : Quantile\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        In general, the expectile at level :math:`\\alpha` of a random variable\n",
            "        :math:`X` with cumulative distribution function (CDF) :math:`F` is given\n",
            "        by the unique solution :math:`t` of:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\alpha E((X - t)_+) = (1 - \\alpha) E((t - X)_+) \\,.\n",
            "        \n",
            "        Here, :math:`(x)_+ = \\max(0, x)` is the positive part of :math:`x`.\n",
            "        This equation can be equivalently written as:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\alpha \\int_t^\\infty (x - t)\\mathrm{d}F(x)\n",
            "            = (1 - \\alpha) \\int_{-\\infty}^t (t - x)\\mathrm{d}F(x) \\,.\n",
            "        \n",
            "        The empirical expectile at level :math:`\\alpha` (`alpha`) of a sample\n",
            "        :math:`a_i` (the array `a`) is defined by plugging in the empirical CDF of\n",
            "        `a`. Given sample or case weights :math:`w` (the array `weights`), it\n",
            "        reads :math:`F_a(x) = \\frac{1}{\\sum_i w_i} \\sum_i w_i 1_{a_i \\leq x}`\n",
            "        with indicator function :math:`1_{A}`. This leads to the definition of the\n",
            "        empirical expectile at level `alpha` as the unique solution :math:`t` of:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\alpha \\sum_{i=1}^n w_i (a_i - t)_+ =\n",
            "                (1 - \\alpha) \\sum_{i=1}^n w_i (t - a_i)_+ \\,.\n",
            "        \n",
            "        For :math:`\\alpha=0.5`, this simplifies to the weighted average.\n",
            "        Furthermore, the larger :math:`\\alpha`, the larger the value of the\n",
            "        expectile.\n",
            "        \n",
            "        As a final remark, the expectile at level :math:`\\alpha` can also be\n",
            "        written as a minimization problem. One often used choice is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\operatorname{argmin}_t\n",
            "            E(\\lvert 1_{t\\geq X} - \\alpha\\rvert(t - X)^2) \\,.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] W. K. Newey and J. L. Powell (1987), \"Asymmetric Least Squares\n",
            "               Estimation and Testing,\" Econometrica, 55, 819-847.\n",
            "        .. [2] T. Gneiting (2009). \"Making and Evaluating Point Forecasts,\"\n",
            "               Journal of the American Statistical Association, 106, 746 - 762.\n",
            "               :doi:`10.48550/arXiv.0912.0902`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import expectile\n",
            "        >>> a = [1, 4, 2, -1]\n",
            "        >>> expectile(a, alpha=0.5) == np.mean(a)\n",
            "        True\n",
            "        >>> expectile(a, alpha=0.2)\n",
            "        0.42857142857142855\n",
            "        >>> expectile(a, alpha=0.8)\n",
            "        2.5714285714285716\n",
            "        >>> weights = [1, 3, 1, 1]\n",
            "    \n",
            "    f_oneway(*samples, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform one-way ANOVA.\n",
            "        \n",
            "        The one-way ANOVA tests the null hypothesis that two or more groups have\n",
            "        the same population mean.  The test is applied to samples from two or\n",
            "        more groups, possibly with differing sizes.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            The sample measurements for each group.  There must be at least\n",
            "            two arguments.  If the arrays are multidimensional, then all the\n",
            "            dimensions of the array must be the same except for `axis`.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The computed F statistic of the test.\n",
            "        pvalue : float\n",
            "            The associated p-value from the F distribution.\n",
            "        \n",
            "        Warns\n",
            "        -----\n",
            "        `~scipy.stats.ConstantInputWarning`\n",
            "            Raised if all values within each of the input arrays are identical.\n",
            "            In this case the F statistic is either infinite or isn't defined,\n",
            "            so ``np.inf`` or ``np.nan`` is returned.\n",
            "        `~scipy.stats.DegenerateDataWarning`\n",
            "            Raised if the length of any input array is 0, or if all the input\n",
            "            arrays have length 1.  ``np.nan`` is returned for the F statistic\n",
            "            and the p-value in these cases.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The ANOVA test has important assumptions that must be satisfied in order\n",
            "        for the associated p-value to be valid.\n",
            "        \n",
            "        1. The samples are independent.\n",
            "        2. Each sample is from a normally distributed population.\n",
            "        3. The population standard deviations of the groups are all equal.  This\n",
            "           property is known as homoscedasticity.\n",
            "        \n",
            "        If these assumptions are not true for a given set of data, it may still\n",
            "        be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n",
            "        the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n",
            "        some loss of power.\n",
            "        \n",
            "        The length of each group must be at least one, and there must be at\n",
            "        least one group with length greater than one.  If these conditions\n",
            "        are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n",
            "        is returned.\n",
            "        \n",
            "        If all values in each group are identical, and there exist at least two\n",
            "        groups with different values, the function generates a warning and\n",
            "        returns (``np.inf``, 0).\n",
            "        \n",
            "        If all values in all groups are the same, function generates a warning\n",
            "        and returns (``np.nan``, ``np.nan``).\n",
            "        \n",
            "        The algorithm is from Heiman [2]_, pp.394-7.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n",
            "               Chapter 14, 2014, http://vassarstats.net/textbook/\n",
            "        \n",
            "        .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n",
            "               integrated introduction for psychology\", Houghton, Mifflin and\n",
            "               Company, 2001.\n",
            "        \n",
            "        .. [3] G.H. McDonald, \"Handbook of Biological Statistics\", One-way ANOVA.\n",
            "               http://www.biostathandbook.com/onewayanova.html\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import f_oneway\n",
            "        \n",
            "        Here are some data [3]_ on a shell measurement (the length of the anterior\n",
            "        adductor muscle scar, standardized by dividing by length) in the mussel\n",
            "        Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n",
            "        Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n",
            "        much larger data set used in McDonald et al. (1991).\n",
            "        \n",
            "        >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n",
            "        ...              0.0659, 0.0923, 0.0836]\n",
            "        >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n",
            "        ...            0.0725]\n",
            "        >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n",
            "        >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n",
            "        ...            0.0689]\n",
            "        >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n",
            "        >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n",
            "        F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n",
            "        \n",
            "        `f_oneway` accepts multidimensional input arrays.  When the inputs\n",
            "        are multidimensional and `axis` is not given, the test is performed\n",
            "        along the first axis of the input arrays.  For the following data, the\n",
            "        test is performed three times, once for each column.\n",
            "        \n",
            "        >>> a = np.array([[9.87, 9.03, 6.81],\n",
            "        ...               [7.18, 8.35, 7.00],\n",
            "        ...               [8.39, 7.58, 7.68],\n",
            "        ...               [7.45, 6.33, 9.35],\n",
            "        ...               [6.41, 7.10, 9.33],\n",
            "        ...               [8.00, 8.24, 8.44]])\n",
            "        >>> b = np.array([[6.35, 7.30, 7.16],\n",
            "        ...               [6.65, 6.68, 7.63],\n",
            "        ...               [5.72, 7.73, 6.72],\n",
            "        ...               [7.01, 9.19, 7.41],\n",
            "        ...               [7.75, 7.87, 8.30],\n",
            "        ...               [6.90, 7.97, 6.97]])\n",
            "        >>> c = np.array([[3.31, 8.77, 1.01],\n",
            "        ...               [8.25, 3.24, 3.62],\n",
            "        ...               [6.32, 8.81, 5.19],\n",
            "        ...               [7.48, 8.83, 8.91],\n",
            "        ...               [8.59, 6.01, 6.07],\n",
            "        ...               [3.07, 9.72, 7.48]])\n",
            "        >>> F, p = f_oneway(a, b, c)\n",
            "        >>> F\n",
            "        array([1.75676344, 0.03701228, 3.76439349])\n",
            "        >>> p\n",
            "        array([0.20630784, 0.96375203, 0.04733157])\n",
            "    \n",
            "    false_discovery_control(ps, *, axis=0, method='bh')\n",
            "        Adjust p-values to control the false discovery rate.\n",
            "        \n",
            "        The false discovery rate (FDR) is the expected proportion of rejected null\n",
            "        hypotheses that are actually true.\n",
            "        If the null hypothesis is rejected when the *adjusted* p-value falls below\n",
            "        a specified level, the false discovery rate is controlled at that level.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        ps : 1D array_like\n",
            "            The p-values to adjust. Elements must be real numbers between 0 and 1.\n",
            "        axis : int\n",
            "            The axis along which to perform the adjustment. The adjustment is\n",
            "            performed independently along each axis-slice. If `axis` is None, `ps`\n",
            "            is raveled before performing the adjustment.\n",
            "        method : {'bh', 'by'}\n",
            "            The false discovery rate control procedure to apply: ``'bh'`` is for\n",
            "            Benjamini-Hochberg [1]_ (Eq. 1), ``'by'`` is for Benjaminini-Yekutieli\n",
            "            [2]_ (Theorem 1.3). The latter is more conservative, but it is\n",
            "            guaranteed to control the FDR even when the p-values are not from\n",
            "            independent tests.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        ps_adusted : array_like\n",
            "            The adjusted p-values. If the null hypothesis is rejected where these\n",
            "            fall below a specified level, the false discovery rate is controlled\n",
            "            at that level.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        combine_pvalues\n",
            "        statsmodels.stats.multitest.multipletests\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        In multiple hypothesis testing, false discovery control procedures tend to\n",
            "        offer higher power than familywise error rate control procedures (e.g.\n",
            "        Bonferroni correction [1]_).\n",
            "        \n",
            "        If the p-values correspond with independent tests (or tests with\n",
            "        \"positive regression dependencies\" [2]_), rejecting null hypotheses\n",
            "        corresponding with Benjamini-Hochberg-adjusted p-values below :math:`q`\n",
            "        controls the false discovery rate at a level less than or equal to\n",
            "        :math:`q m_0 / m`, where :math:`m_0` is the number of true null hypotheses\n",
            "        and :math:`m` is the total number of null hypotheses tested. The same is\n",
            "        true even for dependent tests when the p-values are adjusted accorded to\n",
            "        the more conservative Benjaminini-Yekutieli procedure.\n",
            "        \n",
            "        The adjusted p-values produced by this function are comparable to those\n",
            "        produced by the R function ``p.adjust`` and the statsmodels function\n",
            "        `statsmodels.stats.multitest.multipletests`. Please consider the latter\n",
            "        for more advanced methods of multiple comparison correction.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Benjamini, Yoav, and Yosef Hochberg. \"Controlling the false\n",
            "               discovery rate: a practical and powerful approach to multiple\n",
            "               testing.\" Journal of the Royal statistical society: series B\n",
            "               (Methodological) 57.1 (1995): 289-300.\n",
            "        \n",
            "        .. [2] Benjamini, Yoav, and Daniel Yekutieli. \"The control of the false\n",
            "               discovery rate in multiple testing under dependency.\" Annals of\n",
            "               statistics (2001): 1165-1188.\n",
            "        \n",
            "        .. [3] TileStats. FDR - Benjamini-Hochberg explained - Youtube.\n",
            "               https://www.youtube.com/watch?v=rZKa4tW2NKs.\n",
            "        \n",
            "        .. [4] Neuhaus, Karl-Ludwig, et al. \"Improved thrombolysis in acute\n",
            "               myocardial infarction with front-loaded administration of alteplase:\n",
            "               results of the rt-PA-APSAC patency study (TAPS).\" Journal of the\n",
            "               American College of Cardiology 19.5 (1992): 885-891.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We follow the example from [1]_.\n",
            "        \n",
            "            Thrombolysis with recombinant tissue-type plasminogen activator (rt-PA)\n",
            "            and anisoylated plasminogen streptokinase activator (APSAC) in\n",
            "            myocardial infarction has been proved to reduce mortality. [4]_\n",
            "            investigated the effects of a new front-loaded administration of rt-PA\n",
            "            versus those obtained with a standard regimen of APSAC, in a randomized\n",
            "            multicentre trial in 421 patients with acute myocardial infarction.\n",
            "        \n",
            "        There were four families of hypotheses tested in the study, the last of\n",
            "        which was \"cardiac and other events after the start of thrombolitic\n",
            "        treatment\". FDR control may be desired in this family of hypotheses\n",
            "        because it would not be appropriate to conclude that the front-loaded\n",
            "        treatment is better if it is merely equivalent to the previous treatment.\n",
            "        \n",
            "        The p-values corresponding with the 15 hypotheses in this family were\n",
            "        \n",
            "        >>> ps = [0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344,\n",
            "        ...       0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000]\n",
            "        \n",
            "        If the chosen significance level is 0.05, we may be tempted to reject the\n",
            "        null hypotheses for the tests corresponding with the first nine p-values,\n",
            "        as the first nine p-values fall below the chosen significance level.\n",
            "        However, this would ignore the problem of \"multiplicity\": if we fail to\n",
            "        correct for the fact that multiple comparisons are being performed, we\n",
            "        are more likely to incorrectly reject true null hypotheses.\n",
            "        \n",
            "        One approach to the multiplicity problem is to control the family-wise\n",
            "        error rate (FWER), that is, the rate at which the null hypothesis is\n",
            "        rejected when it is actually true. A common procedure of this kind is the\n",
            "        Bonferroni correction [1]_.  We begin by multiplying the p-values by the\n",
            "        number of hypotheses tested.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> np.array(ps) * len(ps)\n",
            "        array([1.5000e-03, 6.0000e-03, 2.8500e-02, 1.4250e-01, 3.0150e-01,\n",
            "               4.1700e-01, 4.4700e-01, 5.1600e-01, 6.8850e-01, 4.8600e+00,\n",
            "               6.3930e+00, 8.5785e+00, 9.7920e+00, 1.1385e+01, 1.5000e+01])\n",
            "        \n",
            "        To control the FWER at 5%, we reject only the hypotheses corresponding\n",
            "        with adjusted p-values less than 0.05. In this case, only the hypotheses\n",
            "        corresponding with the first three p-values can be rejected. According to\n",
            "        [1]_, these three hypotheses concerned \"allergic reaction\" and \"two\n",
            "        different aspects of bleeding.\"\n",
            "        \n",
            "        An alternative approach is to control the false discovery rate: the\n",
            "        expected fraction of rejected null hypotheses that are actually true. The\n",
            "        advantage of this approach is that it typically affords greater power: an\n",
            "        increased rate of rejecting the null hypothesis when it is indeed false. To\n",
            "        control the false discovery rate at 5%, we apply the Benjamini-Hochberg\n",
            "        p-value adjustment.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> stats.false_discovery_control(ps)\n",
            "        array([0.0015    , 0.003     , 0.0095    , 0.035625  , 0.0603    ,\n",
            "               0.06385714, 0.06385714, 0.0645    , 0.0765    , 0.486     ,\n",
            "               0.58118182, 0.714875  , 0.75323077, 0.81321429, 1.        ])\n",
            "        \n",
            "        Now, the first *four* adjusted p-values fall below 0.05, so we would reject\n",
            "        the null hypotheses corresponding with these *four* p-values. Rejection\n",
            "        of the fourth null hypothesis was particularly important to the original\n",
            "        study as it led to the conclusion that the new treatment had a\n",
            "        \"substantially lower in-hospital mortality rate.\"\n",
            "    \n",
            "    find_repeats(arr)\n",
            "        Find repeats and repeat counts.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        arr : array_like\n",
            "            Input array. This is cast to float64.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        values : ndarray\n",
            "            The unique values from the (flattened) input that are repeated.\n",
            "        \n",
            "        counts : ndarray\n",
            "            Number of times the corresponding 'value' is repeated.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        In numpy >= 1.9 `numpy.unique` provides similar functionality. The main\n",
            "        difference is that `find_repeats` only returns repeated values.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n",
            "        RepeatedResults(values=array([2.]), counts=array([4]))\n",
            "        \n",
            "        >>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n",
            "        RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))\n",
            "    \n",
            "    fisher_exact(table, alternative='two-sided')\n",
            "        Perform a Fisher exact test on a 2x2 contingency table.\n",
            "        \n",
            "        The null hypothesis is that the true odds ratio of the populations\n",
            "        underlying the observations is one, and the observations were sampled\n",
            "        from these populations under a condition: the marginals of the\n",
            "        resulting table must equal those of the observed table. The statistic\n",
            "        returned is the unconditional maximum likelihood estimate of the odds\n",
            "        ratio, and the p-value is the probability under the null hypothesis of\n",
            "        obtaining a table at least as extreme as the one that was actually\n",
            "        observed. There are other possible choices of statistic and two-sided\n",
            "        p-value definition associated with Fisher's exact test; please see the\n",
            "        Notes for more information.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        table : array_like of ints\n",
            "            A 2x2 contingency table.  Elements must be non-negative integers.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "        \n",
            "            * 'two-sided': the odds ratio of the underlying population is not one\n",
            "            * 'less': the odds ratio of the underlying population is less than one\n",
            "            * 'greater': the odds ratio of the underlying population is greater\n",
            "              than one\n",
            "        \n",
            "            See the Notes for more details.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SignificanceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                This is the prior odds ratio, not a posterior estimate.\n",
            "            pvalue : float\n",
            "                The probability under the null hypothesis of obtaining a\n",
            "                table at least as extreme as the one that was actually observed.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        chi2_contingency : Chi-square test of independence of variables in a\n",
            "            contingency table.  This can be used as an alternative to\n",
            "            `fisher_exact` when the numbers in the table are large.\n",
            "        contingency.odds_ratio : Compute the odds ratio (sample or conditional\n",
            "            MLE) for a 2x2 contingency table.\n",
            "        barnard_exact : Barnard's exact test, which is a more powerful alternative\n",
            "            than Fisher's exact test for 2x2 contingency tables.\n",
            "        boschloo_exact : Boschloo's exact test, which is a more powerful\n",
            "            alternative than Fisher's exact test for 2x2 contingency tables.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        *Null hypothesis and p-values*\n",
            "        \n",
            "        The null hypothesis is that the true odds ratio of the populations\n",
            "        underlying the observations is one, and the observations were sampled at\n",
            "        random from these populations under a condition: the marginals of the\n",
            "        resulting table must equal those of the observed table. Equivalently,\n",
            "        the null hypothesis is that the input table is from the hypergeometric\n",
            "        distribution with parameters (as used in `hypergeom`)\n",
            "        ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n",
            "        input table is ``[[a, b], [c, d]]``.  This distribution has support\n",
            "        ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n",
            "        in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n",
            "        can be interpreted as the upper-left element of a 2x2 table, so the\n",
            "        tables in the distribution have form::\n",
            "        \n",
            "            [  x           n - x     ]\n",
            "            [N - x    M - (n + N) + x]\n",
            "        \n",
            "        For example, if::\n",
            "        \n",
            "            table = [6  2]\n",
            "                    [1  4]\n",
            "        \n",
            "        then the support is ``2 <= x <= 7``, and the tables in the distribution\n",
            "        are::\n",
            "        \n",
            "            [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n",
            "            [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n",
            "        \n",
            "        The probability of each table is given by the hypergeometric distribution\n",
            "        ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n",
            "        three significant digits)::\n",
            "        \n",
            "            x       2      3      4      5       6        7\n",
            "            p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n",
            "        \n",
            "        These can be computed with::\n",
            "        \n",
            "            >>> import numpy as np\n",
            "            >>> from scipy.stats import hypergeom\n",
            "            >>> table = np.array([[6, 2], [1, 4]])\n",
            "            >>> M = table.sum()\n",
            "            >>> n = table[0].sum()\n",
            "            >>> N = table[:, 0].sum()\n",
            "            >>> start, end = hypergeom.support(M, n, N)\n",
            "            >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n",
            "            array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n",
            "                   0.004662  ])\n",
            "        \n",
            "        The two-sided p-value is the probability that, under the null hypothesis,\n",
            "        a random table would have a probability equal to or less than the\n",
            "        probability of the input table.  For our example, the probability of\n",
            "        the input table (where ``x = 6``) is 0.0816.  The x values where the\n",
            "        probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n",
            "        is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n",
            "        \n",
            "            >>> from scipy.stats import fisher_exact\n",
            "            >>> res = fisher_exact(table, alternative='two-sided')\n",
            "            >>> res.pvalue\n",
            "            0.10256410256410257\n",
            "        \n",
            "        The one-sided p-value for ``alternative='greater'`` is the probability\n",
            "        that a random table has ``x >= a``, which in our example is ``x >= 6``,\n",
            "        or ``0.0816 + 0.00466 ~= 0.08626``::\n",
            "        \n",
            "            >>> res = fisher_exact(table, alternative='greater')\n",
            "            >>> res.pvalue\n",
            "            0.08624708624708627\n",
            "        \n",
            "        This is equivalent to computing the survival function of the\n",
            "        distribution at ``x = 5`` (one less than ``x`` from the input table,\n",
            "        because we want to include the probability of ``x = 6`` in the sum)::\n",
            "        \n",
            "            >>> hypergeom.sf(5, M, n, N)\n",
            "            0.08624708624708627\n",
            "        \n",
            "        For ``alternative='less'``, the one-sided p-value is the probability\n",
            "        that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n",
            "        or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n",
            "        \n",
            "            >>> res = fisher_exact(table, alternative='less')\n",
            "            >>> res.pvalue\n",
            "            0.9953379953379957\n",
            "        \n",
            "        This is equivalent to computing the cumulative distribution function\n",
            "        of the distribution at ``x = 6``:\n",
            "        \n",
            "            >>> hypergeom.cdf(6, M, n, N)\n",
            "            0.9953379953379957\n",
            "        \n",
            "        *Odds ratio*\n",
            "        \n",
            "        The calculated odds ratio is different from the value computed by the\n",
            "        R function ``fisher.test``.  This implementation returns the \"sample\"\n",
            "        or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n",
            "        in R uses the conditional maximum likelihood estimate.  To compute the\n",
            "        conditional maximum likelihood estimate of the odds ratio, use\n",
            "        `scipy.stats.contingency.odds_ratio`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n",
            "               Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n",
            "        .. [2] \"Fisher's exact test\",\n",
            "               https://en.wikipedia.org/wiki/Fisher's_exact_test\n",
            "        .. [3] Emma V. Low et al. \"Identifying the lowest effective dose of\n",
            "               acetazolamide for the prophylaxis of acute mountain sickness:\n",
            "               systematic review and meta-analysis.\"\n",
            "               BMJ, 345, :doi:`10.1136/bmj.e6779`, 2012.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [3]_, the effective dose of acetazolamide for the prophylaxis of acute\n",
            "        mountain sickness was investigated. The study notably concluded:\n",
            "        \n",
            "            Acetazolamide 250 mg, 500 mg, and 750 mg daily were all efficacious for\n",
            "            preventing acute mountain sickness. Acetazolamide 250 mg was the lowest\n",
            "            effective dose with available evidence for this indication.\n",
            "        \n",
            "        The following table summarizes the results of the experiment in which\n",
            "        some participants took a daily dose of acetazolamide 250 mg while others\n",
            "        took a placebo.\n",
            "        Cases of acute mountain sickness were recorded::\n",
            "        \n",
            "                                        Acetazolamide   Control/Placebo\n",
            "            Acute mountain sickness            7           17\n",
            "            No                                15            5\n",
            "        \n",
            "        \n",
            "        Is there evidence that the acetazolamide 250 mg reduces the risk of\n",
            "        acute mountain sickness?\n",
            "        We begin by formulating a null hypothesis :math:`H_0`:\n",
            "        \n",
            "            The odds of experiencing acute mountain sickness are the same with\n",
            "            the acetazolamide treatment as they are with placebo.\n",
            "        \n",
            "        Let's assess the plausibility of this hypothesis with\n",
            "        Fisher's test.\n",
            "        \n",
            "        >>> from scipy.stats import fisher_exact\n",
            "        >>> res = fisher_exact([[7, 17], [15, 5]], alternative='less')\n",
            "        >>> res.statistic\n",
            "        0.13725490196078433\n",
            "        >>> res.pvalue\n",
            "        0.0028841933752349743\n",
            "        \n",
            "        Using a significance level of 5%, we would reject the null hypothesis in\n",
            "        favor of the alternative hypothesis: \"The odds of experiencing acute\n",
            "        mountain sickness with acetazolamide treatment are less than the odds of\n",
            "        experiencing acute mountain sickness with placebo.\"\n",
            "        \n",
            "        .. note::\n",
            "        \n",
            "            Because the null distribution of Fisher's exact test is formed under\n",
            "            the assumption that both row and column sums are fixed, the result of\n",
            "            the test are conservative when applied to an experiment in which the\n",
            "            row sums are not fixed.\n",
            "        \n",
            "            In this case, the column sums are fixed; there are 22 subjects in each\n",
            "            group. But the number of cases of acute mountain sickness is not\n",
            "            (and cannot be) fixed before conducting the experiment. It is a\n",
            "            consequence.\n",
            "        \n",
            "            Boschloo's test does not depend on the assumption that the row sums\n",
            "            are fixed, and consequently, it provides a more powerful test in this\n",
            "            situation.\n",
            "        \n",
            "            >>> from scipy.stats import boschloo_exact\n",
            "            >>> res = boschloo_exact([[7, 17], [15, 5]], alternative='less')\n",
            "            >>> res.statistic\n",
            "            0.0028841933752349743\n",
            "            >>> res.pvalue\n",
            "            0.0015141406667567101\n",
            "        \n",
            "            We verify that the p-value is less than with `fisher_exact`.\n",
            "    \n",
            "    fit(dist, data, bounds=None, *, guess=None, method='mle', optimizer=<function differential_evolution at 0x7880194f7a60>)\n",
            "        Fit a discrete or continuous distribution to data\n",
            "        \n",
            "        Given a distribution, data, and bounds on the parameters of the\n",
            "        distribution, return maximum likelihood estimates of the parameters.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        dist : `scipy.stats.rv_continuous` or `scipy.stats.rv_discrete`\n",
            "            The object representing the distribution to be fit to the data.\n",
            "        data : 1D array_like\n",
            "            The data to which the distribution is to be fit. If the data contain\n",
            "            any of ``np.nan``, ``np.inf``, or -``np.inf``, the fit method will\n",
            "            raise a ``ValueError``.\n",
            "        bounds : dict or sequence of tuples, optional\n",
            "            If a dictionary, each key is the name of a parameter of the\n",
            "            distribution, and the corresponding value is a tuple containing the\n",
            "            lower and upper bound on that parameter.  If the distribution is\n",
            "            defined only for a finite range of values of that parameter, no entry\n",
            "            for that parameter is required; e.g., some distributions have\n",
            "            parameters which must be on the interval [0, 1]. Bounds for parameters\n",
            "            location (``loc``) and scale (``scale``) are optional; by default,\n",
            "            they are fixed to 0 and 1, respectively.\n",
            "        \n",
            "            If a sequence, element *i* is a tuple containing the lower and upper\n",
            "            bound on the *i*\\ th parameter of the distribution. In this case,\n",
            "            bounds for *all* distribution shape parameters must be provided.\n",
            "            Optionally, bounds for location and scale may follow the\n",
            "            distribution shape parameters.\n",
            "        \n",
            "            If a shape is to be held fixed (e.g. if it is known), the\n",
            "            lower and upper bounds may be equal. If a user-provided lower or upper\n",
            "            bound is beyond a bound of the domain for which the distribution is\n",
            "            defined, the bound of the distribution's domain will replace the\n",
            "            user-provided value. Similarly, parameters which must be integral\n",
            "            will be constrained to integral values within the user-provided bounds.\n",
            "        guess : dict or array_like, optional\n",
            "            If a dictionary, each key is the name of a parameter of the\n",
            "            distribution, and the corresponding value is a guess for the value\n",
            "            of the parameter.\n",
            "        \n",
            "            If a sequence, element *i* is a guess for the *i*\\ th parameter of the\n",
            "            distribution. In this case, guesses for *all* distribution shape\n",
            "            parameters must be provided.\n",
            "        \n",
            "            If `guess` is not provided, guesses for the decision variables will\n",
            "            not be passed to the optimizer. If `guess` is provided, guesses for\n",
            "            any missing parameters will be set at the mean of the lower and\n",
            "            upper bounds. Guesses for parameters which must be integral will be\n",
            "            rounded to integral values, and guesses that lie outside the\n",
            "            intersection of the user-provided bounds and the domain of the\n",
            "            distribution will be clipped.\n",
            "        method : {'mle', 'mse'}\n",
            "            With ``method=\"mle\"`` (default), the fit is computed by minimizing\n",
            "            the negative log-likelihood function. A large, finite penalty\n",
            "            (rather than infinite negative log-likelihood) is applied for\n",
            "            observations beyond the support of the distribution.\n",
            "            With ``method=\"mse\"``, the fit is computed by minimizing\n",
            "            the negative log-product spacing function. The same penalty is applied\n",
            "            for observations beyond the support. We follow the approach of [1]_,\n",
            "            which is generalized for samples with repeated observations.\n",
            "        optimizer : callable, optional\n",
            "            `optimizer` is a callable that accepts the following positional\n",
            "            argument.\n",
            "        \n",
            "            fun : callable\n",
            "                The objective function to be optimized. `fun` accepts one argument\n",
            "                ``x``, candidate shape parameters of the distribution, and returns\n",
            "                the objective function value given ``x``, `dist`, and the provided\n",
            "                `data`.\n",
            "                The job of `optimizer` is to find values of the decision variables\n",
            "                that minimizes `fun`.\n",
            "        \n",
            "            `optimizer` must also accept the following keyword argument.\n",
            "        \n",
            "            bounds : sequence of tuples\n",
            "                The bounds on values of the decision variables; each element will\n",
            "                be a tuple containing the lower and upper bound on a decision\n",
            "                variable.\n",
            "        \n",
            "            If `guess` is provided, `optimizer` must also accept the following\n",
            "            keyword argument.\n",
            "        \n",
            "            x0 : array_like\n",
            "                The guesses for each decision variable.\n",
            "        \n",
            "            If the distribution has any shape parameters that must be integral or\n",
            "            if the distribution is discrete and the location parameter is not\n",
            "            fixed, `optimizer` must also accept the following keyword argument.\n",
            "        \n",
            "            integrality : array_like of bools\n",
            "                For each decision variable, True if the decision variable\n",
            "                must be constrained to integer values and False if the decision\n",
            "                variable is continuous.\n",
            "        \n",
            "            `optimizer` must return an object, such as an instance of\n",
            "            `scipy.optimize.OptimizeResult`, which holds the optimal values of\n",
            "            the decision variables in an attribute ``x``. If attributes\n",
            "            ``fun``, ``status``, or ``message`` are provided, they will be\n",
            "            included in the result object returned by `fit`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.FitResult`\n",
            "            An object with the following fields.\n",
            "        \n",
            "            params : namedtuple\n",
            "                A namedtuple containing the maximum likelihood estimates of the\n",
            "                shape parameters, location, and (if applicable) scale of the\n",
            "                distribution.\n",
            "            success : bool or None\n",
            "                Whether the optimizer considered the optimization to terminate\n",
            "                successfully or not.\n",
            "            message : str or None\n",
            "                Any status message provided by the optimizer.\n",
            "        \n",
            "            The object has the following method:\n",
            "        \n",
            "            nllf(params=None, data=None)\n",
            "                By default, the negative log-likehood function at the fitted\n",
            "                `params` for the given `data`. Accepts a tuple containing\n",
            "                alternative shapes, location, and scale of the distribution and\n",
            "                an array of alternative data.\n",
            "        \n",
            "            plot(ax=None)\n",
            "                Superposes the PDF/PMF of the fitted distribution over a normalized\n",
            "                histogram of the data.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        rv_continuous,  rv_discrete\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Optimization is more likely to converge to the maximum likelihood estimate\n",
            "        when the user provides tight bounds containing the maximum likelihood\n",
            "        estimate. For example, when fitting a binomial distribution to data, the\n",
            "        number of experiments underlying each sample may be known, in which case\n",
            "        the corresponding shape parameter ``n`` can be fixed.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Shao, Yongzhao, and Marjorie G. Hahn. \"Maximum product of spacings\n",
            "               method: a unified formulation with illustration of strong\n",
            "               consistency.\" Illinois Journal of Mathematics 43.3 (1999): 489-499.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to fit a distribution to the following data.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> dist = stats.nbinom\n",
            "        >>> shapes = (5, 0.5)\n",
            "        >>> data = dist.rvs(*shapes, size=1000, random_state=rng)\n",
            "        \n",
            "        Suppose we do not know how the data were generated, but we suspect that\n",
            "        it follows a negative binomial distribution with parameters *n* and *p*\\.\n",
            "        (See `scipy.stats.nbinom`.) We believe that the parameter *n* was fewer\n",
            "        than 30, and we know that the parameter *p* must lie on the interval\n",
            "        [0, 1]. We record this information in a variable `bounds` and pass\n",
            "        this information to `fit`.\n",
            "        \n",
            "        >>> bounds = [(0, 30), (0, 1)]\n",
            "        >>> res = stats.fit(dist, data, bounds)\n",
            "        \n",
            "        `fit` searches within the user-specified `bounds` for the\n",
            "        values that best match the data (in the sense of maximum likelihood\n",
            "        estimation). In this case, it found shape values similar to those\n",
            "        from which the data were actually generated.\n",
            "        \n",
            "        >>> res.params\n",
            "        FitParams(n=5.0, p=0.5028157644634368, loc=0.0)  # may vary\n",
            "        \n",
            "        We can visualize the results by superposing the probability mass function\n",
            "        of the distribution (with the shapes fit to the data) over a normalized\n",
            "        histogram of the data.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt  # matplotlib must be installed to plot\n",
            "        >>> res.plot()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Note that the estimate for *n* was exactly integral; this is because\n",
            "        the domain of the `nbinom` PMF includes only integral *n*, and the `nbinom`\n",
            "        object \"knows\" that. `nbinom` also knows that the shape *p* must be a\n",
            "        value between 0 and 1. In such a case - when the domain of the distribution\n",
            "        with respect to a parameter is finite - we are not required to specify\n",
            "        bounds for the parameter.\n",
            "        \n",
            "        >>> bounds = {'n': (0, 30)}  # omit parameter p using a `dict`\n",
            "        >>> res2 = stats.fit(dist, data, bounds)\n",
            "        >>> res2.params\n",
            "        FitParams(n=5.0, p=0.5016492009232932, loc=0.0)  # may vary\n",
            "        \n",
            "        If we wish to force the distribution to be fit with *n* fixed at 6, we can\n",
            "        set both the lower and upper bounds on *n* to 6. Note, however, that the\n",
            "        value of the objective function being optimized is typically worse (higher)\n",
            "        in this case.\n",
            "        \n",
            "        >>> bounds = {'n': (6, 6)}  # fix parameter `n`\n",
            "        >>> res3 = stats.fit(dist, data, bounds)\n",
            "        >>> res3.params\n",
            "        FitParams(n=6.0, p=0.5486556076755706, loc=0.0)  # may vary\n",
            "        >>> res3.nllf() > res.nllf()\n",
            "        True  # may vary\n",
            "        \n",
            "        Note that the numerical results of the previous examples are typical, but\n",
            "        they may vary because the default optimizer used by `fit`,\n",
            "        `scipy.optimize.differential_evolution`, is stochastic. However, we can\n",
            "        customize the settings used by the optimizer to ensure reproducibility -\n",
            "        or even use a different optimizer entirely - using the `optimizer`\n",
            "        parameter.\n",
            "        \n",
            "        >>> from scipy.optimize import differential_evolution\n",
            "        >>> rng = np.random.default_rng(767585560716548)\n",
            "        >>> def optimizer(fun, bounds, *, integrality):\n",
            "        ...     return differential_evolution(fun, bounds, strategy='best2bin',\n",
            "        ...                                   seed=rng, integrality=integrality)\n",
            "        >>> bounds = [(0, 30), (0, 1)]\n",
            "        >>> res4 = stats.fit(dist, data, bounds, optimizer=optimizer)\n",
            "        >>> res4.params\n",
            "        FitParams(n=5.0, p=0.5015183149259951, loc=0.0)\n",
            "    \n",
            "    fligner(*samples, center='median', proportiontocut=0.05, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform Fligner-Killeen test for equality of variance.\n",
            "        \n",
            "        Fligner's test tests the null hypothesis that all input samples\n",
            "        are from populations with equal variances.  Fligner-Killeen's test is\n",
            "        distribution free when populations are identical [2]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            Arrays of sample data.  Need not be the same length.\n",
            "        center : {'mean', 'median', 'trimmed'}, optional\n",
            "            Keyword argument controlling which function of the data is used in\n",
            "            computing the test statistic.  The default is 'median'.\n",
            "        proportiontocut : float, optional\n",
            "            When `center` is 'trimmed', this gives the proportion of data points\n",
            "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
            "            Default is 0.05.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic.\n",
            "        pvalue : float\n",
            "            The p-value for the hypothesis test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`bartlett`\n",
            "            A parametric test for equality of k variances in normal samples\n",
            "        :func:`levene`\n",
            "            A robust parametric test for equality of k variances\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        As with Levene's test there are three variants of Fligner's test that\n",
            "        differ by the measure of central tendency used in the test.  See `levene`\n",
            "        for more information.\n",
            "        \n",
            "        Conover et al. (1981) examine many of the existing parametric and\n",
            "        nonparametric tests by extensive simulations and they conclude that the\n",
            "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
            "        superior in terms of robustness of departures from normality and power\n",
            "        [3]_.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
            "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
            "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
            "               University.\n",
            "               https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf\n",
            "        .. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample\n",
            "               tests for scale. 'Journal of the American Statistical Association.'\n",
            "               71(353), 210-213.\n",
            "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
            "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
            "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
            "               University.\n",
            "        .. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A\n",
            "               comparative study of tests for homogeneity of variances, with\n",
            "               applications to the outer continental shelf bidding data.\n",
            "               Technometrics, 23(4), 351-361.\n",
            "        .. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n",
            "               Reference to the Vitamins, pp 499-503,\n",
            "               :doi:`10.1016/C2013-0-12584-6`.\n",
            "        .. [6] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [7] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n",
            "               superior to t and F tests in biomedical research. The American\n",
            "               Statistician, 52(2), 127-132.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [5]_, the influence of vitamin C on the tooth growth of guinea pigs\n",
            "        was investigated. In a control study, 60 subjects were divided into\n",
            "        small dose, medium dose, and large dose groups that received\n",
            "        daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\n",
            "        After 42 days, the tooth growth was measured.\n",
            "        \n",
            "        The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\n",
            "        tooth growth measurements of the three groups in microns.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> small_dose = np.array([\n",
            "        ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n",
            "        ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n",
            "        ... ])\n",
            "        >>> medium_dose = np.array([\n",
            "        ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n",
            "        ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n",
            "        ... ])\n",
            "        >>> large_dose = np.array([\n",
            "        ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n",
            "        ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n",
            "        ... ])\n",
            "        \n",
            "        The `fligner` statistic is sensitive to differences in variances\n",
            "        between the samples.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.fligner(small_dose, medium_dose, large_dose)\n",
            "        >>> res.statistic\n",
            "        1.3878943408857916\n",
            "        \n",
            "        The value of the statistic tends to be high when there is a large\n",
            "        difference in variances.\n",
            "        \n",
            "        We can test for inequality of variance among the groups by comparing the\n",
            "        observed value of the statistic against the null distribution: the\n",
            "        distribution of statistic values derived under the null hypothesis that\n",
            "        the population variances of the three groups are equal.\n",
            "        \n",
            "        For this test, the null distribution follows the chi-square distribution\n",
            "        as shown below.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> k = 3  # number of samples\n",
            "        >>> dist = stats.chi2(df=k-1)\n",
            "        >>> val = np.linspace(0, 8, 100)\n",
            "        >>> pdf = dist.pdf(val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(val, pdf, color='C0')\n",
            "        ...     ax.set_title(\"Fligner Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        ...     ax.set_xlim(0, 8)\n",
            "        ...     ax.set_ylim(0, 0.5)\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution greater than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n",
            "        >>> i = val >= res.statistic\n",
            "        >>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> res.pvalue\n",
            "        0.49960016501182125\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from distributions with identical variances that produces\n",
            "        such an extreme value of the statistic - this may be taken as evidence\n",
            "        against the null hypothesis in favor of the alternative: the variances of\n",
            "        the groups are not equal. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [6]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        - Small p-values are not evidence for a *large* effect; rather, they can\n",
            "          only provide evidence for a \"significant\" effect, meaning that they are\n",
            "          unlikely to have occurred under the null hypothesis.\n",
            "        \n",
            "        Note that the chi-square distribution provides an asymptotic approximation\n",
            "        of the null distribution.\n",
            "        For small samples, it may be more appropriate to perform a\n",
            "        permutation test: Under the null hypothesis that all three samples were\n",
            "        drawn from the same population, each of the measurements is equally likely\n",
            "        to have been observed in any of the three samples. Therefore, we can form\n",
            "        a randomized null distribution by calculating the statistic under many\n",
            "        randomly-generated partitionings of the observations into the three\n",
            "        samples.\n",
            "        \n",
            "        >>> def statistic(*samples):\n",
            "        ...     return stats.fligner(*samples).statistic\n",
            "        >>> ref = stats.permutation_test(\n",
            "        ...     (small_dose, medium_dose, large_dose), statistic,\n",
            "        ...     permutation_type='independent', alternative='greater'\n",
            "        ... )\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> bins = np.linspace(0, 8, 25)\n",
            "        >>> ax.hist(\n",
            "        ...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n",
            "        ... )\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'randomized null distribution'])\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> ref.pvalue  # randomized test p-value\n",
            "        0.4332  # may vary\n",
            "        \n",
            "        Note that there is significant disagreement between the p-value calculated\n",
            "        here and the asymptotic approximation returned by `fligner` above.\n",
            "        The statistical inferences that can be drawn rigorously from a permutation\n",
            "        test are limited; nonetheless, they may be the preferred approach in many\n",
            "        circumstances [7]_.\n",
            "        \n",
            "        Following is another generic example where the null hypothesis would be\n",
            "        rejected.\n",
            "        \n",
            "        Test whether the lists `a`, `b` and `c` come from populations\n",
            "        with equal variances.\n",
            "        \n",
            "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
            "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
            "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
            "        >>> stat, p = stats.fligner(a, b, c)\n",
            "        >>> p\n",
            "        0.00450826080004775\n",
            "        \n",
            "        The small p-value suggests that the populations do not have equal\n",
            "        variances.\n",
            "        \n",
            "        This is not surprising, given that the sample variance of `b` is much\n",
            "        larger than that of `a` and `c`:\n",
            "        \n",
            "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
            "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
            "    \n",
            "    friedmanchisquare(*samples, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Compute the Friedman test for repeated samples.\n",
            "        \n",
            "        The Friedman test tests the null hypothesis that repeated samples of\n",
            "        the same individuals have the same distribution.  It is often used\n",
            "        to test for consistency among samples obtained in different ways.\n",
            "        For example, if two sampling techniques are used on the same set of\n",
            "        individuals, the Friedman test can be used to determine if the two\n",
            "        sampling techniques are consistent.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, sample3... : array_like\n",
            "            Arrays of observations.  All of the arrays must have the same number\n",
            "            of elements.  At least three samples must be given.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic, correcting for ties.\n",
            "        pvalue : float\n",
            "            The associated p-value assuming that the test statistic has a chi\n",
            "            squared distribution.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Due to the assumption that the test statistic has a chi squared\n",
            "        distribution, the p-value is only reliable for n > 10 and more than\n",
            "        6 repeated samples.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Friedman_test\n",
            "        .. [2] P. Sprent and N.C. Smeeton, \"Applied Nonparametric Statistical\n",
            "               Methods, Third Edition\". Chapter 6, Section 6.3.2.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [2]_, the pulse rate (per minute) of a group of seven students was\n",
            "        measured before exercise, immediately after exercise and 5 minutes\n",
            "        after exercise. Is there evidence to suggest that the pulse rates on\n",
            "        these three occasions are similar?\n",
            "        \n",
            "        We begin by formulating a null hypothesis :math:`H_0`:\n",
            "        \n",
            "            The pulse rates are identical on these three occasions.\n",
            "        \n",
            "        Let's assess the plausibility of this hypothesis with a Friedman test.\n",
            "        \n",
            "        >>> from scipy.stats import friedmanchisquare\n",
            "        >>> before = [72, 96, 88, 92, 74, 76, 82]\n",
            "        >>> immediately_after = [120, 120, 132, 120, 101, 96, 112]\n",
            "        >>> five_min_after = [76, 95, 104, 96, 84, 72, 76]\n",
            "        >>> res = friedmanchisquare(before, immediately_after, five_min_after)\n",
            "        >>> res.statistic\n",
            "        10.57142857142857\n",
            "        >>> res.pvalue\n",
            "        0.005063414171757498\n",
            "        \n",
            "        Using a significance level of 5%, we would reject the null hypothesis in\n",
            "        favor of the alternative hypothesis: \"the pulse rates are different on\n",
            "        these three occasions\".\n",
            "    \n",
            "    gmean(a, axis=0, dtype=None, weights=None, *, nan_policy='propagate', keepdims=False)\n",
            "        Compute the weighted geometric mean along the specified axis.\n",
            "        \n",
            "        The weighted geometric mean of the array :math:`a_i` associated to weights\n",
            "        :math:`w_i` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\exp \\left( \\frac{ \\sum_{i=1}^n w_i \\ln a_i }{ \\sum_{i=1}^n w_i }\n",
            "                       \\right) \\, ,\n",
            "        \n",
            "        and, with equal weights, it gives:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\sqrt[n]{ \\prod_{i=1}^n a_i } \\, .\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array or object that can be converted to an array.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        dtype : dtype, optional\n",
            "            Type to which the input arrays are cast before the calculation is\n",
            "            performed.\n",
            "        weights : array_like, optional\n",
            "            The `weights` array must be broadcastable to the same shape as `a`.\n",
            "            Default is None, which gives each value a weight of 1.0.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        gmean : ndarray\n",
            "            See `dtype` parameter above.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`numpy.mean`\n",
            "            Arithmetic average\n",
            "        :func:`numpy.average`\n",
            "            Weighted average\n",
            "        :func:`hmean`\n",
            "            Harmonic mean\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Weighted Geometric Mean\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Weighted_geometric_mean.\n",
            "        .. [2] Grossman, J., Grossman, M., Katz, R., \"Averages: A New Approach\",\n",
            "               Archimedes Foundation, 1983\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import gmean\n",
            "        >>> gmean([1, 4])\n",
            "        2.0\n",
            "        >>> gmean([1, 2, 3, 4, 5, 6, 7])\n",
            "        3.3800151591412964\n",
            "        >>> gmean([1, 4, 7], weights=[3, 1, 3])\n",
            "        2.80668351922014\n",
            "    \n",
            "    goodness_of_fit(dist, data, *, known_params=None, fit_params=None, guessed_params=None, statistic='ad', n_mc_samples=9999, random_state=None)\n",
            "        Perform a goodness of fit test comparing data to a distribution family.\n",
            "        \n",
            "        Given a distribution family and data, perform a test of the null hypothesis\n",
            "        that the data were drawn from a distribution in that family. Any known\n",
            "        parameters of the distribution may be specified. Remaining parameters of\n",
            "        the distribution will be fit to the data, and the p-value of the test\n",
            "        is computed accordingly. Several statistics for comparing the distribution\n",
            "        to data are available.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        dist : `scipy.stats.rv_continuous`\n",
            "            The object representing the distribution family under the null\n",
            "            hypothesis.\n",
            "        data : 1D array_like\n",
            "            Finite, uncensored data to be tested.\n",
            "        known_params : dict, optional\n",
            "            A dictionary containing name-value pairs of known distribution\n",
            "            parameters. Monte Carlo samples are randomly drawn from the\n",
            "            null-hypothesized distribution with these values of the parameters.\n",
            "            Before the statistic is evaluated for each Monte Carlo sample, only\n",
            "            remaining unknown parameters of the null-hypothesized distribution\n",
            "            family are fit to the samples; the known parameters are held fixed.\n",
            "            If all parameters of the distribution family are known, then the step\n",
            "            of fitting the distribution family to each sample is omitted.\n",
            "        fit_params : dict, optional\n",
            "            A dictionary containing name-value pairs of distribution parameters\n",
            "            that have already been fit to the data, e.g. using `scipy.stats.fit`\n",
            "            or the ``fit`` method of `dist`. Monte Carlo samples are drawn from the\n",
            "            null-hypothesized distribution with these specified values of the\n",
            "            parameter. On those Monte Carlo samples, however, these and all other\n",
            "            unknown parameters of the null-hypothesized distribution family are\n",
            "            fit before the statistic is evaluated.\n",
            "        guessed_params : dict, optional\n",
            "            A dictionary containing name-value pairs of distribution parameters\n",
            "            which have been guessed. These parameters are always considered as\n",
            "            free parameters and are fit both to the provided `data` as well as\n",
            "            to the Monte Carlo samples drawn from the null-hypothesized\n",
            "            distribution. The purpose of these `guessed_params` is to be used as\n",
            "            initial values for the numerical fitting procedure.\n",
            "        statistic : {\"ad\", \"ks\", \"cvm\", \"filliben\"} or callable, optional\n",
            "            The statistic used to compare data to a distribution after fitting\n",
            "            unknown parameters of the distribution family to the data. The\n",
            "            Anderson-Darling (\"ad\") [1]_, Kolmogorov-Smirnov (\"ks\") [1]_,\n",
            "            Cramer-von Mises (\"cvm\") [1]_, and Filliben (\"filliben\") [7]_\n",
            "            statistics are available.  Alternatively, a callable with signature\n",
            "            ``(dist, data, axis)`` may be supplied to compute the statistic. Here\n",
            "            ``dist`` is a frozen distribution object (potentially with array\n",
            "            parameters), ``data`` is an array of Monte Carlo samples (of\n",
            "            compatible shape), and ``axis`` is the axis of ``data`` along which\n",
            "            the statistic must be computed.\n",
            "        n_mc_samples : int, default: 9999\n",
            "            The number of Monte Carlo samples drawn from the null hypothesized\n",
            "            distribution to form the null distribution of the statistic. The\n",
            "            sample size of each is the same as the given `data`.\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                        `numpy.random.RandomState`}, optional\n",
            "        \n",
            "            Pseudorandom number generator state used to generate the Monte Carlo\n",
            "            samples.\n",
            "        \n",
            "            If `random_state` is ``None`` (default), the\n",
            "            `numpy.random.RandomState` singleton is used.\n",
            "            If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `random_state`.\n",
            "            If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "            instance, then the provided instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : GoodnessOfFitResult\n",
            "            An object with the following attributes.\n",
            "        \n",
            "            fit_result : `~scipy.stats._result_classes.FitResult`\n",
            "                An object representing the fit of the provided `dist` to `data`.\n",
            "                This  object includes the values of distribution family parameters\n",
            "                that fully define the null-hypothesized distribution, that is,\n",
            "                the distribution from which Monte Carlo samples are drawn.\n",
            "            statistic : float\n",
            "                The value of the statistic comparing provided `data` to the\n",
            "                null-hypothesized distribution.\n",
            "            pvalue : float\n",
            "                The proportion of elements in the null distribution with\n",
            "                statistic values at least as extreme as the statistic value of the\n",
            "                provided `data`.\n",
            "            null_distribution : ndarray\n",
            "                The value of the statistic for each Monte Carlo sample\n",
            "                drawn from the null-hypothesized distribution.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This is a generalized Monte Carlo goodness-of-fit procedure, special cases\n",
            "        of which correspond with various Anderson-Darling tests, Lilliefors' test,\n",
            "        etc. The test is described in [2]_, [3]_, and [4]_ as a parametric\n",
            "        bootstrap test. This is a Monte Carlo test in which parameters that\n",
            "        specify the distribution from which samples are drawn have been estimated\n",
            "        from the data. We describe the test using \"Monte Carlo\" rather than\n",
            "        \"parametric bootstrap\" throughout to avoid confusion with the more familiar\n",
            "        nonparametric bootstrap, and describe how the test is performed below.\n",
            "        \n",
            "        *Traditional goodness of fit tests*\n",
            "        \n",
            "        Traditionally, critical values corresponding with a fixed set of\n",
            "        significance levels are pre-calculated using Monte Carlo methods. Users\n",
            "        perform the test by calculating the value of the test statistic only for\n",
            "        their observed `data` and comparing this value to tabulated critical\n",
            "        values. This practice is not very flexible, as tables are not available for\n",
            "        all distributions and combinations of known and unknown parameter values.\n",
            "        Also, results can be inaccurate when critical values are interpolated from\n",
            "        limited tabulated data to correspond with the user's sample size and\n",
            "        fitted parameter values. To overcome these shortcomings, this function\n",
            "        allows the user to perform the Monte Carlo trials adapted to their\n",
            "        particular data.\n",
            "        \n",
            "        *Algorithmic overview*\n",
            "        \n",
            "        In brief, this routine executes the following steps:\n",
            "        \n",
            "          1. Fit unknown parameters to the given `data`, thereby forming the\n",
            "             \"null-hypothesized\" distribution, and compute the statistic of\n",
            "             this pair of data and distribution.\n",
            "          2. Draw random samples from this null-hypothesized distribution.\n",
            "          3. Fit the unknown parameters to each random sample.\n",
            "          4. Calculate the statistic between each sample and the distribution that\n",
            "             has been fit to the sample.\n",
            "          5. Compare the value of the statistic corresponding with `data` from (1)\n",
            "             against the values of the statistic corresponding with the random\n",
            "             samples from (4). The p-value is the proportion of samples with a\n",
            "             statistic value greater than or equal to the statistic of the observed\n",
            "             data.\n",
            "        \n",
            "        In more detail, the steps are as follows.\n",
            "        \n",
            "        First, any unknown parameters of the distribution family specified by\n",
            "        `dist` are fit to the provided `data` using maximum likelihood estimation.\n",
            "        (One exception is the normal distribution with unknown location and scale:\n",
            "        we use the bias-corrected standard deviation ``np.std(data, ddof=1)`` for\n",
            "        the scale as recommended in [1]_.)\n",
            "        These values of the parameters specify a particular member of the\n",
            "        distribution family referred to as the \"null-hypothesized distribution\",\n",
            "        that is, the distribution from which the data were sampled under the null\n",
            "        hypothesis. The `statistic`, which compares data to a distribution, is\n",
            "        computed between `data` and the null-hypothesized distribution.\n",
            "        \n",
            "        Next, many (specifically `n_mc_samples`) new samples, each containing the\n",
            "        same number of observations as `data`, are drawn from the\n",
            "        null-hypothesized distribution. All unknown parameters of the distribution\n",
            "        family `dist` are fit to *each resample*, and the `statistic` is computed\n",
            "        between each sample and its corresponding fitted distribution. These\n",
            "        values of the statistic form the Monte Carlo null distribution (not to be\n",
            "        confused with the \"null-hypothesized distribution\" above).\n",
            "        \n",
            "        The p-value of the test is the proportion of statistic values in the Monte\n",
            "        Carlo null distribution that are at least as extreme as the statistic value\n",
            "        of the provided `data`. More precisely, the p-value is given by\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            p = \\frac{b + 1}\n",
            "                     {m + 1}\n",
            "        \n",
            "        where :math:`b` is the number of statistic values in the Monte Carlo null\n",
            "        distribution that are greater than or equal to the statistic value\n",
            "        calculated for `data`, and :math:`m` is the number of elements in the\n",
            "        Monte Carlo null distribution (`n_mc_samples`). The addition of :math:`1`\n",
            "        to the numerator and denominator can be thought of as including the\n",
            "        value of the statistic corresponding with `data` in the null distribution,\n",
            "        but a more formal explanation is given in [5]_.\n",
            "        \n",
            "        *Limitations*\n",
            "        \n",
            "        The test can be very slow for some distribution families because unknown\n",
            "        parameters of the distribution family must be fit to each of the Monte\n",
            "        Carlo samples, and for most distributions in SciPy, distribution fitting\n",
            "        performed via numerical optimization.\n",
            "        \n",
            "        *Anti-Pattern*\n",
            "        \n",
            "        For this reason, it may be tempting\n",
            "        to treat parameters of the distribution pre-fit to `data` (by the user)\n",
            "        as though they were `known_params`, as specification of all parameters of\n",
            "        the distribution precludes the need to fit the distribution to each Monte\n",
            "        Carlo sample. (This is essentially how the original Kilmogorov-Smirnov\n",
            "        test is performed.) Although such a test can provide evidence against the\n",
            "        null hypothesis, the test is conservative in the sense that small p-values\n",
            "        will tend to (greatly) *overestimate* the probability of making a type I\n",
            "        error (that is, rejecting the null hypothesis although it is true), and the\n",
            "        power of the test is low (that is, it is less likely to reject the null\n",
            "        hypothesis even when the null hypothesis is false).\n",
            "        This is because the Monte Carlo samples are less likely to agree with the\n",
            "        null-hypothesized distribution as well as `data`. This tends to increase\n",
            "        the values of the statistic recorded in the null distribution, so that a\n",
            "        larger number of them exceed the value of statistic for `data`, thereby\n",
            "        inflating the p-value.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] M. A. Stephens (1974). \"EDF Statistics for Goodness of Fit and\n",
            "               Some Comparisons.\" Journal of the American Statistical Association,\n",
            "               Vol. 69, pp. 730-737.\n",
            "        .. [2] W. Stute, W. G. Manteiga, and M. P. Quindimil (1993).\n",
            "               \"Bootstrap based goodness-of-fit-tests.\" Metrika 40.1: 243-256.\n",
            "        .. [3] C. Genest, & B Rmillard. (2008). \"Validity of the parametric\n",
            "               bootstrap for goodness-of-fit testing in semiparametric models.\"\n",
            "               Annales de l'IHP Probabilits et statistiques. Vol. 44. No. 6.\n",
            "        .. [4] I. Kojadinovic and J. Yan (2012). \"Goodness-of-fit testing based on\n",
            "               a weighted bootstrap: A fast large-sample alternative to the\n",
            "               parametric bootstrap.\" Canadian Journal of Statistics 40.3: 480-500.\n",
            "        .. [5] B. Phipson and G. K. Smyth (2010). \"Permutation P-values Should\n",
            "               Never Be Zero: Calculating Exact P-values When Permutations Are\n",
            "               Randomly Drawn.\" Statistical Applications in Genetics and Molecular\n",
            "               Biology 9.1.\n",
            "        .. [6] H. W. Lilliefors (1967). \"On the Kolmogorov-Smirnov test for\n",
            "               normality with mean and variance unknown.\" Journal of the American\n",
            "               statistical Association 62.318: 399-402.\n",
            "        .. [7] Filliben, James J. \"The probability plot correlation coefficient\n",
            "               test for normality.\" Technometrics 17.1 (1975): 111-117.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        A well-known test of the null hypothesis that data were drawn from a\n",
            "        given distribution is the Kolmogorov-Smirnov (KS) test, available in SciPy\n",
            "        as `scipy.stats.ks_1samp`. Suppose we wish to test whether the following\n",
            "        data:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.uniform.rvs(size=75, random_state=rng)\n",
            "        \n",
            "        were sampled from a normal distribution. To perform a KS test, the\n",
            "        empirical distribution function of the observed data will be compared\n",
            "        against the (theoretical) cumulative distribution function of a normal\n",
            "        distribution. Of course, to do this, the normal distribution under the null\n",
            "        hypothesis must be fully specified. This is commonly done by first fitting\n",
            "        the ``loc`` and ``scale`` parameters of the distribution to the observed\n",
            "        data, then performing the test.\n",
            "        \n",
            "        >>> loc, scale = np.mean(x), np.std(x, ddof=1)\n",
            "        >>> cdf = stats.norm(loc, scale).cdf\n",
            "        >>> stats.ks_1samp(x, cdf)\n",
            "        KstestResult(statistic=0.1119257570456813, pvalue=0.2827756409939257)\n",
            "        \n",
            "        An advantage of the KS-test is that the p-value - the probability of\n",
            "        obtaining a value of the test statistic under the null hypothesis as\n",
            "        extreme as the value obtained from the observed data - can be calculated\n",
            "        exactly and efficiently. `goodness_of_fit` can only approximate these\n",
            "        results.\n",
            "        \n",
            "        >>> known_params = {'loc': loc, 'scale': scale}\n",
            "        >>> res = stats.goodness_of_fit(stats.norm, x, known_params=known_params,\n",
            "        ...                             statistic='ks', random_state=rng)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.1119257570456813, 0.2788)\n",
            "        \n",
            "        The statistic matches exactly, but the p-value is estimated by forming\n",
            "        a \"Monte Carlo null distribution\", that is, by explicitly drawing random\n",
            "        samples from `scipy.stats.norm` with the provided parameters and\n",
            "        calculating the stastic for each. The fraction of these statistic values\n",
            "        at least as extreme as ``res.statistic`` approximates the exact p-value\n",
            "        calculated by `scipy.stats.ks_1samp`.\n",
            "        \n",
            "        However, in many cases, we would prefer to test only that the data were\n",
            "        sampled from one of *any* member of the normal distribution family, not\n",
            "        specifically from the normal distribution with the location and scale\n",
            "        fitted to the observed sample. In this case, Lilliefors [6]_ argued that\n",
            "        the KS test is far too conservative (that is, the p-value overstates\n",
            "        the actual probability of rejecting a true null hypothesis) and thus lacks\n",
            "        power - the ability to reject the null hypothesis when the null hypothesis\n",
            "        is actually false.\n",
            "        Indeed, our p-value above is approximately 0.28, which is far too large\n",
            "        to reject the null hypothesis at any common significance level.\n",
            "        \n",
            "        Consider why this might be. Note that in the KS test above, the statistic\n",
            "        always compares data against the CDF of a normal distribution fitted to the\n",
            "        *observed data*. This tends to reduce the value of the statistic for the\n",
            "        observed data, but it is \"unfair\" when computing the statistic for other\n",
            "        samples, such as those we randomly draw to form the Monte Carlo null\n",
            "        distribution. It is easy to correct for this: whenever we compute the KS\n",
            "        statistic of a sample, we use the CDF of a normal distribution fitted\n",
            "        to *that sample*. The null distribution in this case has not been\n",
            "        calculated exactly and is tyically approximated using Monte Carlo methods\n",
            "        as described above. This is where `goodness_of_fit` excels.\n",
            "        \n",
            "        >>> res = stats.goodness_of_fit(stats.norm, x, statistic='ks',\n",
            "        ...                             random_state=rng)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.1119257570456813, 0.0196)\n",
            "        \n",
            "        Indeed, this p-value is much smaller, and small enough to (correctly)\n",
            "        reject the null hypothesis at common significance levels, including 5% and\n",
            "        2.5%.\n",
            "        \n",
            "        However, the KS statistic is not very sensitive to all deviations from\n",
            "        normality. The original advantage of the KS statistic was the ability\n",
            "        to compute the null distribution theoretically, but a more sensitive\n",
            "        statistic - resulting in a higher test power - can be used now that we can\n",
            "        approximate the null distribution\n",
            "        computationally. The Anderson-Darling statistic [1]_ tends to be more\n",
            "        sensitive, and critical values of the this statistic have been tabulated\n",
            "        for various significance levels and sample sizes using Monte Carlo methods.\n",
            "        \n",
            "        >>> res = stats.anderson(x, 'norm')\n",
            "        >>> print(res.statistic)\n",
            "        1.2139573337497467\n",
            "        >>> print(res.critical_values)\n",
            "        [0.549 0.625 0.75  0.875 1.041]\n",
            "        >>> print(res.significance_level)\n",
            "        [15.  10.   5.   2.5  1. ]\n",
            "        \n",
            "        Here, the observed value of the statistic exceeds the critical value\n",
            "        corresponding with a 1% significance level. This tells us that the p-value\n",
            "        of the observed data is less than 1%, but what is it? We could interpolate\n",
            "        from these (already-interpolated) values, but `goodness_of_fit` can\n",
            "        estimate it directly.\n",
            "        \n",
            "        >>> res = stats.goodness_of_fit(stats.norm, x, statistic='ad',\n",
            "        ...                             random_state=rng)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (1.2139573337497467, 0.0034)\n",
            "        \n",
            "        A further advantage is that use of `goodness_of_fit` is not limited to\n",
            "        a particular set of distributions or conditions on which parameters\n",
            "        are known versus which must be estimated from data. Instead,\n",
            "        `goodness_of_fit` can estimate p-values relatively quickly for any\n",
            "        distribution with a sufficiently fast and reliable ``fit`` method. For\n",
            "        instance, here we perform a goodness of fit test using the Cramer-von Mises\n",
            "        statistic against the Rayleigh distribution with known location and unknown\n",
            "        scale.\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.chi(df=2.2, loc=0, scale=2).rvs(size=1000, random_state=rng)\n",
            "        >>> res = stats.goodness_of_fit(stats.rayleigh, x, statistic='cvm',\n",
            "        ...                             known_params={'loc': 0}, random_state=rng)\n",
            "        \n",
            "        This executes fairly quickly, but to check the reliability of the ``fit``\n",
            "        method, we should inspect the fit result.\n",
            "        \n",
            "        >>> res.fit_result  # location is as specified, and scale is reasonable\n",
            "          params: FitParams(loc=0.0, scale=2.1026719844231243)\n",
            "         success: True\n",
            "         message: 'The fit was performed successfully.'\n",
            "        >>> import matplotlib.pyplot as plt  # matplotlib must be installed to plot\n",
            "        >>> res.fit_result.plot()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        If the distribution is not fit to the observed data as well as possible,\n",
            "        the test may not control the type I error rate, that is, the chance of\n",
            "        rejecting the null hypothesis even when it is true.\n",
            "        \n",
            "        We should also look for extreme outliers in the null distribution that\n",
            "        may be caused by unreliable fitting. These do not necessarily invalidate\n",
            "        the result, but they tend to reduce the test's power.\n",
            "        \n",
            "        >>> _, ax = plt.subplots()\n",
            "        >>> ax.hist(np.log10(res.null_distribution))\n",
            "        >>> ax.set_xlabel(\"log10 of CVM statistic under the null hypothesis\")\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> ax.set_title(\"Histogram of the Monte Carlo null distribution\")\n",
            "        >>> plt.show()\n",
            "        \n",
            "        This plot seems reassuring.\n",
            "        \n",
            "        If ``fit`` method is working reliably, and if the distribution of the test\n",
            "        statistic is not particularly sensitive to the values of the fitted\n",
            "        parameters, then the p-value provided by `goodness_of_fit` is expected to\n",
            "        be a good approximation.\n",
            "        \n",
            "        >>> res.statistic, res.pvalue\n",
            "        (0.2231991510248692, 0.0525)\n",
            "    \n",
            "    gstd(a, axis=0, ddof=1)\n",
            "        Calculate the geometric standard deviation of an array.\n",
            "        \n",
            "        The geometric standard deviation describes the spread of a set of numbers\n",
            "        where the geometric mean is preferred. It is a multiplicative factor, and\n",
            "        so a dimensionless quantity.\n",
            "        \n",
            "        It is defined as the exponent of the standard deviation of ``log(a)``.\n",
            "        Mathematically the population geometric standard deviation can be\n",
            "        evaluated as::\n",
            "        \n",
            "            gstd = exp(std(log(a)))\n",
            "        \n",
            "        .. versionadded:: 1.3.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            An array like object containing the sample data.\n",
            "        axis : int, tuple or None, optional\n",
            "            Axis along which to operate. Default is 0. If None, compute over\n",
            "            the whole array `a`.\n",
            "        ddof : int, optional\n",
            "            Degree of freedom correction in the calculation of the\n",
            "            geometric standard deviation. Default is 1.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        gstd : ndarray or float\n",
            "            An array of the geometric standard deviation. If `axis` is None or `a`\n",
            "            is a 1d array a float is returned.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gmean : Geometric mean\n",
            "        numpy.std : Standard deviation\n",
            "        gzscore : Geometric standard score\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        As the calculation requires the use of logarithms the geometric standard\n",
            "        deviation only supports strictly positive values. Any non-positive or\n",
            "        infinite values will raise a `ValueError`.\n",
            "        The geometric standard deviation is sometimes confused with the exponent of\n",
            "        the standard deviation, ``exp(std(a))``. Instead the geometric standard\n",
            "        deviation is ``exp(std(log(a)))``.\n",
            "        The default value for `ddof` is different to the default value (0) used\n",
            "        by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Geometric standard deviation\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Geometric_standard_deviation.\n",
            "        .. [2] Kirkwood, T. B., \"Geometric means and measures of dispersion\",\n",
            "               Biometrics, vol. 35, pp. 908-909, 1979\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Find the geometric standard deviation of a log-normally distributed sample.\n",
            "        Note that the standard deviation of the distribution is one, on a\n",
            "        log scale this evaluates to approximately ``exp(1)``.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gstd\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> sample = rng.lognormal(mean=0, sigma=1, size=1000)\n",
            "        >>> gstd(sample)\n",
            "        2.810010162475324\n",
            "        \n",
            "        Compute the geometric standard deviation of a multidimensional array and\n",
            "        of a given axis.\n",
            "        \n",
            "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
            "        >>> gstd(a, axis=None)\n",
            "        2.2944076136018947\n",
            "        >>> gstd(a, axis=2)\n",
            "        array([[1.82424757, 1.22436866, 1.13183117],\n",
            "               [1.09348306, 1.07244798, 1.05914985]])\n",
            "        >>> gstd(a, axis=(1,2))\n",
            "        array([2.12939215, 1.22120169])\n",
            "        \n",
            "        The geometric standard deviation further handles masked arrays.\n",
            "        \n",
            "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
            "        >>> ma = np.ma.masked_where(a > 16, a)\n",
            "        >>> ma\n",
            "        masked_array(\n",
            "          data=[[[1, 2, 3, 4],\n",
            "                 [5, 6, 7, 8],\n",
            "                 [9, 10, 11, 12]],\n",
            "                [[13, 14, 15, 16],\n",
            "                 [--, --, --, --],\n",
            "                 [--, --, --, --]]],\n",
            "          mask=[[[False, False, False, False],\n",
            "                 [False, False, False, False],\n",
            "                 [False, False, False, False]],\n",
            "                [[False, False, False, False],\n",
            "                 [ True,  True,  True,  True],\n",
            "                 [ True,  True,  True,  True]]],\n",
            "          fill_value=999999)\n",
            "        >>> gstd(ma, axis=2)\n",
            "        masked_array(\n",
            "          data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],\n",
            "                [1.0934830582350938, --, --]],\n",
            "          mask=[[False, False, False],\n",
            "                [False,  True,  True]],\n",
            "          fill_value=999999)\n",
            "    \n",
            "    gzscore(a, *, axis=0, ddof=0, nan_policy='propagate')\n",
            "        Compute the geometric standard score.\n",
            "        \n",
            "        Compute the geometric z score of each strictly positive value in the\n",
            "        sample, relative to the geometric mean and standard deviation.\n",
            "        Mathematically the geometric z score can be evaluated as::\n",
            "        \n",
            "            gzscore = log(a/gmu) / log(gsigma)\n",
            "        \n",
            "        where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n",
            "        deviation).\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Sample data.\n",
            "        axis : int or None, optional\n",
            "            Axis along which to operate. Default is 0. If None, compute over\n",
            "            the whole array `a`.\n",
            "        ddof : int, optional\n",
            "            Degrees of freedom correction in the calculation of the\n",
            "            standard deviation. Default is 0.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
            "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
            "            values. Default is 'propagate'.  Note that when the value is 'omit',\n",
            "            nans in the input also propagate to the output, but they do not affect\n",
            "            the geometric z scores computed for the non-nan values.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        gzscore : array_like\n",
            "            The geometric z scores, standardized by geometric mean and geometric\n",
            "            standard deviation of input array `a`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gmean : Geometric mean\n",
            "        gstd : Geometric standard deviation\n",
            "        zscore : Standard score\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function preserves ndarray subclasses, and works also with\n",
            "        matrices and masked arrays (it uses ``asanyarray`` instead of\n",
            "        ``asarray`` for parameters).\n",
            "        \n",
            "        .. versionadded:: 1.8\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Geometric standard score\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Draw samples from a log-normal distribution:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import zscore, gzscore\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> mu, sigma = 3., 1.  # mean and standard deviation\n",
            "        >>> x = rng.lognormal(mu, sigma, size=500)\n",
            "        \n",
            "        Display the histogram of the samples:\n",
            "        \n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(x, 50)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Display the histogram of the samples standardized by the classical zscore.\n",
            "        Distribution is rescaled but its shape is unchanged.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(zscore(x), 50)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Demonstrate that the distribution of geometric zscores is rescaled and\n",
            "        quasinormal:\n",
            "        \n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(gzscore(x), 50)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    hmean(a, axis=0, dtype=None, *, weights=None, nan_policy='propagate', keepdims=False)\n",
            "        Calculate the weighted harmonic mean along the specified axis.\n",
            "        \n",
            "        The weighted harmonic mean of the array :math:`a_i` associated to weights\n",
            "        :math:`w_i` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\frac{ \\sum_{i=1}^n w_i }{ \\sum_{i=1}^n \\frac{w_i}{a_i} } \\, ,\n",
            "        \n",
            "        and, with equal weights, it gives:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\frac{ n }{ \\sum_{i=1}^n \\frac{1}{a_i} } \\, .\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array, masked array or object that can be converted to an array.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        dtype : dtype, optional\n",
            "            Type of the returned array and of the accumulator in which the\n",
            "            elements are summed. If `dtype` is not specified, it defaults to the\n",
            "            dtype of `a`, unless `a` has an integer `dtype` with a precision less\n",
            "            than that of the default platform integer. In that case, the default\n",
            "            platform integer is used.\n",
            "        weights : array_like, optional\n",
            "            The weights array can either be 1-D (in which case its length must be\n",
            "            the size of `a` along the given `axis`) or of the same shape as `a`.\n",
            "            Default is None, which gives each value a weight of 1.0.\n",
            "            \n",
            "            .. versionadded:: 1.9\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        hmean : ndarray\n",
            "            See `dtype` parameter above.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`numpy.mean`\n",
            "            Arithmetic average\n",
            "        :func:`numpy.average`\n",
            "            Weighted average\n",
            "        :func:`gmean`\n",
            "            Geometric mean\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The harmonic mean is computed over a single dimension of the input\n",
            "        array, axis=0 by default, or all values in the array if axis=None.\n",
            "        float64 intermediate and return values are used for integer inputs.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Weighted Harmonic Mean\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Harmonic_mean#Weighted_harmonic_mean\n",
            "        .. [2] Ferger, F., \"The nature and use of the harmonic mean\", Journal of\n",
            "               the American Statistical Association, vol. 26, pp. 36-40, 1931\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import hmean\n",
            "        >>> hmean([1, 4])\n",
            "        1.6000000000000001\n",
            "        >>> hmean([1, 2, 3, 4, 5, 6, 7])\n",
            "        2.6997245179063363\n",
            "        >>> hmean([1, 4, 7], weights=[3, 1, 3])\n",
            "        1.9029126213592233\n",
            "    \n",
            "    iqr(x, axis=None, rng=(25, 75), scale=1.0, nan_policy='propagate', interpolation='linear', keepdims=False)\n",
            "        Compute the interquartile range of the data along the specified axis.\n",
            "        \n",
            "        The interquartile range (IQR) is the difference between the 75th and\n",
            "        25th percentile of the data. It is a measure of the dispersion\n",
            "        similar to standard deviation or variance, but is much more robust\n",
            "        against outliers [2]_.\n",
            "        \n",
            "        The ``rng`` parameter allows this function to compute other\n",
            "        percentile ranges than the actual IQR. For example, setting\n",
            "        ``rng=(0, 100)`` is equivalent to `numpy.ptp`.\n",
            "        \n",
            "        The IQR of an empty array is `np.nan`.\n",
            "        \n",
            "        .. versionadded:: 0.18.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array or object that can be converted to an array.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        rng : Two-element sequence containing floats in range of [0,100] optional\n",
            "            Percentiles over which to compute the range. Each must be\n",
            "            between 0 and 100, inclusive. The default is the true IQR:\n",
            "            ``(25, 75)``. The order of the elements is not important.\n",
            "        scale : scalar or str or array_like of reals, optional\n",
            "            The numerical value of scale will be divided out of the final\n",
            "            result. The following string value is also recognized:\n",
            "            \n",
            "              * 'normal' : Scale by\n",
            "                :math:`2 \\sqrt{2} erf^{-1}(\\frac{1}{2}) \\approx 1.349`.\n",
            "            \n",
            "            The default is 1.0.\n",
            "            Array-like `scale` of real dtype is also allowed, as long\n",
            "            as it broadcasts correctly to the output such that\n",
            "            ``out / scale`` is a valid operation. The output dimensions\n",
            "            depend on the input array, `x`, the `axis` argument, and the\n",
            "            `keepdims` flag.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        interpolation : str, optional\n",
            "            Specifies the interpolation method to use when the percentile\n",
            "            boundaries lie between two data points ``i`` and ``j``.\n",
            "            The following options are available (default is 'linear'):\n",
            "            \n",
            "              * 'linear': ``i + (j - i)*fraction``, where ``fraction`` is the\n",
            "                fractional part of the index surrounded by ``i`` and ``j``.\n",
            "              * 'lower': ``i``.\n",
            "              * 'higher': ``j``.\n",
            "              * 'nearest': ``i`` or ``j`` whichever is nearest.\n",
            "              * 'midpoint': ``(i + j)/2``.\n",
            "            \n",
            "            For NumPy >= 1.22.0, the additional options provided by the ``method``\n",
            "            keyword of `numpy.percentile` are also valid.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        iqr : scalar or ndarray\n",
            "            If ``axis=None``, a scalar is returned. If the input contains\n",
            "            integers or floats of smaller precision than ``np.float64``, then the\n",
            "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
            "            the same as that of the input.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`numpy.std`, :func:`numpy.var`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Interquartile range\" https://en.wikipedia.org/wiki/Interquartile_range\n",
            "        .. [2] \"Robust measures of scale\" https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
            "        .. [3] \"Quantile\" https://en.wikipedia.org/wiki/Quantile\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import iqr\n",
            "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
            "        >>> x\n",
            "        array([[10,  7,  4],\n",
            "               [ 3,  2,  1]])\n",
            "        >>> iqr(x)\n",
            "        4.0\n",
            "        >>> iqr(x, axis=0)\n",
            "        array([ 3.5,  2.5,  1.5])\n",
            "        >>> iqr(x, axis=1)\n",
            "        array([ 3.,  1.])\n",
            "        >>> iqr(x, axis=1, keepdims=True)\n",
            "        array([[ 3.],\n",
            "               [ 1.]])\n",
            "    \n",
            "    jarque_bera(x, *, axis=None, nan_policy='propagate', keepdims=False)\n",
            "        Perform the Jarque-Bera goodness of fit test on sample data.\n",
            "        \n",
            "        The Jarque-Bera test tests whether the sample data has the skewness and\n",
            "        kurtosis matching a normal distribution.\n",
            "        \n",
            "        Note that this test only works for a large enough number of data samples\n",
            "        (>2000) as the test statistic asymptotically has a Chi-squared distribution\n",
            "        with 2 degrees of freedom.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Observations of a random variable.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : SignificanceResult\n",
            "            An object with the following attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                The test statistic.\n",
            "            pvalue : float\n",
            "                The p-value for the hypothesis test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Jarque, C. and Bera, A. (1980) \"Efficient tests for normality,\n",
            "               homoscedasticity and serial independence of regression residuals\",\n",
            "               6 Econometric Letters 255-259.\n",
            "        .. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n",
            "               for normality (complete samples). Biometrika, 52(3/4), 591-611.\n",
            "        .. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [4] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n",
            "               research. The open cardiovascular medicine journal, 2, 97.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to infer from measurements whether the weights of adult\n",
            "        human males in a medical study are not normally distributed [2]_.\n",
            "        The weights (lbs) are recorded in the array ``x`` below.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n",
            "        \n",
            "        The Jarque-Bera test begins by computing a statistic based on the sample\n",
            "        skewness and kurtosis.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.jarque_bera(x)\n",
            "        >>> res.statistic\n",
            "        6.982848237344646\n",
            "        \n",
            "        Because the normal distribution has zero skewness and zero\n",
            "        (\"excess\" or \"Fisher\") kurtosis, the value of this statistic tends to be\n",
            "        low for samples drawn from a normal distribution.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the statistic\n",
            "        against the null distribution: the distribution of statistic values derived\n",
            "        under the null hypothesis that the weights were drawn from a normal\n",
            "        distribution.\n",
            "        For the Jarque-Bera test, the null distribution for very large samples is\n",
            "        the chi-squared distribution with two degrees of freedom.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> dist = stats.chi2(df=2)\n",
            "        >>> jb_val = np.linspace(0, 11, 100)\n",
            "        >>> pdf = dist.pdf(jb_val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def jb_plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(jb_val, pdf)\n",
            "        ...     ax.set_title(\"Jarque-Bera Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> jb_plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution greater than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> jb_plot(ax)\n",
            "        >>> pvalue = dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (7.5, 0.01), (8, 0.05), arrowprops=props)\n",
            "        >>> i = jb_val >= res.statistic  # indices of more extreme statistic values\n",
            "        >>> ax.fill_between(jb_val[i], y1=0, y2=pdf[i])\n",
            "        >>> ax.set_xlim(0, 11)\n",
            "        >>> ax.set_ylim(0, 0.3)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.03045746622458189\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from a normally distributed population that produces such an\n",
            "        extreme value of the statistic - this may be taken as evidence against\n",
            "        the null hypothesis in favor of the alternative: the weights were not\n",
            "        drawn from a normal distribution. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [3]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        \n",
            "        Note that the chi-squared distribution provides an asymptotic approximation\n",
            "        of the null distribution; it is only accurate for samples with many\n",
            "        observations. For small samples like ours, `scipy.stats.monte_carlo_test`\n",
            "        may provide a more accurate, albeit stochastic, approximation of the\n",
            "        exact p-value.\n",
            "        \n",
            "        >>> def statistic(x, axis):\n",
            "        ...     # underlying calculation of the Jarque Bera statistic\n",
            "        ...     s = stats.skew(x, axis=axis)\n",
            "        ...     k = stats.kurtosis(x, axis=axis)\n",
            "        ...     return x.shape[axis]/6 * (s**2 + k**2/4)\n",
            "        >>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n",
            "        ...                              alternative='greater')\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> jb_plot(ax)\n",
            "        >>> ax.hist(res.null_distribution, np.linspace(0, 10, 50),\n",
            "        ...         density=True)\n",
            "        >>> ax.legend(['aymptotic approximation (many observations)',\n",
            "        ...            'Monte Carlo approximation (11 observations)'])\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0097  # may vary\n",
            "        \n",
            "        Furthermore, despite their stochastic nature, p-values computed in this way\n",
            "        can be used to exactly control the rate of false rejections of the null\n",
            "        hypothesis [4]_.\n",
            "    \n",
            "    kendalltau(x, y, *, initial_lexsort=<object object at 0x7880431b5360>, nan_policy='propagate', method='auto', variant='b', alternative='two-sided')\n",
            "        Calculate Kendall's tau, a correlation measure for ordinal data.\n",
            "        \n",
            "        Kendall's tau is a measure of the correspondence between two rankings.\n",
            "        Values close to 1 indicate strong agreement, and values close to -1\n",
            "        indicate strong disagreement. This implements two variants of Kendall's\n",
            "        tau: tau-b (the default) and tau-c (also known as Stuart's tau-c). These\n",
            "        differ only in how they are normalized to lie within the range -1 to 1;\n",
            "        the hypothesis tests (their p-values) are identical. Kendall's original\n",
            "        tau-a is not implemented separately because both tau-b and tau-c reduce\n",
            "        to tau-a in the absence of ties.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Arrays of rankings, of the same shape. If arrays are not 1-D, they\n",
            "            will be flattened to 1-D.\n",
            "        initial_lexsort : bool, optional, deprecated\n",
            "            This argument is unused.\n",
            "        \n",
            "            .. deprecated:: 1.10.0\n",
            "               `kendalltau` keyword argument `initial_lexsort` is deprecated as it\n",
            "               is unused and will be removed in SciPy 1.14.0.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "              * 'propagate': returns nan\n",
            "              * 'raise': throws an error\n",
            "              * 'omit': performs the calculations ignoring nan values\n",
            "        \n",
            "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
            "            Defines which method is used to calculate the p-value [5]_.\n",
            "            The following options are available (default is 'auto'):\n",
            "        \n",
            "              * 'auto': selects the appropriate method based on a trade-off\n",
            "                between speed and accuracy\n",
            "              * 'asymptotic': uses a normal approximation valid for large samples\n",
            "              * 'exact': computes the exact p-value, but can only be used if no ties\n",
            "                are present. As the sample size increases, the 'exact' computation\n",
            "                time may grow and the result may lose some precision.\n",
            "        variant : {'b', 'c'}, optional\n",
            "            Defines which variant of Kendall's tau is returned. Default is 'b'.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "        \n",
            "            * 'two-sided': the rank correlation is nonzero\n",
            "            * 'less': the rank correlation is negative (less than zero)\n",
            "            * 'greater':  the rank correlation is positive (greater than zero)\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SignificanceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "               The tau statistic.\n",
            "            pvalue : float\n",
            "               The p-value for a hypothesis test whose null hypothesis is\n",
            "               an absence of association, tau = 0.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
            "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
            "        weightedtau : Computes a weighted version of Kendall's tau.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The definition of Kendall's tau that is used is [2]_::\n",
            "        \n",
            "          tau_b = (P - Q) / sqrt((P + Q + T) * (P + Q + U))\n",
            "        \n",
            "          tau_c = 2 (P - Q) / (n**2 * (m - 1) / m)\n",
            "        \n",
            "        where P is the number of concordant pairs, Q the number of discordant\n",
            "        pairs, T the number of ties only in `x`, and U the number of ties only in\n",
            "        `y`.  If a tie occurs for the same pair in both `x` and `y`, it is not\n",
            "        added to either T or U. n is the total number of samples, and m is the\n",
            "        number of unique values in either `x` or `y`, whichever is smaller.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n",
            "               Vol. 30, No. 1/2, pp. 81-93, 1938.\n",
            "        .. [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n",
            "               Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n",
            "        .. [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\", John\n",
            "               Wiley & Sons, 1967.\n",
            "        .. [4] Peter M. Fenwick, \"A new data structure for cumulative frequency\n",
            "               tables\", Software: Practice and Experience, Vol. 24, No. 3,\n",
            "               pp. 327-336, 1994.\n",
            "        .. [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n",
            "               Charles Griffin & Co., 1970.\n",
            "        .. [6] Kershenobich, D., Fierro, F. J., & Rojkind, M. (1970). The\n",
            "               relationship between the free pool of proline and collagen content\n",
            "               in human liver cirrhosis. The Journal of Clinical Investigation,\n",
            "               49(12), 2246-2249.\n",
            "        .. [7] Hollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric\n",
            "               statistical methods. John Wiley & Sons.\n",
            "        .. [8] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Consider the following data from [6]_, which studied the relationship\n",
            "        between free proline (an amino acid) and total collagen (a protein often\n",
            "        found in connective tissue) in unhealthy human livers.\n",
            "        \n",
            "        The ``x`` and ``y`` arrays below record measurements of the two compounds.\n",
            "        The observations are paired: each free proline measurement was taken from\n",
            "        the same liver as the total collagen measurement at the same index.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> # total collagen (mg/g dry weight of liver)\n",
            "        >>> x = np.array([7.1, 7.1, 7.2, 8.3, 9.4, 10.5, 11.4])\n",
            "        >>> # free proline ( mole/g dry weight of liver)\n",
            "        >>> y = np.array([2.8, 2.9, 2.8, 2.6, 3.5, 4.6, 5.0])\n",
            "        \n",
            "        These data were analyzed in [7]_ using Spearman's correlation coefficient,\n",
            "        a statistic similar to to Kendall's tau in that it is also sensitive to\n",
            "        ordinal correlation between the samples. Let's perform an analogous study\n",
            "        using Kendall's tau.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.kendalltau(x, y)\n",
            "        >>> res.statistic\n",
            "        0.5499999999999999\n",
            "        \n",
            "        The value of this statistic tends to be high (close to 1) for samples with\n",
            "        a strongly positive ordinal correlation, low (close to -1) for samples with\n",
            "        a strongly negative ordinal correlation, and small in magnitude (close to\n",
            "        zero) for samples with weak ordinal correlation.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the\n",
            "        statistic against the null distribution: the distribution of statistic\n",
            "        values derived under the null hypothesis that total collagen and free\n",
            "        proline measurements are independent.\n",
            "        \n",
            "        For this test, the null distribution for large samples without ties is\n",
            "        approximated as the normal distribution with variance\n",
            "        ``(2*(2*n + 5))/(9*n*(n - 1))``, where ``n = len(x)``.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> n = len(x)  # len(x) == len(y)\n",
            "        >>> var = (2*(2*n + 5))/(9*n*(n - 1))\n",
            "        >>> dist = stats.norm(scale=np.sqrt(var))\n",
            "        >>> z_vals = np.linspace(-1.25, 1.25, 100)\n",
            "        >>> pdf = dist.pdf(z_vals)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(z_vals, pdf)\n",
            "        ...     ax.set_title(\"Kendall Tau Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution as extreme or more extreme than the observed\n",
            "        value of the statistic. In a two-sided test in which the statistic is\n",
            "        positive, elements of the null distribution greater than the transformed\n",
            "        statistic and elements of the null distribution less than the negative of\n",
            "        the observed statistic are both considered \"more extreme\".\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (0.65, 0.15), (0.8, 0.3), arrowprops=props)\n",
            "        >>> i = z_vals >= res.statistic\n",
            "        >>> ax.fill_between(z_vals[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> i = z_vals <= -res.statistic\n",
            "        >>> ax.fill_between(z_vals[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> ax.set_xlim(-1.25, 1.25)\n",
            "        >>> ax.set_ylim(0, 0.5)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.09108705741631495  # approximate p-value\n",
            "        \n",
            "        Note that there is slight disagreement between the shaded area of the curve\n",
            "        and the p-value returned by `kendalltau`. This is because our data has\n",
            "        ties, and we have neglected a tie correction to the null distribution\n",
            "        variance that `kendalltau` performs. For samples without ties, the shaded\n",
            "        areas of our plot and p-value returned by `kendalltau` would match exactly.\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from independent distributions that produces such an extreme\n",
            "        value of the statistic - this may be taken as evidence against the null\n",
            "        hypothesis in favor of the alternative: the distribution of total collagen\n",
            "        and free proline are *not* independent. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [8]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        - Small p-values are not evidence for a *large* effect; rather, they can\n",
            "          only provide evidence for a \"significant\" effect, meaning that they are\n",
            "          unlikely to have occurred under the null hypothesis.\n",
            "        \n",
            "        For samples without ties of moderate size, `kendalltau` can compute the\n",
            "        p-value exactly. However, in the presence of ties, `kendalltau` resorts\n",
            "        to an asymptotic approximation. Nonetheles, we can use a permutation test\n",
            "        to compute the null distribution exactly: Under the null hypothesis that\n",
            "        total collagen and free proline are independent, each of the free proline\n",
            "        measurements were equally likely to have been observed with any of the\n",
            "        total collagen measurements. Therefore, we can form an *exact* null\n",
            "        distribution by calculating the statistic under each possible pairing of\n",
            "        elements between ``x`` and ``y``.\n",
            "        \n",
            "        >>> def statistic(x):  # explore all possible pairings by permuting `x`\n",
            "        ...     return stats.kendalltau(x, y).statistic  # ignore pvalue\n",
            "        >>> ref = stats.permutation_test((x,), statistic,\n",
            "        ...                              permutation_type='pairings')\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> bins = np.linspace(-1.25, 1.25, 25)\n",
            "        >>> ax.hist(ref.null_distribution, bins=bins, density=True)\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'exact null distribution'])\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        >>> ref.pvalue\n",
            "        0.12222222222222222  # exact p-value\n",
            "        \n",
            "        Note that there is significant disagreement between the exact p-value\n",
            "        calculated here and the approximation returned by `kendalltau` above. For\n",
            "        small samples with ties, consider performing a permutation test for more\n",
            "        accurate results.\n",
            "    \n",
            "    kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)\n",
            "        Compute the Kruskal-Wallis H-test for independent samples.\n",
            "        \n",
            "        The Kruskal-Wallis H-test tests the null hypothesis that the population\n",
            "        median of all of the groups are equal.  It is a non-parametric version of\n",
            "        ANOVA.  The test works on 2 or more independent samples, which may have\n",
            "        different sizes.  Note that rejecting the null hypothesis does not\n",
            "        indicate which of the groups differs.  Post hoc comparisons between\n",
            "        groups are required to determine which groups are different.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            Two or more arrays with the sample measurements can be given as\n",
            "            arguments. Samples must be one-dimensional.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The Kruskal-Wallis H statistic, corrected for ties.\n",
            "        pvalue : float\n",
            "            The p-value for the test using the assumption that H has a chi\n",
            "            square distribution. The p-value returned is the survival function of\n",
            "            the chi square distribution evaluated at H.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`f_oneway`\n",
            "            1-way ANOVA.\n",
            "        :func:`mannwhitneyu`\n",
            "            Mann-Whitney rank test on two samples.\n",
            "        :func:`friedmanchisquare`\n",
            "            Friedman test for repeated measurements.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Due to the assumption that H has a chi square distribution, the number\n",
            "        of samples in each group must not be too small.  A typical rule is\n",
            "        that each sample must have at least 5 measurements.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n",
            "           One-Criterion Variance Analysis\", Journal of the American Statistical\n",
            "           Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n",
            "        .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> x = [1, 3, 5, 7, 9]\n",
            "        >>> y = [2, 4, 6, 8, 10]\n",
            "        >>> stats.kruskal(x, y)\n",
            "        KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
            "        \n",
            "        >>> x = [1, 1, 1]\n",
            "        >>> y = [2, 2, 2]\n",
            "        >>> z = [2, 2]\n",
            "        >>> stats.kruskal(x, y, z)\n",
            "        KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
            "    \n",
            "    ks_1samp(x, cdf, args=(), alternative='two-sided', method='auto', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n",
            "        \n",
            "        This test compares the underlying distribution F(x) of a sample\n",
            "        against a given continuous distribution G(x). See Notes for a description\n",
            "        of the available null and alternative hypotheses.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            a 1-D array of observations of iid random variables.\n",
            "        cdf : callable\n",
            "            callable used to calculate the cdf.\n",
            "        args : tuple, sequence, optional\n",
            "            Distribution parameters, used with `cdf`.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
            "            Please see explanations in the Notes below.\n",
            "        method : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
            "            Defines the distribution used for calculating the p-value.\n",
            "            The following options are available (default is 'auto'):\n",
            "            \n",
            "              * 'auto' : selects one of the other options.\n",
            "              * 'exact' : uses the exact distribution of test statistic.\n",
            "              * 'approx' : approximates the two-sided probability with twice\n",
            "                the one-sided probability\n",
            "              * 'asymp': uses asymptotic distribution of test statistic\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: KstestResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                KS test statistic, either D+, D-, or D (the maximum of the two)\n",
            "            pvalue : float\n",
            "                One-tailed or two-tailed p-value.\n",
            "            statistic_location : float\n",
            "                Value of `x` corresponding with the KS statistic; i.e., the\n",
            "                distance between the empirical distribution function and the\n",
            "                hypothesized cumulative distribution function is measured at this\n",
            "                observation.\n",
            "            statistic_sign : int\n",
            "                +1 if the KS statistic is the maximum positive difference between\n",
            "                the empirical distribution function and the hypothesized cumulative\n",
            "                distribution function (D+); -1 if the KS statistic is the maximum\n",
            "                negative difference (D-).\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`ks_2samp`, :func:`kstest`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        There are three options for the null and corresponding alternative\n",
            "        hypothesis that can be selected using the `alternative` parameter.\n",
            "        \n",
            "        - `two-sided`: The null hypothesis is that the two distributions are\n",
            "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
            "          identical.\n",
            "        \n",
            "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
            "          alternative is that F(x) < G(x) for at least one x.\n",
            "        \n",
            "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
            "          alternative is that F(x) > G(x) for at least one x.\n",
            "        \n",
            "        Note that the alternative hypotheses describe the *CDFs* of the\n",
            "        underlying distributions, not the observed values. For example,\n",
            "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
            "        x1 tend to be less than those in x2.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test the null hypothesis that a sample is distributed\n",
            "        according to the standard normal.\n",
            "        We choose a confidence level of 95%; that is, we will reject the null\n",
            "        hypothesis in favor of the alternative if the p-value is less than 0.05.\n",
            "        \n",
            "        When testing uniformly distributed data, we would expect the\n",
            "        null hypothesis to be rejected.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n",
            "        ...                stats.norm.cdf)\n",
            "        KstestResult(statistic=0.5001899973268688, pvalue=1.1616392184763533e-23)\n",
            "        \n",
            "        Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n",
            "        null hypothesis in favor of the default \"two-sided\" alternative: the data\n",
            "        are *not* distributed according to the standard normal.\n",
            "        \n",
            "        When testing random variates from the standard normal distribution, we\n",
            "        expect the data to be consistent with the null hypothesis most of the time.\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=100, random_state=rng)\n",
            "        >>> stats.ks_1samp(x, stats.norm.cdf)\n",
            "        KstestResult(statistic=0.05345882212970396, pvalue=0.9227159037744717)\n",
            "        \n",
            "        As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n",
            "        we cannot reject the null hypothesis.\n",
            "        \n",
            "        Suppose, however, that the random variates are distributed according to\n",
            "        a normal distribution that is shifted toward greater values. In this case,\n",
            "        the cumulative density function (CDF) of the underlying distribution tends\n",
            "        to be *less* than the CDF of the standard normal. Therefore, we would\n",
            "        expect the null hypothesis to be rejected with ``alternative='less'``:\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n",
            "        >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n",
            "        KstestResult(statistic=0.17482387821055168, pvalue=0.001913921057766743)\n",
            "        \n",
            "        and indeed, with p-value smaller than our threshold, we reject the null\n",
            "        hypothesis in favor of the alternative.\n",
            "    \n",
            "    ks_2samp(data1, data2, alternative='two-sided', method='auto', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n",
            "        \n",
            "        This test compares the underlying continuous distributions F(x) and G(x)\n",
            "        of two independent samples.  See Notes for a description of the available\n",
            "        null and alternative hypotheses.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data1, data2 : array_like, 1-Dimensional\n",
            "            Two arrays of sample observations assumed to be drawn from a continuous\n",
            "            distribution, sample sizes can be different.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
            "            Please see explanations in the Notes below.\n",
            "        method : {'auto', 'exact', 'asymp'}, optional\n",
            "            Defines the method used for calculating the p-value.\n",
            "            The following options are available (default is 'auto'):\n",
            "            \n",
            "              * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n",
            "              * 'exact' : use exact distribution of test statistic\n",
            "              * 'asymp' : use asymptotic distribution of test statistic\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: KstestResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                KS test statistic.\n",
            "            pvalue : float\n",
            "                One-tailed or two-tailed p-value.\n",
            "            statistic_location : float\n",
            "                Value from `data1` or `data2` corresponding with the KS statistic;\n",
            "                i.e., the distance between the empirical distribution functions is\n",
            "                measured at this observation.\n",
            "            statistic_sign : int\n",
            "                +1 if the empirical distribution function of `data1` exceeds\n",
            "                the empirical distribution function of `data2` at\n",
            "                `statistic_location`, otherwise -1.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kstest`, :func:`ks_1samp`, :func:`epps_singleton_2samp`, :func:`anderson_ksamp`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        There are three options for the null and corresponding alternative\n",
            "        hypothesis that can be selected using the `alternative` parameter.\n",
            "        \n",
            "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
            "          alternative is that F(x) < G(x) for at least one x. The statistic\n",
            "          is the magnitude of the minimum (most negative) difference between the\n",
            "          empirical distribution functions of the samples.\n",
            "        \n",
            "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
            "          alternative is that F(x) > G(x) for at least one x. The statistic\n",
            "          is the maximum (most positive) difference between the empirical\n",
            "          distribution functions of the samples.\n",
            "        \n",
            "        - `two-sided`: The null hypothesis is that the two distributions are\n",
            "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
            "          identical. The statistic is the maximum absolute difference between the\n",
            "          empirical distribution functions of the samples.\n",
            "        \n",
            "        Note that the alternative hypotheses describe the *CDFs* of the\n",
            "        underlying distributions, not the observed values of the data. For example,\n",
            "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
            "        x1 tend to be less than those in x2.\n",
            "        \n",
            "        If the KS statistic is large, then the p-value will be small, and this may\n",
            "        be taken as evidence against the null hypothesis in favor of the\n",
            "        alternative.\n",
            "        \n",
            "        If ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\n",
            "        that is, the probability under the null hypothesis of obtaining a test\n",
            "        statistic value as extreme as the value computed from the data.\n",
            "        If ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\n",
            "        used to compute an approximate p-value.\n",
            "        If ``method='auto'``, an exact p-value computation is attempted if both\n",
            "        sample sizes are less than 10000; otherwise, the asymptotic method is used.\n",
            "        In any case, if an exact p-value calculation is attempted and fails, a\n",
            "        warning will be emitted, and the asymptotic p-value will be returned.\n",
            "        \n",
            "        The 'two-sided' 'exact' computation computes the complementary probability\n",
            "        and then subtracts from 1.  As such, the minimum probability it can return\n",
            "        is about 1e-16.  While the algorithm itself is exact, numerical\n",
            "        errors may accumulate for large sample sizes.   It is most suited to\n",
            "        situations in which one of the sample sizes is only a few thousand.\n",
            "        \n",
            "        We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n",
            "               Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test the null hypothesis that two samples were drawn\n",
            "        from the same distribution.\n",
            "        We choose a confidence level of 95%; that is, we will reject the null\n",
            "        hypothesis in favor of the alternative if the p-value is less than 0.05.\n",
            "        \n",
            "        If the first sample were drawn from a uniform distribution and the second\n",
            "        were drawn from the standard normal, we would expect the null hypothesis\n",
            "        to be rejected.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n",
            "        >>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n",
            "        >>> stats.ks_2samp(sample1, sample2)\n",
            "        KstestResult(statistic=0.5454545454545454, pvalue=7.37417839555191e-15)\n",
            "        \n",
            "        Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n",
            "        null hypothesis in favor of the default \"two-sided\" alternative: the data\n",
            "        were *not* drawn from the same distribution.\n",
            "        \n",
            "        When both samples are drawn from the same distribution, we expect the data\n",
            "        to be consistent with the null hypothesis most of the time.\n",
            "        \n",
            "        >>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n",
            "        >>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n",
            "        >>> stats.ks_2samp(sample1, sample2)\n",
            "        KstestResult(statistic=0.10927318295739348, pvalue=0.5438289009927495)\n",
            "        \n",
            "        As expected, the p-value of 0.54 is not below our threshold of 0.05, so\n",
            "        we cannot reject the null hypothesis.\n",
            "        \n",
            "        Suppose, however, that the first sample were drawn from\n",
            "        a normal distribution shifted toward greater values. In this case,\n",
            "        the cumulative density function (CDF) of the underlying distribution tends\n",
            "        to be *less* than the CDF underlying the second sample. Therefore, we would\n",
            "        expect the null hypothesis to be rejected with ``alternative='less'``:\n",
            "        \n",
            "        >>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n",
            "        >>> stats.ks_2samp(sample1, sample2, alternative='less')\n",
            "        KstestResult(statistic=0.4055137844611529, pvalue=3.5474563068855554e-08)\n",
            "        \n",
            "        and indeed, with p-value smaller than our threshold, we reject the null\n",
            "        hypothesis in favor of the alternative.\n",
            "    \n",
            "    kstat(data, n=2, *, axis=None, nan_policy='propagate', keepdims=False)\n",
            "        Return the nth k-statistic (1<=n<=4 so far).\n",
            "        \n",
            "        The nth k-statistic k_n is the unique symmetric unbiased estimator of the\n",
            "        nth cumulant kappa_n.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array_like\n",
            "            Input array. Note that n-D input gets flattened.\n",
            "        n : int, {1, 2, 3, 4}, optional\n",
            "            Default is equal to 2.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        kstat : float\n",
            "            The nth k-statistic.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kstatvar`\n",
            "            Returns an unbiased estimator of the variance of the k-statistic\n",
            "        :func:`moment`\n",
            "            Returns the n-th central moment about the mean for a sample.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        For a sample size n, the first few k-statistics are given by:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            k_{1} = \\mu\n",
            "            k_{2} = \\frac{n}{n-1} m_{2}\n",
            "            k_{3} = \\frac{ n^{2} } {(n-1) (n-2)} m_{3}\n",
            "            k_{4} = \\frac{ n^{2} [(n + 1)m_{4} - 3(n - 1) m^2_{2}]} {(n-1) (n-2) (n-3)}\n",
            "        \n",
            "        where :math:`\\mu` is the sample mean, :math:`m_2` is the sample\n",
            "        variance, and :math:`m_i` is the i-th sample central moment.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        http://mathworld.wolfram.com/k-Statistic.html\n",
            "        \n",
            "        http://mathworld.wolfram.com/Cumulant.html\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> from numpy.random import default_rng\n",
            "        >>> rng = default_rng()\n",
            "        \n",
            "        As sample size increases, n-th moment and n-th k-statistic converge to the\n",
            "        same number (although they aren't identical). In the case of the normal\n",
            "        distribution, they converge to zero.\n",
            "        \n",
            "        >>> for n in [2, 3, 4, 5, 6, 7]:\n",
            "        ...     x = rng.normal(size=10**n)\n",
            "        ...     m, k = stats.moment(x, 3), stats.kstat(x, 3)\n",
            "        ...     print(\"%.3g %.3g %.3g\" % (m, k, m-k))\n",
            "        -0.631 -0.651 0.0194  # random\n",
            "        0.0282 0.0283 -8.49e-05\n",
            "        -0.0454 -0.0454 1.36e-05\n",
            "        7.53e-05 7.53e-05 -2.26e-09\n",
            "        0.00166 0.00166 -4.99e-09\n",
            "        -2.88e-06 -2.88e-06 8.63e-13\n",
            "    \n",
            "    kstatvar(data, n=2, *, axis=None, nan_policy='propagate', keepdims=False)\n",
            "        Return an unbiased estimator of the variance of the k-statistic.\n",
            "        \n",
            "        See `kstat` for more details of the k-statistic.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array_like\n",
            "            Input array. Note that n-D input gets flattened.\n",
            "        n : int, {1, 2}, optional\n",
            "            Default is equal to 2.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        kstatvar : float\n",
            "            The nth k-statistic variance.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kstat`\n",
            "            Returns the n-th k-statistic.\n",
            "        :func:`moment`\n",
            "            Returns the n-th central moment about the mean for a sample.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The variances of the first few k-statistics are given by:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            var(k_{1}) = \\frac{\\kappa^2}{n}\n",
            "            var(k_{2}) = \\frac{\\kappa^4}{n} + \\frac{2\\kappa^2_{2}}{n - 1}\n",
            "            var(k_{3}) = \\frac{\\kappa^6}{n} + \\frac{9 \\kappa_2 \\kappa_4}{n - 1} +\n",
            "                         \\frac{9 \\kappa^2_{3}}{n - 1} +\n",
            "                         \\frac{6 n \\kappa^3_{2}}{(n-1) (n-2)}\n",
            "            var(k_{4}) = \\frac{\\kappa^8}{n} + \\frac{16 \\kappa_2 \\kappa_6}{n - 1} +\n",
            "                         \\frac{48 \\kappa_{3} \\kappa_5}{n - 1} +\n",
            "                         \\frac{34 \\kappa^2_{4}}{n-1} +\n",
            "                         \\frac{72 n \\kappa^2_{2} \\kappa_4}{(n - 1) (n - 2)} +\n",
            "                         \\frac{144 n \\kappa_{2} \\kappa^2_{3}}{(n - 1) (n - 2)} +\n",
            "                         \\frac{24 (n + 1) n \\kappa^4_{2}}{(n - 1) (n - 2) (n - 3)}\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "    \n",
            "    kstest(rvs, cdf, args=(), N=20, alternative='two-sided', method='auto', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n",
            "        goodness of fit.\n",
            "        \n",
            "        The one-sample test compares the underlying distribution F(x) of a sample\n",
            "        against a given distribution G(x). The two-sample test compares the\n",
            "        underlying distributions of two independent samples. Both tests are valid\n",
            "        only for continuous distributions.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        rvs : str, array_like, or callable\n",
            "            If an array, it should be a 1-D array of observations of random\n",
            "            variables.\n",
            "            If a callable, it should be a function to generate random variables;\n",
            "            it is required to have a keyword argument `size`.\n",
            "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
            "            which will be used to generate random variables.\n",
            "        cdf : str, array_like or callable\n",
            "            If array_like, it should be a 1-D array of observations of random\n",
            "            variables, and the two-sample test is performed\n",
            "            (and rvs must be array_like).\n",
            "            If a callable, that callable is used to calculate the cdf.\n",
            "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
            "            which will be used as the cdf function.\n",
            "        args : tuple, sequence, optional\n",
            "            Distribution parameters, used if `rvs` or `cdf` are strings or\n",
            "            callables.\n",
            "        N : int, optional\n",
            "            Sample size if `rvs` is string or callable.  Default is 20.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
            "            Please see explanations in the Notes below.\n",
            "        method : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
            "            Defines the distribution used for calculating the p-value.\n",
            "            The following options are available (default is 'auto'):\n",
            "            \n",
            "              * 'auto' : selects one of the other options.\n",
            "              * 'exact' : uses the exact distribution of test statistic.\n",
            "              * 'approx' : approximates the two-sided probability with twice the\n",
            "                one-sided probability\n",
            "              * 'asymp': uses asymptotic distribution of test statistic\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: KstestResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                KS test statistic, either D+, D-, or D (the maximum of the two)\n",
            "            pvalue : float\n",
            "                One-tailed or two-tailed p-value.\n",
            "            statistic_location : float\n",
            "                In a one-sample test, this is the value of `rvs`\n",
            "                corresponding with the KS statistic; i.e., the distance between\n",
            "                the empirical distribution function and the hypothesized cumulative\n",
            "                distribution function is measured at this observation.\n",
            "            \n",
            "                In a two-sample test, this is the value from `rvs` or `cdf`\n",
            "                corresponding with the KS statistic; i.e., the distance between\n",
            "                the empirical distribution functions is measured at this\n",
            "                observation.\n",
            "            statistic_sign : int\n",
            "                In a one-sample test, this is +1 if the KS statistic is the\n",
            "                maximum positive difference between the empirical distribution\n",
            "                function and the hypothesized cumulative distribution function\n",
            "                (D+); it is -1 if the KS statistic is the maximum negative\n",
            "                difference (D-).\n",
            "            \n",
            "                In a two-sample test, this is +1 if the empirical distribution\n",
            "                function of `rvs` exceeds the empirical distribution\n",
            "                function of `cdf` at `statistic_location`, otherwise -1.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`ks_1samp`, :func:`ks_2samp`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        There are three options for the null and corresponding alternative\n",
            "        hypothesis that can be selected using the `alternative` parameter.\n",
            "        \n",
            "        - `two-sided`: The null hypothesis is that the two distributions are\n",
            "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
            "          identical.\n",
            "        \n",
            "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
            "          alternative is that F(x) < G(x) for at least one x.\n",
            "        \n",
            "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
            "          alternative is that F(x) > G(x) for at least one x.\n",
            "        \n",
            "        Note that the alternative hypotheses describe the *CDFs* of the\n",
            "        underlying distributions, not the observed values. For example,\n",
            "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
            "        x1 tend to be less than those in x2.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test the null hypothesis that a sample is distributed\n",
            "        according to the standard normal.\n",
            "        We choose a confidence level of 95%; that is, we will reject the null\n",
            "        hypothesis in favor of the alternative if the p-value is less than 0.05.\n",
            "        \n",
            "        When testing uniformly distributed data, we would expect the\n",
            "        null hypothesis to be rejected.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n",
            "        ...              stats.norm.cdf)\n",
            "        KstestResult(statistic=0.5001899973268688, pvalue=1.1616392184763533e-23)\n",
            "        \n",
            "        Indeed, the p-value is lower than our threshold of 0.05, so we reject the\n",
            "        null hypothesis in favor of the default \"two-sided\" alternative: the data\n",
            "        are *not* distributed according to the standard normal.\n",
            "        \n",
            "        When testing random variates from the standard normal distribution, we\n",
            "        expect the data to be consistent with the null hypothesis most of the time.\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=100, random_state=rng)\n",
            "        >>> stats.kstest(x, stats.norm.cdf)\n",
            "        KstestResult(statistic=0.05345882212970396, pvalue=0.9227159037744717)\n",
            "        \n",
            "        As expected, the p-value of 0.92 is not below our threshold of 0.05, so\n",
            "        we cannot reject the null hypothesis.\n",
            "        \n",
            "        Suppose, however, that the random variates are distributed according to\n",
            "        a normal distribution that is shifted toward greater values. In this case,\n",
            "        the cumulative density function (CDF) of the underlying distribution tends\n",
            "        to be *less* than the CDF of the standard normal. Therefore, we would\n",
            "        expect the null hypothesis to be rejected with ``alternative='less'``:\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n",
            "        >>> stats.kstest(x, stats.norm.cdf, alternative='less')\n",
            "        KstestResult(statistic=0.17482387821055168, pvalue=0.001913921057766743)\n",
            "        \n",
            "        and indeed, with p-value smaller than our threshold, we reject the null\n",
            "        hypothesis in favor of the alternative.\n",
            "        \n",
            "        For convenience, the previous test can be performed using the name of the\n",
            "        distribution as the second argument.\n",
            "        \n",
            "        >>> stats.kstest(x, \"norm\", alternative='less')\n",
            "        KstestResult(statistic=0.17482387821055168, pvalue=0.001913921057766743)\n",
            "        \n",
            "        The examples above have all been one-sample tests identical to those\n",
            "        performed by `ks_1samp`. Note that `kstest` can also perform two-sample\n",
            "        tests identical to those performed by `ks_2samp`. For example, when two\n",
            "        samples are drawn from the same distribution, we expect the data to be\n",
            "        consistent with the null hypothesis most of the time.\n",
            "        \n",
            "        >>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n",
            "        >>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n",
            "        >>> stats.kstest(sample1, sample2)\n",
            "        KstestResult(statistic=0.11779448621553884, pvalue=0.4494256912629795)\n",
            "        \n",
            "        As expected, the p-value of 0.45 is not below our threshold of 0.05, so\n",
            "        we cannot reject the null hypothesis.\n",
            "    \n",
            "    kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the kurtosis (Fisher or Pearson) of a dataset.\n",
            "        \n",
            "        Kurtosis is the fourth central moment divided by the square of the\n",
            "        variance. If Fisher's definition is used, then 3.0 is subtracted from\n",
            "        the result to give 0.0 for a normal distribution.\n",
            "        \n",
            "        If bias is False then the kurtosis is calculated using k statistics to\n",
            "        eliminate bias coming from biased moment estimators\n",
            "        \n",
            "        Use `kurtosistest` to see if result is close enough to normal.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array\n",
            "            Data for which the kurtosis is calculated.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        fisher : bool, optional\n",
            "            If True, Fisher's definition is used (normal ==> 0.0). If False,\n",
            "            Pearson's definition is used (normal ==> 3.0).\n",
            "        bias : bool, optional\n",
            "            If False, then the calculations are corrected for statistical bias.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        kurtosis : array\n",
            "            The kurtosis of values along an axis, returning NaN where all values\n",
            "            are equal.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
            "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
            "           York. 2000.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In Fisher's definition, the kurtosis of the normal distribution is zero.\n",
            "        In the following example, the kurtosis is close to zero, because it was\n",
            "        calculated from the dataset, not from the continuous distribution.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import norm, kurtosis\n",
            "        >>> data = norm.rvs(size=1000, random_state=3)\n",
            "        >>> kurtosis(data)\n",
            "        -0.06928694200380558\n",
            "        \n",
            "        The distribution with a higher kurtosis has a heavier tail.\n",
            "        The zero valued kurtosis of the normal distribution in Fisher's definition\n",
            "        can serve as a reference point.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> import scipy.stats as stats\n",
            "        >>> from scipy.stats import kurtosis\n",
            "        \n",
            "        >>> x = np.linspace(-5, 5, 100)\n",
            "        >>> ax = plt.subplot()\n",
            "        >>> distnames = ['laplace', 'norm', 'uniform']\n",
            "        \n",
            "        >>> for distname in distnames:\n",
            "        ...     if distname == 'uniform':\n",
            "        ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n",
            "        ...     else:\n",
            "        ...         dist = getattr(stats, distname)\n",
            "        ...     data = dist.rvs(size=1000)\n",
            "        ...     kur = kurtosis(data, fisher=True)\n",
            "        ...     y = dist.pdf(x)\n",
            "        ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n",
            "        ...     ax.legend()\n",
            "        \n",
            "        The Laplace distribution has a heavier tail than the normal distribution.\n",
            "        The uniform distribution (which has negative kurtosis) has the thinnest\n",
            "        tail.\n",
            "    \n",
            "    kurtosistest(a, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
            "        Test whether a dataset has normal kurtosis.\n",
            "        \n",
            "        This function tests the null hypothesis that the kurtosis\n",
            "        of the population from which the sample was drawn is that\n",
            "        of the normal distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array\n",
            "            Array of the sample data.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "            \n",
            "            * 'two-sided': the kurtosis of the distribution underlying the sample\n",
            "              is different from that of the normal distribution\n",
            "            * 'less': the kurtosis of the distribution underlying the sample\n",
            "              is less than that of the normal distribution\n",
            "            * 'greater': the kurtosis of the distribution underlying the sample\n",
            "              is greater than that of the normal distribution\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The computed z-score for this test.\n",
            "        pvalue : float\n",
            "            The p-value for the hypothesis test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Valid only for n>20. This function uses the method described in [1]_.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] see e.g. F. J. Anscombe, W. J. Glynn, \"Distribution of the kurtosis\n",
            "           statistic b2 for normal samples\", Biometrika, vol. 70, pp. 227-234, 1983.\n",
            "        .. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n",
            "               for normality (complete samples). Biometrika, 52(3/4), 591-611.\n",
            "        .. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [4] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n",
            "               research. The open cardiovascular medicine journal, 2, 97.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to infer from measurements whether the weights of adult\n",
            "        human males in a medical study are not normally distributed [2]_.\n",
            "        The weights (lbs) are recorded in the array ``x`` below.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n",
            "        \n",
            "        The kurtosis test from [1]_ begins by computing a statistic based on the\n",
            "        sample (excess/Fisher) kurtosis.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.kurtosistest(x)\n",
            "        >>> res.statistic\n",
            "        2.3048235214240873\n",
            "        \n",
            "        (The test warns that our sample has too few observations to perform the\n",
            "        test. We'll return to this at the end of the example.)\n",
            "        Because normal distributions have zero excess kurtosis (by definition),\n",
            "        the magnitude of this statistic tends to be low for samples drawn from a\n",
            "        normal distribution.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the\n",
            "        statistic against the null distribution: the distribution of statistic\n",
            "        values derived under the null hypothesis that the weights were drawn from\n",
            "        a normal distribution.\n",
            "        \n",
            "        For this test, the null distribution of the statistic for very large\n",
            "        samples is the standard normal distribution.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> dist = stats.norm()\n",
            "        >>> kt_val = np.linspace(-5, 5, 100)\n",
            "        >>> pdf = dist.pdf(kt_val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def kt_plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(kt_val, pdf)\n",
            "        ...     ax.set_title(\"Kurtosis Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> kt_plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution as extreme or more extreme than the observed\n",
            "        value of the statistic. In a two-sided test in which the statistic is\n",
            "        positive, elements of the null distribution greater than the observed\n",
            "        statistic and elements of the null distribution less than the negative of\n",
            "        the observed statistic are both considered \"more extreme\".\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> kt_plot(ax)\n",
            "        >>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (3, 0.005), (3.25, 0.02), arrowprops=props)\n",
            "        >>> i = kt_val >= res.statistic\n",
            "        >>> ax.fill_between(kt_val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> i = kt_val <= -res.statistic\n",
            "        >>> ax.fill_between(kt_val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> ax.set_xlim(-5, 5)\n",
            "        >>> ax.set_ylim(0, 0.1)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0211764592113868\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from a normally distributed population that produces such an\n",
            "        extreme value of the statistic - this may be taken as evidence against\n",
            "        the null hypothesis in favor of the alternative: the weights were not\n",
            "        drawn from a normal distribution. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [3]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        \n",
            "        Note that the standard normal distribution provides an asymptotic\n",
            "        approximation of the null distribution; it is only accurate for samples\n",
            "        with many observations. This is the reason we received a warning at the\n",
            "        beginning of the example; our sample is quite small. In this case,\n",
            "        `scipy.stats.monte_carlo_test` may provide a more accurate, albeit\n",
            "        stochastic, approximation of the exact p-value.\n",
            "        \n",
            "        >>> def statistic(x, axis):\n",
            "        ...     # get just the skewtest statistic; ignore the p-value\n",
            "        ...     return stats.kurtosistest(x, axis=axis).statistic\n",
            "        >>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> kt_plot(ax)\n",
            "        >>> ax.hist(res.null_distribution, np.linspace(-5, 5, 50),\n",
            "        ...         density=True)\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'Monte Carlo approximation\\n(11 observations)'])\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0272  # may vary\n",
            "        \n",
            "        Furthermore, despite their stochastic nature, p-values computed in this way\n",
            "        can be used to exactly control the rate of false rejections of the null\n",
            "        hypothesis [4]_.\n",
            "    \n",
            "    levene(*samples, center='median', proportiontocut=0.05, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Perform Levene test for equal variances.\n",
            "        \n",
            "        The Levene test tests the null hypothesis that all input samples\n",
            "        are from populations with equal variances.  Levene's test is an\n",
            "        alternative to Bartlett's test `bartlett` in the case where\n",
            "        there are significant deviations from normality.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            The sample data, possibly with different lengths. Only one-dimensional\n",
            "            samples are accepted.\n",
            "        center : {'mean', 'median', 'trimmed'}, optional\n",
            "            Which function of the data to use in the test.  The default\n",
            "            is 'median'.\n",
            "        proportiontocut : float, optional\n",
            "            When `center` is 'trimmed', this gives the proportion of data points\n",
            "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
            "            Default is 0.05.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic.\n",
            "        pvalue : float\n",
            "            The p-value for the test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`fligner`\n",
            "            A non-parametric test for the equality of k variances\n",
            "        :func:`bartlett`\n",
            "            A parametric test for equality of k variances in normal samples\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Three variations of Levene's test are possible.  The possibilities\n",
            "        and their recommended usages are:\n",
            "        \n",
            "          * 'median' : Recommended for skewed (non-normal) distributions>\n",
            "          * 'mean' : Recommended for symmetric, moderate-tailed distributions.\n",
            "          * 'trimmed' : Recommended for heavy-tailed distributions.\n",
            "        \n",
            "        The test version using the mean was proposed in the original article\n",
            "        of Levene ([2]_) while the median and trimmed mean have been studied by\n",
            "        Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe\n",
            "        test.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm\n",
            "        .. [2] Levene, H. (1960). In Contributions to Probability and Statistics:\n",
            "               Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,\n",
            "               Stanford University Press, pp. 278-292.\n",
            "        .. [3] Brown, M. B. and Forsythe, A. B. (1974), Journal of the American\n",
            "               Statistical Association, 69, 364-367\n",
            "        .. [4] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n",
            "               Reference to the Vitamins, pp 499-503,\n",
            "               :doi:`10.1016/C2013-0-12584-6`.\n",
            "        .. [5] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [6] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n",
            "               superior to t and F tests in biomedical research. The American\n",
            "               Statistician, 52(2), 127-132.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [4]_, the influence of vitamin C on the tooth growth of guinea pigs\n",
            "        was investigated. In a control study, 60 subjects were divided into\n",
            "        small dose, medium dose, and large dose groups that received\n",
            "        daily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\n",
            "        After 42 days, the tooth growth was measured.\n",
            "        \n",
            "        The ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\n",
            "        tooth growth measurements of the three groups in microns.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> small_dose = np.array([\n",
            "        ...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n",
            "        ...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n",
            "        ... ])\n",
            "        >>> medium_dose = np.array([\n",
            "        ...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n",
            "        ...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n",
            "        ... ])\n",
            "        >>> large_dose = np.array([\n",
            "        ...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n",
            "        ...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n",
            "        ... ])\n",
            "        \n",
            "        The `levene` statistic is sensitive to differences in variances\n",
            "        between the samples.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.levene(small_dose, medium_dose, large_dose)\n",
            "        >>> res.statistic\n",
            "        0.6457341109631506\n",
            "        \n",
            "        The value of the statistic tends to be high when there is a large\n",
            "        difference in variances.\n",
            "        \n",
            "        We can test for inequality of variance among the groups by comparing the\n",
            "        observed value of the statistic against the null distribution: the\n",
            "        distribution of statistic values derived under the null hypothesis that\n",
            "        the population variances of the three groups are equal.\n",
            "        \n",
            "        For this test, the null distribution follows the F distribution as shown\n",
            "        below.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> k, n = 3, 60   # number of samples, total number of observations\n",
            "        >>> dist = stats.f(dfn=k-1, dfd=n-k)\n",
            "        >>> val = np.linspace(0, 5, 100)\n",
            "        >>> pdf = dist.pdf(val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(val, pdf, color='C0')\n",
            "        ...     ax.set_title(\"Levene Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        ...     ax.set_xlim(0, 5)\n",
            "        ...     ax.set_ylim(0, 1)\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution greater than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n",
            "        >>> i = val >= res.statistic\n",
            "        >>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> res.pvalue\n",
            "        0.5280694573759905\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from distributions with identical variances that produces\n",
            "        such an extreme value of the statistic - this may be taken as evidence\n",
            "        against the null hypothesis in favor of the alternative: the variances of\n",
            "        the groups are not equal. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [5]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        - Small p-values are not evidence for a *large* effect; rather, they can\n",
            "          only provide evidence for a \"significant\" effect, meaning that they are\n",
            "          unlikely to have occurred under the null hypothesis.\n",
            "        \n",
            "        Note that the F distribution provides an asymptotic approximation of the\n",
            "        null distribution.\n",
            "        For small samples, it may be more appropriate to perform a permutation\n",
            "        test: Under the null hypothesis that all three samples were drawn from\n",
            "        the same population, each of the measurements is equally likely to have\n",
            "        been observed in any of the three samples. Therefore, we can form a\n",
            "        randomized null distribution by calculating the statistic under many\n",
            "        randomly-generated partitionings of the observations into the three\n",
            "        samples.\n",
            "        \n",
            "        >>> def statistic(*samples):\n",
            "        ...     return stats.levene(*samples).statistic\n",
            "        >>> ref = stats.permutation_test(\n",
            "        ...     (small_dose, medium_dose, large_dose), statistic,\n",
            "        ...     permutation_type='independent', alternative='greater'\n",
            "        ... )\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> bins = np.linspace(0, 5, 25)\n",
            "        >>> ax.hist(\n",
            "        ...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n",
            "        ... )\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'randomized null distribution'])\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        >>> ref.pvalue  # randomized test p-value\n",
            "        0.4559  # may vary\n",
            "        \n",
            "        Note that there is significant disagreement between the p-value calculated\n",
            "        here and the asymptotic approximation returned by `levene` above.\n",
            "        The statistical inferences that can be drawn rigorously from a permutation\n",
            "        test are limited; nonetheless, they may be the preferred approach in many\n",
            "        circumstances [6]_.\n",
            "        \n",
            "        Following is another generic example where the null hypothesis would be\n",
            "        rejected.\n",
            "        \n",
            "        Test whether the lists `a`, `b` and `c` come from populations\n",
            "        with equal variances.\n",
            "        \n",
            "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
            "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
            "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
            "        >>> stat, p = stats.levene(a, b, c)\n",
            "        >>> p\n",
            "        0.002431505967249681\n",
            "        \n",
            "        The small p-value suggests that the populations do not have equal\n",
            "        variances.\n",
            "        \n",
            "        This is not surprising, given that the sample variance of `b` is much\n",
            "        larger than that of `a` and `c`:\n",
            "        \n",
            "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
            "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
            "    \n",
            "    linregress(x, y=None, alternative='two-sided')\n",
            "        Calculate a linear least-squares regression for two sets of measurements.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Two sets of measurements.  Both arrays should have the same length.  If\n",
            "            only `x` is given (and ``y=None``), then it must be a two-dimensional\n",
            "            array where one dimension has length 2.  The two sets of measurements\n",
            "            are then found by splitting the array along the length-2 dimension. In\n",
            "            the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\n",
            "            equivalent to ``linregress(x[0], x[1])``.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "        \n",
            "            * 'two-sided': the slope of the regression line is nonzero\n",
            "            * 'less': the slope of the regression line is less than zero\n",
            "            * 'greater':  the slope of the regression line is greater than zero\n",
            "        \n",
            "            .. versionadded:: 1.7.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : ``LinregressResult`` instance\n",
            "            The return value is an object with the following attributes:\n",
            "        \n",
            "            slope : float\n",
            "                Slope of the regression line.\n",
            "            intercept : float\n",
            "                Intercept of the regression line.\n",
            "            rvalue : float\n",
            "                The Pearson correlation coefficient. The square of ``rvalue``\n",
            "                is equal to the coefficient of determination.\n",
            "            pvalue : float\n",
            "                The p-value for a hypothesis test whose null hypothesis is\n",
            "                that the slope is zero, using Wald Test with t-distribution of\n",
            "                the test statistic. See `alternative` above for alternative\n",
            "                hypotheses.\n",
            "            stderr : float\n",
            "                Standard error of the estimated slope (gradient), under the\n",
            "                assumption of residual normality.\n",
            "            intercept_stderr : float\n",
            "                Standard error of the estimated intercept, under the assumption\n",
            "                of residual normality.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.optimize.curve_fit :\n",
            "            Use non-linear least squares to fit a function to data.\n",
            "        scipy.optimize.leastsq :\n",
            "            Minimize the sum of squares of a set of equations.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Missing values are considered pair-wise: if a value is missing in `x`,\n",
            "        the corresponding value in `y` is masked.\n",
            "        \n",
            "        For compatibility with older versions of SciPy, the return value acts\n",
            "        like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n",
            "        ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n",
            "        \n",
            "            slope, intercept, r, p, se = linregress(x, y)\n",
            "        \n",
            "        With that style, however, the standard error of the intercept is not\n",
            "        available.  To have access to all the computed values, including the\n",
            "        standard error of the intercept, use the return value as an object\n",
            "        with attributes, e.g.::\n",
            "        \n",
            "            result = linregress(x, y)\n",
            "            print(result.intercept, result.intercept_stderr)\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        \n",
            "        Generate some data:\n",
            "        \n",
            "        >>> x = rng.random(10)\n",
            "        >>> y = 1.6*x + rng.random(10)\n",
            "        \n",
            "        Perform the linear regression:\n",
            "        \n",
            "        >>> res = stats.linregress(x, y)\n",
            "        \n",
            "        Coefficient of determination (R-squared):\n",
            "        \n",
            "        >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n",
            "        R-squared: 0.717533\n",
            "        \n",
            "        Plot the data along with the fitted line:\n",
            "        \n",
            "        >>> plt.plot(x, y, 'o', label='original data')\n",
            "        >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n",
            "        >>> plt.legend()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Calculate 95% confidence interval on slope and intercept:\n",
            "        \n",
            "        >>> # Two-sided inverse Students t-distribution\n",
            "        >>> # p - probability, df - degrees of freedom\n",
            "        >>> from scipy.stats import t\n",
            "        >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n",
            "        \n",
            "        >>> ts = tinv(0.05, len(x)-2)\n",
            "        >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n",
            "        slope (95%): 1.453392 +/- 0.743465\n",
            "        >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n",
            "        ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n",
            "        intercept (95%): 0.616950 +/- 0.544475\n",
            "    \n",
            "    logrank(x: 'npt.ArrayLike | CensoredData', y: 'npt.ArrayLike | CensoredData', alternative: \"Literal['two-sided', 'less', 'greater']\" = 'two-sided') -> 'LogRankResult'\n",
            "        Compare the survival distributions of two samples via the logrank test.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like or CensoredData\n",
            "            Samples to compare based on their empirical survival functions.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "        \n",
            "            The null hypothesis is that the survival distributions of the two\n",
            "            groups, say *X* and *Y*, are identical.\n",
            "        \n",
            "            The following alternative hypotheses [4]_ are available (default is\n",
            "            'two-sided'):\n",
            "        \n",
            "            * 'two-sided': the survival distributions of the two groups are not\n",
            "              identical.\n",
            "            * 'less': survival of group *X* is favored: the group *X* failure rate\n",
            "              function is less than the group *Y* failure rate function at some\n",
            "              times.\n",
            "            * 'greater': survival of group *Y* is favored: the group *X* failure\n",
            "              rate function is greater than the group *Y* failure rate function at\n",
            "              some times.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : `~scipy.stats._result_classes.LogRankResult`\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float ndarray\n",
            "                The computed statistic (defined below). Its magnitude is the\n",
            "                square root of the magnitude returned by most other logrank test\n",
            "                implementations.\n",
            "            pvalue : float ndarray\n",
            "                The computed p-value of the test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.ecdf\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The logrank test [1]_ compares the observed number of events to\n",
            "        the expected number of events under the null hypothesis that the two\n",
            "        samples were drawn from the same distribution. The statistic is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            Z_i = \\frac{\\sum_{j=1}^J(O_{i,j}-E_{i,j})}{\\sqrt{\\sum_{j=1}^J V_{i,j}}}\n",
            "            \\rightarrow \\mathcal{N}(0,1)\n",
            "        \n",
            "        where\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            E_{i,j} = O_j \\frac{N_{i,j}}{N_j},\n",
            "            \\qquad\n",
            "            V_{i,j} = E_{i,j} \\left(\\frac{N_j-O_j}{N_j}\\right)\n",
            "            \\left(\\frac{N_j-N_{i,j}}{N_j-1}\\right),\n",
            "        \n",
            "        :math:`i` denotes the group (i.e. it may assume values :math:`x` or\n",
            "        :math:`y`, or it may be omitted to refer to the combined sample)\n",
            "        :math:`j` denotes the time (at which an event occurred),\n",
            "        :math:`N` is the number of subjects at risk just before an event occurred,\n",
            "        and :math:`O` is the observed number of events at that time.\n",
            "        \n",
            "        The ``statistic`` :math:`Z_x` returned by `logrank` is the (signed) square\n",
            "        root of the statistic returned by many other implementations. Under the\n",
            "        null hypothesis, :math:`Z_x**2` is asymptotically distributed according to\n",
            "        the chi-squared distribution with one degree of freedom. Consequently,\n",
            "        :math:`Z_x` is asymptotically distributed according to the standard normal\n",
            "        distribution. The advantage of using :math:`Z_x` is that the sign\n",
            "        information (i.e. whether the observed number of events tends to be less\n",
            "        than or greater than the number expected under the null hypothesis) is\n",
            "        preserved, allowing `scipy.stats.logrank` to offer one-sided alternative\n",
            "        hypotheses.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Mantel N. \"Evaluation of survival data and two new rank order\n",
            "               statistics arising in its consideration.\"\n",
            "               Cancer Chemotherapy Reports, 50(3):163-170, PMID: 5910392, 1966\n",
            "        .. [2] Bland, Altman, \"The logrank test\", BMJ, 328:1073,\n",
            "               :doi:`10.1136/bmj.328.7447.1073`, 2004\n",
            "        .. [3] \"Logrank test\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Logrank_test\n",
            "        .. [4] Brown, Mark. \"On the choice of variance for the log rank test.\"\n",
            "               Biometrika 71.1 (1984): 65-74.\n",
            "        .. [5] Klein, John P., and Melvin L. Moeschberger. Survival analysis:\n",
            "               techniques for censored and truncated data. Vol. 1230. New York:\n",
            "               Springer, 2003.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Reference [2]_ compared the survival times of patients with two different\n",
            "        types of recurrent malignant gliomas. The samples below record the time\n",
            "        (number of weeks) for which each patient participated in the study. The\n",
            "        `scipy.stats.CensoredData` class is used because the data is\n",
            "        right-censored: the uncensored observations correspond with observed deaths\n",
            "        whereas the censored observations correspond with the patient leaving the\n",
            "        study for another reason.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> x = stats.CensoredData(\n",
            "        ...     uncensored=[6, 13, 21, 30, 37, 38, 49, 50,\n",
            "        ...                 63, 79, 86, 98, 202, 219],\n",
            "        ...     right=[31, 47, 80, 82, 82, 149]\n",
            "        ... )\n",
            "        >>> y = stats.CensoredData(\n",
            "        ...     uncensored=[10, 10, 12, 13, 14, 15, 16, 17, 18, 20, 24, 24,\n",
            "        ...                 25, 28,30, 33, 35, 37, 40, 40, 46, 48, 76, 81,\n",
            "        ...                 82, 91, 112, 181],\n",
            "        ...     right=[34, 40, 70]\n",
            "        ... )\n",
            "        \n",
            "        We can calculate and visualize the empirical survival functions\n",
            "        of both groups as follows.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> ax = plt.subplot()\n",
            "        >>> ecdf_x = stats.ecdf(x)\n",
            "        >>> ecdf_x.sf.plot(ax, label='Astrocytoma')\n",
            "        >>> ecdf_y = stats.ecdf(y)\n",
            "        >>> ecdf_y.sf.plot(ax, label='Glioblastoma')\n",
            "        >>> ax.set_xlabel('Time to death (weeks)')\n",
            "        >>> ax.set_ylabel('Empirical SF')\n",
            "        >>> plt.legend()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Visual inspection of the empirical survival functions suggests that the\n",
            "        survival times tend to be different between the two groups. To formally\n",
            "        assess whether the difference is significant at the 1% level, we use the\n",
            "        logrank test.\n",
            "        \n",
            "        >>> res = stats.logrank(x=x, y=y)\n",
            "        >>> res.statistic\n",
            "        -2.73799...\n",
            "        >>> res.pvalue\n",
            "        0.00618...\n",
            "        \n",
            "        The p-value is less than 1%, so we can consider the data to be evidence\n",
            "        against the null hypothesis in favor of the alternative that there is a\n",
            "        difference between the two survival functions.\n",
            "    \n",
            "    mannwhitneyu(x, y, use_continuity=True, alternative='two-sided', axis=0, method='auto', *, nan_policy='propagate', keepdims=False)\n",
            "        Perform the Mann-Whitney U rank test on two independent samples.\n",
            "        \n",
            "        The Mann-Whitney U test is a nonparametric test of the null hypothesis\n",
            "        that the distribution underlying sample `x` is the same as the\n",
            "        distribution underlying sample `y`. It is often used as a test of\n",
            "        difference in location between distributions.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array-like\n",
            "            N-d arrays of samples. The arrays must be broadcastable except along\n",
            "            the dimension given by `axis`.\n",
            "        use_continuity : bool, optional\n",
            "            Whether a continuity correction (1/2) should be applied.\n",
            "            Default is True when `method` is ``'asymptotic'``; has no effect\n",
            "            otherwise.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            Let *F(u)* and *G(u)* be the cumulative distribution functions of the\n",
            "            distributions underlying `x` and `y`, respectively. Then the following\n",
            "            alternative hypotheses are available:\n",
            "            \n",
            "            * 'two-sided': the distributions are not equal, i.e. *F(u)  G(u)* for\n",
            "              at least one *u*.\n",
            "            * 'less': the distribution underlying `x` is stochastically less\n",
            "              than the distribution underlying `y`, i.e. *F(u) > G(u)* for all *u*.\n",
            "            * 'greater': the distribution underlying `x` is stochastically greater\n",
            "              than the distribution underlying `y`, i.e. *F(u) < G(u)* for all *u*.\n",
            "            \n",
            "            Note that the mathematical expressions in the alternative hypotheses\n",
            "            above describe the CDFs of the underlying distributions. The directions\n",
            "            of the inequalities appear inconsistent with the natural language\n",
            "            description at first glance, but they are not. For example, suppose\n",
            "            *X* and *Y* are random variables that follow distributions with CDFs\n",
            "            *F* and *G*, respectively. If *F(u) > G(u)* for all *u*, samples drawn\n",
            "            from *X* tend to be less than those drawn from *Y*.\n",
            "            \n",
            "            Under a more restrictive set of assumptions, the alternative hypotheses\n",
            "            can be expressed in terms of the locations of the distributions;\n",
            "            see [5] section 5.1.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        method : {'auto', 'asymptotic', 'exact'} or `PermutationMethod` instance, optional\n",
            "            Selects the method used to calculate the *p*-value.\n",
            "            Default is 'auto'. The following options are available.\n",
            "            \n",
            "            * ``'asymptotic'``: compares the standardized test statistic\n",
            "              against the normal distribution, correcting for ties.\n",
            "            * ``'exact'``: computes the exact *p*-value by comparing the observed\n",
            "              :math:`U` statistic against the exact distribution of the :math:`U`\n",
            "              statistic under the null hypothesis. No correction is made for ties.\n",
            "            * ``'auto'``: chooses ``'exact'`` when the size of one of the samples\n",
            "              is less than or equal to 8 and there are no ties;\n",
            "              chooses ``'asymptotic'`` otherwise.\n",
            "            * `PermutationMethod` instance. In this case, the p-value\n",
            "              is computed using `permutation_test` with the provided\n",
            "              configuration options and other appropriate settings.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : MannwhitneyuResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : float\n",
            "                The Mann-Whitney U statistic corresponding with sample `x`. See\n",
            "                Notes for the test statistic corresponding with sample `y`.\n",
            "            pvalue : float\n",
            "                The associated *p*-value for the chosen `alternative`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`scipy.stats.wilcoxon`, :func:`scipy.stats.ranksums`, :func:`scipy.stats.ttest_ind`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        If ``U1`` is the statistic corresponding with sample `x`, then the\n",
            "        statistic corresponding with sample `y` is\n",
            "        ``U2 = x.shape[axis] * y.shape[axis] - U1``.\n",
            "        \n",
            "        `mannwhitneyu` is for independent samples. For related / paired samples,\n",
            "        consider `scipy.stats.wilcoxon`.\n",
            "        \n",
            "        `method` ``'exact'`` is recommended when there are no ties and when either\n",
            "        sample size is less than 8 [1]_. The implementation follows the recurrence\n",
            "        relation originally proposed in [1]_ as it is described in [3]_.\n",
            "        Note that the exact method is *not* corrected for ties, but\n",
            "        `mannwhitneyu` will not raise errors or warnings if there are ties in the\n",
            "        data. If there are ties and either samples is small (fewer than ~10\n",
            "        observations), consider passing an instance of `PermutationMethod`\n",
            "        as the `method` to perform a permutation test.\n",
            "        \n",
            "        The Mann-Whitney U test is a non-parametric version of the t-test for\n",
            "        independent samples. When the means of samples from the populations\n",
            "        are normally distributed, consider `scipy.stats.ttest_ind`.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] H.B. Mann and D.R. Whitney, \"On a test of whether one of two random\n",
            "               variables is stochastically larger than the other\", The Annals of\n",
            "               Mathematical Statistics, Vol. 18, pp. 50-60, 1947.\n",
            "        .. [2] Mann-Whitney U Test, Wikipedia,\n",
            "               http://en.wikipedia.org/wiki/Mann-Whitney_U_test\n",
            "        .. [3] A. Di Bucchianico, \"Combinatorics, computer algebra, and the\n",
            "               Wilcoxon-Mann-Whitney test\", Journal of Statistical Planning and\n",
            "               Inference, Vol. 79, pp. 349-364, 1999.\n",
            "        .. [4] Rosie Shier, \"Statistics: 2.3 The Mann-Whitney U Test\", Mathematics\n",
            "               Learning Support Centre, 2004.\n",
            "        .. [5] Michael P. Fay and Michael A. Proschan. \"Wilcoxon-Mann-Whitney\n",
            "               or t-test? On assumptions for hypothesis tests and multiple \\\n",
            "               interpretations of decision rules.\" Statistics surveys, Vol. 4, pp.\n",
            "               1-39, 2010. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We follow the example from [4]_: nine randomly sampled young adults were\n",
            "        diagnosed with type II diabetes at the ages below.\n",
            "        \n",
            "        >>> males = [19, 22, 16, 29, 24]\n",
            "        >>> females = [20, 11, 17, 12]\n",
            "        \n",
            "        We use the Mann-Whitney U test to assess whether there is a statistically\n",
            "        significant difference in the diagnosis age of males and females.\n",
            "        The null hypothesis is that the distribution of male diagnosis ages is\n",
            "        the same as the distribution of female diagnosis ages. We decide\n",
            "        that a confidence level of 95% is required to reject the null hypothesis\n",
            "        in favor of the alternative that the distributions are different.\n",
            "        Since the number of samples is very small and there are no ties in the\n",
            "        data, we can compare the observed test statistic against the *exact*\n",
            "        distribution of the test statistic under the null hypothesis.\n",
            "        \n",
            "        >>> from scipy.stats import mannwhitneyu\n",
            "        >>> U1, p = mannwhitneyu(males, females, method=\"exact\")\n",
            "        >>> print(U1)\n",
            "        17.0\n",
            "        \n",
            "        `mannwhitneyu` always reports the statistic associated with the first\n",
            "        sample, which, in this case, is males. This agrees with :math:`U_M = 17`\n",
            "        reported in [4]_. The statistic associated with the second statistic\n",
            "        can be calculated:\n",
            "        \n",
            "        >>> nx, ny = len(males), len(females)\n",
            "        >>> U2 = nx*ny - U1\n",
            "        >>> print(U2)\n",
            "        3.0\n",
            "        \n",
            "        This agrees with :math:`U_F = 3` reported in [4]_. The two-sided\n",
            "        *p*-value can be calculated from either statistic, and the value produced\n",
            "        by `mannwhitneyu` agrees with :math:`p = 0.11` reported in [4]_.\n",
            "        \n",
            "        >>> print(p)\n",
            "        0.1111111111111111\n",
            "        \n",
            "        The exact distribution of the test statistic is asymptotically normal, so\n",
            "        the example continues by comparing the exact *p*-value against the\n",
            "        *p*-value produced using the normal approximation.\n",
            "        \n",
            "        >>> _, pnorm = mannwhitneyu(males, females, method=\"asymptotic\")\n",
            "        >>> print(pnorm)\n",
            "        0.11134688653314041\n",
            "        \n",
            "        Here `mannwhitneyu`'s reported *p*-value appears to conflict with the\n",
            "        value :math:`p = 0.09` given in [4]_. The reason is that [4]_\n",
            "        does not apply the continuity correction performed by `mannwhitneyu`;\n",
            "        `mannwhitneyu` reduces the distance between the test statistic and the\n",
            "        mean :math:`\\mu = n_x n_y / 2` by 0.5 to correct for the fact that the\n",
            "        discrete statistic is being compared against a continuous distribution.\n",
            "        Here, the :math:`U` statistic used is less than the mean, so we reduce\n",
            "        the distance by adding 0.5 in the numerator.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import norm\n",
            "        >>> U = min(U1, U2)\n",
            "        >>> N = nx + ny\n",
            "        >>> z = (U - nx*ny/2 + 0.5) / np.sqrt(nx*ny * (N + 1)/ 12)\n",
            "        >>> p = 2 * norm.cdf(z)  # use CDF to get p-value from smaller statistic\n",
            "        >>> print(p)\n",
            "        0.11134688653314041\n",
            "        \n",
            "        If desired, we can disable the continuity correction to get a result\n",
            "        that agrees with that reported in [4]_.\n",
            "        \n",
            "        >>> _, pnorm = mannwhitneyu(males, females, use_continuity=False,\n",
            "        ...                         method=\"asymptotic\")\n",
            "        >>> print(pnorm)\n",
            "        0.0864107329737\n",
            "        \n",
            "        Regardless of whether we perform an exact or asymptotic test, the\n",
            "        probability of the test statistic being as extreme or more extreme by\n",
            "        chance exceeds 5%, so we do not consider the results statistically\n",
            "        significant.\n",
            "        \n",
            "        Suppose that, before seeing the data, we had hypothesized that females\n",
            "        would tend to be diagnosed at a younger age than males.\n",
            "        In that case, it would be natural to provide the female ages as the\n",
            "        first input, and we would have performed a one-sided test using\n",
            "        ``alternative = 'less'``: females are diagnosed at an age that is\n",
            "        stochastically less than that of males.\n",
            "        \n",
            "        >>> res = mannwhitneyu(females, males, alternative=\"less\", method=\"exact\")\n",
            "        >>> print(res)\n",
            "        MannwhitneyuResult(statistic=3.0, pvalue=0.05555555555555555)\n",
            "        \n",
            "        Again, the probability of getting a sufficiently low value of the\n",
            "        test statistic by chance under the null hypothesis is greater than 5%,\n",
            "        so we do not reject the null hypothesis in favor of our alternative.\n",
            "        \n",
            "        If it is reasonable to assume that the means of samples from the\n",
            "        populations are normally distributed, we could have used a t-test to\n",
            "        perform the analysis.\n",
            "        \n",
            "        >>> from scipy.stats import ttest_ind\n",
            "        >>> res = ttest_ind(females, males, alternative=\"less\")\n",
            "        >>> print(res)\n",
            "        Ttest_indResult(statistic=-2.239334696520584, pvalue=0.030068441095757924)\n",
            "        \n",
            "        Under this assumption, the *p*-value would be low enough to reject the\n",
            "        null hypothesis in favor of the alternative.\n",
            "    \n",
            "    median_abs_deviation(x, axis=0, center=<function median at 0x788054d3caf0>, scale=1.0, nan_policy='propagate')\n",
            "        Compute the median absolute deviation of the data along the given axis.\n",
            "        \n",
            "        The median absolute deviation (MAD, [1]_) computes the median over the\n",
            "        absolute deviations from the median. It is a measure of dispersion\n",
            "        similar to the standard deviation but more robust to outliers [2]_.\n",
            "        \n",
            "        The MAD of an empty array is ``np.nan``.\n",
            "        \n",
            "        .. versionadded:: 1.5.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array or object that can be converted to an array.\n",
            "        axis : int or None, optional\n",
            "            Axis along which the range is computed. Default is 0. If None, compute\n",
            "            the MAD over the entire array.\n",
            "        center : callable, optional\n",
            "            A function that will return the central value. The default is to use\n",
            "            np.median. Any user defined function used will need to have the\n",
            "            function signature ``func(arr, axis)``.\n",
            "        scale : scalar or str, optional\n",
            "            The numerical value of scale will be divided out of the final\n",
            "            result. The default is 1.0. The string \"normal\" is also accepted,\n",
            "            and results in `scale` being the inverse of the standard normal\n",
            "            quantile function at 0.75, which is approximately 0.67449.\n",
            "            Array-like scale is also allowed, as long as it broadcasts correctly\n",
            "            to the output such that ``out / scale`` is a valid operation. The\n",
            "            output dimensions depend on the input array, `x`, and the `axis`\n",
            "            argument.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "            * 'propagate': returns nan\n",
            "            * 'raise': throws an error\n",
            "            * 'omit': performs the calculations ignoring nan values\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        mad : scalar or ndarray\n",
            "            If ``axis=None``, a scalar is returned. If the input contains\n",
            "            integers or floats of smaller precision than ``np.float64``, then the\n",
            "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
            "            the same as that of the input.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n",
            "        scipy.stats.tstd, scipy.stats.tvar\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The `center` argument only affects the calculation of the central value\n",
            "        around which the MAD is calculated. That is, passing in ``center=np.mean``\n",
            "        will calculate the MAD around the mean - it will not calculate the *mean*\n",
            "        absolute deviation.\n",
            "        \n",
            "        The input array may contain `inf`, but if `center` returns `inf`, the\n",
            "        corresponding MAD for that data will be `nan`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Median absolute deviation\",\n",
            "               https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
            "        .. [2] \"Robust measures of scale\",\n",
            "               https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        When comparing the behavior of `median_abs_deviation` with ``np.std``,\n",
            "        the latter is affected when we change a single value of an array to have an\n",
            "        outlier value while the MAD hardly changes:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n",
            "        >>> x.std()\n",
            "        0.9973906394005013\n",
            "        >>> stats.median_abs_deviation(x)\n",
            "        0.82832610097857\n",
            "        >>> x[0] = 345.6\n",
            "        >>> x.std()\n",
            "        34.42304872314415\n",
            "        >>> stats.median_abs_deviation(x)\n",
            "        0.8323442311590675\n",
            "        \n",
            "        Axis handling example:\n",
            "        \n",
            "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
            "        >>> x\n",
            "        array([[10,  7,  4],\n",
            "               [ 3,  2,  1]])\n",
            "        >>> stats.median_abs_deviation(x)\n",
            "        array([3.5, 2.5, 1.5])\n",
            "        >>> stats.median_abs_deviation(x, axis=None)\n",
            "        2.0\n",
            "        \n",
            "        Scale normal example:\n",
            "        \n",
            "        >>> x = stats.norm.rvs(size=1000000, scale=2, random_state=123456)\n",
            "        >>> stats.median_abs_deviation(x)\n",
            "        1.3487398527041636\n",
            "        >>> stats.median_abs_deviation(x, scale='normal')\n",
            "        1.9996446978061115\n",
            "    \n",
            "    median_test(*samples, ties='below', correction=True, lambda_=1, nan_policy='propagate')\n",
            "        Perform a Mood's median test.\n",
            "        \n",
            "        Test that two or more samples come from populations with the same median.\n",
            "        \n",
            "        Let ``n = len(samples)`` be the number of samples.  The \"grand median\" of\n",
            "        all the data is computed, and a contingency table is formed by\n",
            "        classifying the values in each sample as being above or below the grand\n",
            "        median.  The contingency table, along with `correction` and `lambda_`,\n",
            "        are passed to `scipy.stats.chi2_contingency` to compute the test statistic\n",
            "        and p-value.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            The set of samples.  There must be at least two samples.\n",
            "            Each sample must be a one-dimensional sequence containing at least\n",
            "            one value.  The samples are not required to have the same length.\n",
            "        ties : str, optional\n",
            "            Determines how values equal to the grand median are classified in\n",
            "            the contingency table.  The string must be one of::\n",
            "        \n",
            "                \"below\":\n",
            "                    Values equal to the grand median are counted as \"below\".\n",
            "                \"above\":\n",
            "                    Values equal to the grand median are counted as \"above\".\n",
            "                \"ignore\":\n",
            "                    Values equal to the grand median are not counted.\n",
            "        \n",
            "            The default is \"below\".\n",
            "        correction : bool, optional\n",
            "            If True, *and* there are just two samples, apply Yates' correction\n",
            "            for continuity when computing the test statistic associated with\n",
            "            the contingency table.  Default is True.\n",
            "        lambda_ : float or str, optional\n",
            "            By default, the statistic computed in this test is Pearson's\n",
            "            chi-squared statistic.  `lambda_` allows a statistic from the\n",
            "            Cressie-Read power divergence family to be used instead.  See\n",
            "            `power_divergence` for details.\n",
            "            Default is 1 (Pearson's chi-squared statistic).\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
            "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
            "            values. Default is 'propagate'.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : MedianTestResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                The test statistic.  The statistic that is returned is determined\n",
            "                by `lambda_`.  The default is Pearson's chi-squared statistic.\n",
            "            pvalue : float\n",
            "                The p-value of the test.\n",
            "            median : float\n",
            "                The grand median.\n",
            "            table : ndarray\n",
            "                The contingency table.  The shape of the table is (2, n), where\n",
            "                n is the number of samples.  The first row holds the counts of the\n",
            "                values above the grand median, and the second row holds the counts\n",
            "                of the values below the grand median.  The table allows further\n",
            "                analysis with, for example, `scipy.stats.chi2_contingency`, or with\n",
            "                `scipy.stats.fisher_exact` if there are two samples, without having\n",
            "                to recompute the table.  If ``nan_policy`` is \"propagate\" and there\n",
            "                are nans in the input, the return value for ``table`` is ``None``.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kruskal : Compute the Kruskal-Wallis H-test for independent samples.\n",
            "        mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        .. versionadded:: 0.15.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill\n",
            "            (1950), pp. 394-399.\n",
            "        .. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).\n",
            "            See Sections 8.12 and 10.15.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        A biologist runs an experiment in which there are three groups of plants.\n",
            "        Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.\n",
            "        Each plant produces a number of seeds.  The seed counts for each group\n",
            "        are::\n",
            "        \n",
            "            Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49\n",
            "            Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99\n",
            "            Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84\n",
            "        \n",
            "        The following code applies Mood's median test to these samples.\n",
            "        \n",
            "        >>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]\n",
            "        >>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]\n",
            "        >>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]\n",
            "        >>> from scipy.stats import median_test\n",
            "        >>> res = median_test(g1, g2, g3)\n",
            "        \n",
            "        The median is\n",
            "        \n",
            "        >>> res.median\n",
            "        34.0\n",
            "        \n",
            "        and the contingency table is\n",
            "        \n",
            "        >>> res.table\n",
            "        array([[ 5, 10,  7],\n",
            "               [11,  5, 10]])\n",
            "        \n",
            "        `p` is too large to conclude that the medians are not the same:\n",
            "        \n",
            "        >>> res.pvalue\n",
            "        0.12609082774093244\n",
            "        \n",
            "        The \"G-test\" can be performed by passing ``lambda_=\"log-likelihood\"`` to\n",
            "        `median_test`.\n",
            "        \n",
            "        >>> res = median_test(g1, g2, g3, lambda_=\"log-likelihood\")\n",
            "        >>> res.pvalue\n",
            "        0.12224779737117837\n",
            "        \n",
            "        The median occurs several times in the data, so we'll get a different\n",
            "        result if, for example, ``ties=\"above\"`` is used:\n",
            "        \n",
            "        >>> res = median_test(g1, g2, g3, ties=\"above\")\n",
            "        >>> res.pvalue\n",
            "        0.063873276069553273\n",
            "        \n",
            "        >>> res.table\n",
            "        array([[ 5, 11,  9],\n",
            "               [11,  4,  8]])\n",
            "        \n",
            "        This example demonstrates that if the data set is not large and there\n",
            "        are values equal to the median, the p-value can be sensitive to the\n",
            "        choice of `ties`.\n",
            "    \n",
            "    mode(a, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Return an array of the modal (most common) value in the passed array.\n",
            "        \n",
            "        If there is more than one such value, only one is returned.\n",
            "        The bin-count for the modal bins is also returned.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Numeric, n-dimensional array of which to find mode(s).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        mode : ndarray\n",
            "            Array of modal values.\n",
            "        count : ndarray\n",
            "            Array of counts for each mode.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The mode  is calculated using `numpy.unique`.\n",
            "        In NumPy versions 1.21 and after, all NaNs - even those with different\n",
            "        binary representations - are treated as equivalent and counted as separate\n",
            "        instances of the same value.\n",
            "        \n",
            "        By convention, the mode of an empty array is NaN, and the associated count\n",
            "        is zero.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> a = np.array([[3, 0, 3, 7],\n",
            "        ...               [3, 2, 6, 2],\n",
            "        ...               [1, 7, 2, 8],\n",
            "        ...               [3, 0, 6, 1],\n",
            "        ...               [3, 2, 5, 5]])\n",
            "        >>> from scipy import stats\n",
            "        >>> stats.mode(a, keepdims=True)\n",
            "        ModeResult(mode=array([[3, 0, 6, 1]]), count=array([[4, 2, 2, 1]]))\n",
            "        \n",
            "        To get mode of whole array, specify ``axis=None``:\n",
            "        \n",
            "        >>> stats.mode(a, axis=None, keepdims=True)\n",
            "        ModeResult(mode=[[3]], count=[[5]])\n",
            "        >>> stats.mode(a, axis=None, keepdims=False)\n",
            "        ModeResult(mode=3, count=5)\n",
            "    \n",
            "    moment(a, order=1, axis=0, nan_policy='propagate', *, center=None, keepdims=False)\n",
            "        Calculate the nth moment about the mean for a sample.\n",
            "        \n",
            "        A moment is a specific quantitative measure of the shape of a set of\n",
            "        points. It is often used to calculate coefficients of skewness and kurtosis\n",
            "        due to its close relationship with them.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        order : int or array_like of ints, optional\n",
            "            Order of central moment that is returned. Default is 1.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        center : float or None, optional\n",
            "            The point about which moments are taken. This can be the sample mean,\n",
            "            the origin, or any other be point. If `None` (default) compute the\n",
            "            center as the sample mean.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        n-th moment about the `center` : ndarray or float\n",
            "            The appropriate moment along the given axis or over all values if axis\n",
            "            is None. The denominator for the moment calculation is the number of\n",
            "            observations, no degrees of freedom correction is done.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kurtosis`, :func:`skew`, :func:`describe`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The k-th moment of a data sample is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - c)^k\n",
            "        \n",
            "        Where `n` is the number of samples, and `c` is the center around which the\n",
            "        moment is calculated. This function uses exponentiation by squares [1]_ for\n",
            "        efficiency.\n",
            "        \n",
            "        Note that, if `a` is an empty array (``a.size == 0``), array `moment` with\n",
            "        one element (`moment.size == 1`) is treated the same as scalar `moment`\n",
            "        (``np.isscalar(moment)``). This might produce arrays of unexpected shape.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import moment\n",
            "        >>> moment([1, 2, 3, 4, 5], order=1)\n",
            "        0.0\n",
            "        >>> moment([1, 2, 3, 4, 5], order=2)\n",
            "        2.0\n",
            "    \n",
            "    monte_carlo_test(data, rvs, statistic, *, vectorized=None, n_resamples=9999, batch=None, alternative='two-sided', axis=0)\n",
            "        Perform a Monte Carlo hypothesis test.\n",
            "        \n",
            "        `data` contains a sample or a sequence of one or more samples. `rvs`\n",
            "        specifies the distribution(s) of the sample(s) in `data` under the null\n",
            "        hypothesis. The value of `statistic` for the given `data` is compared\n",
            "        against a Monte Carlo null distribution: the value of the statistic for\n",
            "        each of `n_resamples` sets of samples generated using `rvs`. This gives\n",
            "        the p-value, the probability of observing such an extreme value of the\n",
            "        test statistic under the null hypothesis.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array-like or sequence of array-like\n",
            "            An array or sequence of arrays of observations.\n",
            "        rvs : callable or tuple of callables\n",
            "            A callable or sequence of callables that generates random variates\n",
            "            under the null hypothesis. Each element of `rvs` must be a callable\n",
            "            that accepts keyword argument ``size`` (e.g. ``rvs(size=(m, n))``) and\n",
            "            returns an N-d array sample of that shape. If `rvs` is a sequence, the\n",
            "            number of callables in `rvs` must match the number of samples in\n",
            "            `data`, i.e. ``len(rvs) == len(data)``. If `rvs` is a single callable,\n",
            "            `data` is treated as a single sample.\n",
            "        statistic : callable\n",
            "            Statistic for which the p-value of the hypothesis test is to be\n",
            "            calculated. `statistic` must be a callable that accepts a sample\n",
            "            (e.g. ``statistic(sample)``) or ``len(rvs)`` separate samples (e.g.\n",
            "            ``statistic(samples1, sample2)`` if `rvs` contains two callables and\n",
            "            `data` contains two samples) and returns the resulting statistic.\n",
            "            If `vectorized` is set ``True``, `statistic` must also accept a keyword\n",
            "            argument `axis` and be vectorized to compute the statistic along the\n",
            "            provided `axis` of the samples in `data`.\n",
            "        vectorized : bool, optional\n",
            "            If `vectorized` is set ``False``, `statistic` will not be passed\n",
            "            keyword argument `axis` and is expected to calculate the statistic\n",
            "            only for 1D samples. If ``True``, `statistic` will be passed keyword\n",
            "            argument `axis` and is expected to calculate the statistic along `axis`\n",
            "            when passed ND sample arrays. If ``None`` (default), `vectorized`\n",
            "            will be set ``True`` if ``axis`` is a parameter of `statistic`. Use of\n",
            "            a vectorized statistic typically reduces computation time.\n",
            "        n_resamples : int, default: 9999\n",
            "            Number of samples drawn from each of the callables of `rvs`.\n",
            "            Equivalently, the number statistic values under the null hypothesis\n",
            "            used as the Monte Carlo null distribution.\n",
            "        batch : int, optional\n",
            "            The number of Monte Carlo samples to process in each call to\n",
            "            `statistic`. Memory usage is O( `batch` * ``sample.size[axis]`` ). Default\n",
            "            is ``None``, in which case `batch` equals `n_resamples`.\n",
            "        alternative : {'two-sided', 'less', 'greater'}\n",
            "            The alternative hypothesis for which the p-value is calculated.\n",
            "            For each alternative, the p-value is defined as follows.\n",
            "        \n",
            "            - ``'greater'`` : the percentage of the null distribution that is\n",
            "              greater than or equal to the observed value of the test statistic.\n",
            "            - ``'less'`` : the percentage of the null distribution that is\n",
            "              less than or equal to the observed value of the test statistic.\n",
            "            - ``'two-sided'`` : twice the smaller of the p-values above.\n",
            "        \n",
            "        axis : int, default: 0\n",
            "            The axis of `data` (or each sample within `data`) over which to\n",
            "            calculate the statistic.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : MonteCarloTestResult\n",
            "            An object with attributes:\n",
            "        \n",
            "            statistic : float or ndarray\n",
            "                The test statistic of the observed `data`.\n",
            "            pvalue : float or ndarray\n",
            "                The p-value for the given alternative.\n",
            "            null_distribution : ndarray\n",
            "                The values of the test statistic generated under the null\n",
            "                hypothesis.\n",
            "        \n",
            "        .. warning::\n",
            "            The p-value is calculated by counting the elements of the null\n",
            "            distribution that are as extreme or more extreme than the observed\n",
            "            value of the statistic. Due to the use of finite precision arithmetic,\n",
            "            some statistic functions return numerically distinct values when the\n",
            "            theoretical values would be exactly equal. In some cases, this could\n",
            "            lead to a large error in the calculated p-value. `monte_carlo_test`\n",
            "            guards against this by considering elements in the null distribution\n",
            "            that are \"close\" (within a relative tolerance of 100 times the\n",
            "            floating point epsilon of inexact dtypes) to the observed\n",
            "            value of the test statistic as equal to the observed value of the\n",
            "            test statistic. However, the user is advised to inspect the null\n",
            "            distribution to assess whether this method of comparison is\n",
            "            appropriate, and if not, calculate the p-value manually.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "           Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n",
            "           Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        Suppose we wish to test whether a small sample has been drawn from a normal\n",
            "        distribution. We decide that we will use the skew of the sample as a\n",
            "        test statistic, and we will consider a p-value of 0.05 to be statistically\n",
            "        significant.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> def statistic(x, axis):\n",
            "        ...     return stats.skew(x, axis)\n",
            "        \n",
            "        After collecting our data, we calculate the observed value of the test\n",
            "        statistic.\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.skewnorm.rvs(a=1, size=50, random_state=rng)\n",
            "        >>> statistic(x, axis=0)\n",
            "        0.12457412450240658\n",
            "        \n",
            "        To determine the probability of observing such an extreme value of the\n",
            "        skewness by chance if the sample were drawn from the normal distribution,\n",
            "        we can perform a Monte Carlo hypothesis test. The test will draw many\n",
            "        samples at random from their normal distribution, calculate the skewness\n",
            "        of each sample, and compare our original skewness against this\n",
            "        distribution to determine an approximate p-value.\n",
            "        \n",
            "        >>> from scipy.stats import monte_carlo_test\n",
            "        >>> # because our statistic is vectorized, we pass `vectorized=True`\n",
            "        >>> rvs = lambda size: stats.norm.rvs(size=size, random_state=rng)\n",
            "        >>> res = monte_carlo_test(x, rvs, statistic, vectorized=True)\n",
            "        >>> print(res.statistic)\n",
            "        0.12457412450240658\n",
            "        >>> print(res.pvalue)\n",
            "        0.7012\n",
            "        \n",
            "        The probability of obtaining a test statistic less than or equal to the\n",
            "        observed value under the null hypothesis is ~70%. This is greater than\n",
            "        our chosen threshold of 5%, so we cannot consider this to be significant\n",
            "        evidence against the null hypothesis.\n",
            "        \n",
            "        Note that this p-value essentially matches that of\n",
            "        `scipy.stats.skewtest`, which relies on an asymptotic distribution of a\n",
            "        test statistic based on the sample skewness.\n",
            "        \n",
            "        >>> stats.skewtest(x).pvalue\n",
            "        0.6892046027110614\n",
            "        \n",
            "        This asymptotic approximation is not valid for small sample sizes, but\n",
            "        `monte_carlo_test` can be used with samples of any size.\n",
            "        \n",
            "        >>> x = stats.skewnorm.rvs(a=1, size=7, random_state=rng)\n",
            "        >>> # stats.skewtest(x) would produce an error due to small sample\n",
            "        >>> res = monte_carlo_test(x, rvs, statistic, vectorized=True)\n",
            "        \n",
            "        The Monte Carlo distribution of the test statistic is provided for\n",
            "        further investigation.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots()\n",
            "        >>> ax.hist(res.null_distribution, bins=50)\n",
            "        >>> ax.set_title(\"Monte Carlo distribution of test statistic\")\n",
            "        >>> ax.set_xlabel(\"Value of Statistic\")\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> plt.show()\n",
            "    \n",
            "    mood(x, y, axis=0, alternative='two-sided', *, nan_policy='propagate', keepdims=False)\n",
            "        Perform Mood's test for equal scale parameters.\n",
            "        \n",
            "        Mood's two-sample test for scale parameters is a non-parametric\n",
            "        test for the null hypothesis that two samples are drawn from the\n",
            "        same distribution with the same scale parameter.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Arrays of sample data.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "            \n",
            "            * 'two-sided': the scales of the distributions underlying `x` and `y`\n",
            "              are different.\n",
            "            * 'less': the scale of the distribution underlying `x` is less than\n",
            "              the scale of the distribution underlying `y`.\n",
            "            * 'greater': the scale of the distribution underlying `x` is greater\n",
            "              than the scale of the distribution underlying `y`.\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SignificanceResult\n",
            "            An object containing attributes:\n",
            "            \n",
            "            statistic : scalar or ndarray\n",
            "                The z-score for the hypothesis test.  For 1-D inputs a scalar is\n",
            "                returned.\n",
            "            pvalue : scalar ndarray\n",
            "                The p-value for the hypothesis test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`fligner`\n",
            "            A non-parametric test for the equality of k variances\n",
            "        :func:`ansari`\n",
            "            A non-parametric test for the equality of 2 variances\n",
            "        :func:`bartlett`\n",
            "            A parametric test for equality of k variances in normal samples\n",
            "        :func:`levene`\n",
            "            A parametric test for equality of k variances\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The data are assumed to be drawn from probability distributions ``f(x)``\n",
            "        and ``f(x/s) / s`` respectively, for some probability density function f.\n",
            "        The null hypothesis is that ``s == 1``.\n",
            "        \n",
            "        For multi-dimensional arrays, if the inputs are of shapes\n",
            "        ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\n",
            "        resulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n",
            "        ``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        [1] Mielke, Paul W. \"Note on Some Squared Rank Tests with Existing Ties.\"\n",
            "            Technometrics, vol. 9, no. 2, 1967, pp. 312-14. JSTOR,\n",
            "            https://doi.org/10.2307/1266427. Accessed 18 May 2022.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x2 = rng.standard_normal((2, 45, 6, 7))\n",
            "        >>> x1 = rng.standard_normal((2, 30, 6, 7))\n",
            "        >>> res = stats.mood(x1, x2, axis=1)\n",
            "        >>> res.pvalue.shape\n",
            "        (2, 6, 7)\n",
            "        \n",
            "        Find the number of points where the difference in scale is not significant:\n",
            "        \n",
            "        >>> (res.pvalue > 0.1).sum()\n",
            "        78\n",
            "        \n",
            "        Perform the test with different scales:\n",
            "        \n",
            "        >>> x1 = rng.standard_normal((2, 30))\n",
            "        >>> x2 = rng.standard_normal((2, 35)) * 10.0\n",
            "        >>> stats.mood(x1, x2, axis=1)\n",
            "        SignificanceResult(statistic=array([-5.76174136, -6.12650783]),\n",
            "                           pvalue=array([8.32505043e-09, 8.98287869e-10]))\n",
            "    \n",
            "    multiscale_graphcorr(x, y, compute_distance=<function _euclidean_dist at 0x788018b77240>, reps=1000, workers=1, is_twosamp=False, random_state=None)\n",
            "        Computes the Multiscale Graph Correlation (MGC) test statistic.\n",
            "        \n",
            "        Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for\n",
            "        one property (e.g. cloud density), and the :math:`l`-nearest neighbors for\n",
            "        the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is\n",
            "        called the \"scale\". A priori, however, it is not know which scales will be\n",
            "        most informative. So, MGC computes all distance pairs, and then efficiently\n",
            "        computes the distance correlations for all scales. The local correlations\n",
            "        illustrate which scales are relatively informative about the relationship.\n",
            "        The key, therefore, to successfully discover and decipher relationships\n",
            "        between disparate data modalities is to adaptively determine which scales\n",
            "        are the most informative, and the geometric implication for the most\n",
            "        informative scales. Doing so not only provides an estimate of whether the\n",
            "        modalities are related, but also provides insight into how the\n",
            "        determination was made. This is especially important in high-dimensional\n",
            "        data, where simple visualizations do not reveal relationships to the\n",
            "        unaided human eye. Characterizations of this implementation in particular\n",
            "        have been derived from and benchmarked within in [2]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : ndarray\n",
            "            If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is\n",
            "            the number of samples and `p` and `q` are the number of dimensions,\n",
            "            then the MGC independence test will be run.  Alternatively, ``x`` and\n",
            "            ``y`` can have shapes ``(n, n)`` if they are distance or similarity\n",
            "            matrices, and ``compute_distance`` must be sent to ``None``. If ``x``\n",
            "            and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired\n",
            "            two-sample MGC test will be run.\n",
            "        compute_distance : callable, optional\n",
            "            A function that computes the distance or similarity among the samples\n",
            "            within each data matrix. Set to ``None`` if ``x`` and ``y`` are\n",
            "            already distance matrices. The default uses the euclidean norm metric.\n",
            "            If you are calling a custom function, either create the distance\n",
            "            matrix before-hand or create a function of the form\n",
            "            ``compute_distance(x)`` where `x` is the data matrix for which\n",
            "            pairwise distances are calculated.\n",
            "        reps : int, optional\n",
            "            The number of replications used to estimate the null when using the\n",
            "            permutation test. The default is ``1000``.\n",
            "        workers : int or map-like callable, optional\n",
            "            If ``workers`` is an int the population is subdivided into ``workers``\n",
            "            sections and evaluated in parallel (uses ``multiprocessing.Pool\n",
            "            <multiprocessing>``). Supply ``-1`` to use all cores available to the\n",
            "            Process. Alternatively supply a map-like callable, such as\n",
            "            ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.\n",
            "            This evaluation is carried out as ``workers(func, iterable)``.\n",
            "            Requires that `func` be pickleable. The default is ``1``.\n",
            "        is_twosamp : bool, optional\n",
            "            If `True`, a two sample test will be run. If ``x`` and ``y`` have\n",
            "            shapes ``(n, p)`` and ``(m, p)``, this optional will be overridden and\n",
            "            set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes\n",
            "            ``(n, p)`` and a two sample test is desired. The default is ``False``.\n",
            "            Note that this will not run if inputs are distance matrices.\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                        `numpy.random.RandomState`}, optional\n",
            "        \n",
            "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "            singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `seed`.\n",
            "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "            that instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : MGCResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                The sample MGC test statistic within `[-1, 1]`.\n",
            "            pvalue : float\n",
            "                The p-value obtained via permutation.\n",
            "            mgc_dict : dict\n",
            "                Contains additional useful results:\n",
            "        \n",
            "                    - mgc_map : ndarray\n",
            "                        A 2D representation of the latent geometry of the\n",
            "                        relationship.\n",
            "                    - opt_scale : (int, int)\n",
            "                        The estimated optimal scale as a `(x, y)` pair.\n",
            "                    - null_dist : list\n",
            "                        The null distribution derived from the permuted matrices.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        pearsonr : Pearson correlation coefficient and p-value for testing\n",
            "                   non-correlation.\n",
            "        kendalltau : Calculates Kendall's tau.\n",
            "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        A description of the process of MGC and applications on neuroscience data\n",
            "        can be found in [1]_. It is performed using the following steps:\n",
            "        \n",
            "        #. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and\n",
            "           modified to be mean zero columnwise. This results in two\n",
            "           :math:`n \\times n` distance matrices :math:`A` and :math:`B` (the\n",
            "           centering and unbiased modification) [3]_.\n",
            "        \n",
            "        #. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,\n",
            "        \n",
            "           * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs\n",
            "             are calculated for each property. Here, :math:`G_k (i, j)` indicates\n",
            "             the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`\n",
            "             and :math:`H_l (i, j)` indicates the :math:`l` smallested values of\n",
            "             the :math:`i`-th row of :math:`B`\n",
            "        \n",
            "           * Let :math:`\\circ` denotes the entry-wise matrix product, then local\n",
            "             correlations are summed and normalized using the following statistic:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            c^{kl} = \\frac{\\sum_{ij} A G_k B H_l}\n",
            "                          {\\sqrt{\\sum_{ij} A^2 G_k \\times \\sum_{ij} B^2 H_l}}\n",
            "        \n",
            "        #. The MGC test statistic is the smoothed optimal local correlation of\n",
            "           :math:`\\{ c^{kl} \\}`. Denote the smoothing operation as :math:`R(\\cdot)`\n",
            "           (which essentially set all isolated large correlations) as 0 and\n",
            "           connected large correlations the same as before, see [3]_.) MGC is,\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            MGC_n (x, y) = \\max_{(k, l)} R \\left(c^{kl} \\left( x_n, y_n \\right)\n",
            "                                                        \\right)\n",
            "        \n",
            "        The test statistic returns a value between :math:`(-1, 1)` since it is\n",
            "        normalized.\n",
            "        \n",
            "        The p-value returned is calculated using a permutation test. This process\n",
            "        is completed by first randomly permuting :math:`y` to estimate the null\n",
            "        distribution and then calculating the probability of observing a test\n",
            "        statistic, under the null, at least as extreme as the observed test\n",
            "        statistic.\n",
            "        \n",
            "        MGC requires at least 5 samples to run with reliable results. It can also\n",
            "        handle high-dimensional data sets.\n",
            "        In addition, by manipulating the input data matrices, the two-sample\n",
            "        testing problem can be reduced to the independence testing problem [4]_.\n",
            "        Given sample data :math:`U` and :math:`V` of sizes :math:`p \\times n`\n",
            "        :math:`p \\times m`, data matrix :math:`X` and :math:`Y` can be created as\n",
            "        follows:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            X = [U | V] \\in \\mathcal{R}^{p \\times (n + m)}\n",
            "            Y = [0_{1 \\times n} | 1_{1 \\times m}] \\in \\mathcal{R}^{(n + m)}\n",
            "        \n",
            "        Then, the MGC statistic can be calculated as normal. This methodology can\n",
            "        be extended to similar tests such as distance correlation [4]_.\n",
            "        \n",
            "        .. versionadded:: 1.4.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,\n",
            "               Maggioni, M., & Shen, C. (2019). Discovering and deciphering\n",
            "               relationships across disparate data modalities. ELife.\n",
            "        .. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,\n",
            "               Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).\n",
            "               mgcpy: A Comprehensive High Dimensional Independence Testing Python\n",
            "               Package. :arXiv:`1907.02088`\n",
            "        .. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance\n",
            "               correlation to multiscale graph correlation. Journal of the American\n",
            "               Statistical Association.\n",
            "        .. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of\n",
            "               Distance and Kernel Methods for Hypothesis Testing.\n",
            "               :arXiv:`1806.05514`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import multiscale_graphcorr\n",
            "        >>> x = np.arange(100)\n",
            "        >>> y = x\n",
            "        >>> res = multiscale_graphcorr(x, y)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (1.0, 0.001)\n",
            "        \n",
            "        To run an unpaired two-sample test,\n",
            "        \n",
            "        >>> x = np.arange(100)\n",
            "        >>> y = np.arange(79)\n",
            "        >>> res = multiscale_graphcorr(x, y)\n",
            "        >>> res.statistic, res.pvalue  # doctest: +SKIP\n",
            "        (0.033258146255703246, 0.023)\n",
            "        \n",
            "        or, if shape of the inputs are the same,\n",
            "        \n",
            "        >>> x = np.arange(100)\n",
            "        >>> y = x\n",
            "        >>> res = multiscale_graphcorr(x, y, is_twosamp=True)\n",
            "        >>> res.statistic, res.pvalue  # doctest: +SKIP\n",
            "        (-0.008021809890200488, 1.0)\n",
            "    \n",
            "    mvsdist(data)\n",
            "        'Frozen' distributions for mean, variance, and standard deviation of data.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array_like\n",
            "            Input array. Converted to 1-D using ravel.\n",
            "            Requires 2 or more data-points.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        mdist : \"frozen\" distribution object\n",
            "            Distribution object representing the mean of the data.\n",
            "        vdist : \"frozen\" distribution object\n",
            "            Distribution object representing the variance of the data.\n",
            "        sdist : \"frozen\" distribution object\n",
            "            Distribution object representing the standard deviation of the data.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        bayes_mvs\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The return values from ``bayes_mvs(data)`` is equivalent to\n",
            "        ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n",
            "        \n",
            "        In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n",
            "        on the three distribution objects returned from this function will give\n",
            "        the same results that are returned from `bayes_mvs`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
            "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
            "        2006.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
            "        >>> mean, var, std = stats.mvsdist(data)\n",
            "        \n",
            "        We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n",
            "        examine:\n",
            "        \n",
            "        >>> mean.mean()\n",
            "        9.0\n",
            "        >>> mean.interval(0.95)\n",
            "        (6.6120585482655692, 11.387941451734431)\n",
            "        >>> mean.std()\n",
            "        1.1952286093343936\n",
            "    \n",
            "    normaltest(a, axis=0, nan_policy='propagate', *, keepdims=False)\n",
            "        Test whether a sample differs from a normal distribution.\n",
            "        \n",
            "        This function tests the null hypothesis that a sample comes\n",
            "        from a normal distribution.  It is based on D'Agostino and\n",
            "        Pearson's [1]_, [2]_ test that combines skew and kurtosis to\n",
            "        produce an omnibus test of normality.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            The array containing the sample to be tested.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float or array\n",
            "            ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n",
            "            ``k`` is the z-score returned by `kurtosistest`.\n",
            "        pvalue : float or array\n",
            "            A 2-sided chi squared probability for the hypothesis test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n",
            "               moderate and large sample size\", Biometrika, 58, 341-348\n",
            "        .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n",
            "               normality\", Biometrika, 60, 613-622\n",
            "        .. [3] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n",
            "               for normality (complete samples). Biometrika, 52(3/4), 591-611.\n",
            "        .. [4] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        .. [5] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n",
            "               research. The open cardiovascular medicine journal, 2, 97.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to infer from measurements whether the weights of adult\n",
            "        human males in a medical study are not normally distributed [3]_.\n",
            "        The weights (lbs) are recorded in the array ``x`` below.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n",
            "        \n",
            "        The normality test of [1]_ and [2]_ begins by computing a statistic based\n",
            "        on the sample skewness and kurtosis.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.normaltest(x)\n",
            "        >>> res.statistic\n",
            "        13.034263121192582\n",
            "        \n",
            "        (The test warns that our sample has too few observations to perform the\n",
            "        test. We'll return to this at the end of the example.)\n",
            "        Because the normal distribution has zero skewness and zero\n",
            "        (\"excess\" or \"Fisher\") kurtosis, the value of this statistic tends to be\n",
            "        low for samples drawn from a normal distribution.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the statistic\n",
            "        against the null distribution: the distribution of statistic values derived\n",
            "        under the null hypothesis that the weights were drawn from a normal\n",
            "        distribution.\n",
            "        For this normality test, the null distribution for very large samples is\n",
            "        the chi-squared distribution with two degrees of freedom.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> dist = stats.chi2(df=2)\n",
            "        >>> stat_vals = np.linspace(0, 16, 100)\n",
            "        >>> pdf = dist.pdf(stat_vals)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(stat_vals, pdf)\n",
            "        ...     ax.set_title(\"Normality Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution greater than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (13.5, 5e-4), (14, 5e-3), arrowprops=props)\n",
            "        >>> i = stat_vals >= res.statistic  # index more extreme statistic values\n",
            "        >>> ax.fill_between(stat_vals[i], y1=0, y2=pdf[i])\n",
            "        >>> ax.set_xlim(8, 16)\n",
            "        >>> ax.set_ylim(0, 0.01)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0014779023013100172\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from a normally distributed population that produces such an\n",
            "        extreme value of the statistic - this may be taken as evidence against\n",
            "        the null hypothesis in favor of the alternative: the weights were not\n",
            "        drawn from a normal distribution. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [4]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        \n",
            "        Note that the chi-squared distribution provides an asymptotic\n",
            "        approximation of the null distribution; it is only accurate for samples\n",
            "        with many observations. This is the reason we received a warning at the\n",
            "        beginning of the example; our sample is quite small. In this case,\n",
            "        `scipy.stats.monte_carlo_test` may provide a more accurate, albeit\n",
            "        stochastic, approximation of the exact p-value.\n",
            "        \n",
            "        >>> def statistic(x, axis):\n",
            "        ...     # Get only the `normaltest` statistic; ignore approximate p-value\n",
            "        ...     return stats.normaltest(x, axis=axis).statistic\n",
            "        >>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n",
            "        ...                              alternative='greater')\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> ax.hist(res.null_distribution, np.linspace(0, 25, 50),\n",
            "        ...         density=True)\n",
            "        >>> ax.legend(['aymptotic approximation (many observations)',\n",
            "        ...            'Monte Carlo approximation (11 observations)'])\n",
            "        >>> ax.set_xlim(0, 14)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0082  # may vary\n",
            "        \n",
            "        Furthermore, despite their stochastic nature, p-values computed in this way\n",
            "        can be used to exactly control the rate of false rejections of the null\n",
            "        hypothesis [5]_.\n",
            "    \n",
            "    obrientransform(*samples)\n",
            "        Compute the O'Brien transform on input data (any number of arrays).\n",
            "        \n",
            "        Used to test for homogeneity of variance prior to running one-way stats.\n",
            "        Each array in ``*samples`` is one level of a factor.\n",
            "        If `f_oneway` is run on the transformed data and found significant,\n",
            "        the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            Any number of arrays.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        obrientransform : ndarray\n",
            "            Transformed data for use in an ANOVA.  The first dimension\n",
            "            of the result corresponds to the sequence of transformed\n",
            "            arrays.  If the arrays given are all 1-D of the same length,\n",
            "            the return value is a 2-D array; otherwise it is a 1-D array\n",
            "            of type object, with each element being an ndarray.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n",
            "               Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We'll test the following data sets for differences in their variance.\n",
            "        \n",
            "        >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n",
            "        >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n",
            "        \n",
            "        Apply the O'Brien transform to the data.\n",
            "        \n",
            "        >>> from scipy.stats import obrientransform\n",
            "        >>> tx, ty = obrientransform(x, y)\n",
            "        \n",
            "        Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n",
            "        transformed data.\n",
            "        \n",
            "        >>> from scipy.stats import f_oneway\n",
            "        >>> F, p = f_oneway(tx, ty)\n",
            "        >>> p\n",
            "        0.1314139477040335\n",
            "        \n",
            "        If we require that ``p < 0.05`` for significance, we cannot conclude\n",
            "        that the variances are different.\n",
            "    \n",
            "    page_trend_test(data, ranked=False, predicted_ranks=None, method='auto')\n",
            "        Perform Page's Test, a measure of trend in observations between treatments.\n",
            "        \n",
            "        Page's Test (also known as Page's :math:`L` test) is useful when:\n",
            "        \n",
            "        * there are :math:`n \\geq 3` treatments,\n",
            "        * :math:`m \\geq 2` subjects are observed for each treatment, and\n",
            "        * the observations are hypothesized to have a particular order.\n",
            "        \n",
            "        Specifically, the test considers the null hypothesis that\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            m_1 = m_2 = m_3 \\cdots = m_n,\n",
            "        \n",
            "        where :math:`m_j` is the mean of the observed quantity under treatment\n",
            "        :math:`j`, against the alternative hypothesis that\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            m_1 \\leq m_2 \\leq m_3 \\leq \\cdots \\leq m_n,\n",
            "        \n",
            "        where at least one inequality is strict.\n",
            "        \n",
            "        As noted by [4]_, Page's :math:`L` test has greater statistical power than\n",
            "        the Friedman test against the alternative that there is a difference in\n",
            "        trend, as Friedman's test only considers a difference in the means of the\n",
            "        observations without considering their order. Whereas Spearman :math:`\\rho`\n",
            "        considers the correlation between the ranked observations of two variables\n",
            "        (e.g. the airspeed velocity of a swallow vs. the weight of the coconut it\n",
            "        carries), Page's :math:`L` is concerned with a trend in an observation\n",
            "        (e.g. the airspeed velocity of a swallow) across several distinct\n",
            "        treatments (e.g. carrying each of five coconuts of different weight) even\n",
            "        as the observation is repeated with multiple subjects (e.g. one European\n",
            "        swallow and one African swallow).\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : array-like\n",
            "            A :math:`m \\times n` array; the element in row :math:`i` and\n",
            "            column :math:`j` is the observation corresponding with subject\n",
            "            :math:`i` and treatment :math:`j`. By default, the columns are\n",
            "            assumed to be arranged in order of increasing predicted mean.\n",
            "        \n",
            "        ranked : boolean, optional\n",
            "            By default, `data` is assumed to be observations rather than ranks;\n",
            "            it will be ranked with `scipy.stats.rankdata` along ``axis=1``. If\n",
            "            `data` is provided in the form of ranks, pass argument ``True``.\n",
            "        \n",
            "        predicted_ranks : array-like, optional\n",
            "            The predicted ranks of the column means. If not specified,\n",
            "            the columns are assumed to be arranged in order of increasing\n",
            "            predicted mean, so the default `predicted_ranks` are\n",
            "            :math:`[1, 2, \\dots, n-1, n]`.\n",
            "        \n",
            "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
            "            Selects the method used to calculate the *p*-value. The following\n",
            "            options are available.\n",
            "        \n",
            "            * 'auto': selects between 'exact' and 'asymptotic' to\n",
            "              achieve reasonably accurate results in reasonable time (default)\n",
            "            * 'asymptotic': compares the standardized test statistic against\n",
            "              the normal distribution\n",
            "            * 'exact': computes the exact *p*-value by comparing the observed\n",
            "              :math:`L` statistic against those realized by all possible\n",
            "              permutations of ranks (under the null hypothesis that each\n",
            "              permutation is equally likely)\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : PageTrendTestResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                Page's :math:`L` test statistic.\n",
            "            pvalue : float\n",
            "                The associated *p*-value\n",
            "            method : {'asymptotic', 'exact'}\n",
            "                The method used to compute the *p*-value\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        rankdata, friedmanchisquare, spearmanr\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        As noted in [1]_, \"the :math:`n` 'treatments' could just as well represent\n",
            "        :math:`n` objects or events or performances or persons or trials ranked.\"\n",
            "        Similarly, the :math:`m` 'subjects' could equally stand for :math:`m`\n",
            "        \"groupings by ability or some other control variable, or judges doing\n",
            "        the ranking, or random replications of some other sort.\"\n",
            "        \n",
            "        The procedure for calculating the :math:`L` statistic, adapted from\n",
            "        [1]_, is:\n",
            "        \n",
            "        1. \"Predetermine with careful logic the appropriate hypotheses\n",
            "           concerning the predicted ordering of the experimental results.\n",
            "           If no reasonable basis for ordering any treatments is known, the\n",
            "           :math:`L` test is not appropriate.\"\n",
            "        2. \"As in other experiments, determine at what level of confidence\n",
            "           you will reject the null hypothesis that there is no agreement of\n",
            "           experimental results with the monotonic hypothesis.\"\n",
            "        3. \"Cast the experimental material into a two-way table of :math:`n`\n",
            "           columns (treatments, objects ranked, conditions) and :math:`m`\n",
            "           rows (subjects, replication groups, levels of control variables).\"\n",
            "        4. \"When experimental observations are recorded, rank them across each\n",
            "           row\", e.g. ``ranks = scipy.stats.rankdata(data, axis=1)``.\n",
            "        5. \"Add the ranks in each column\", e.g.\n",
            "           ``colsums = np.sum(ranks, axis=0)``.\n",
            "        6. \"Multiply each sum of ranks by the predicted rank for that same\n",
            "           column\", e.g. ``products = predicted_ranks * colsums``.\n",
            "        7. \"Sum all such products\", e.g. ``L = products.sum()``.\n",
            "        \n",
            "        [1]_ continues by suggesting use of the standardized statistic\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\chi_L^2 = \\frac{\\left[12L-3mn(n+1)^2\\right]^2}{mn^2(n^2-1)(n+1)}\n",
            "        \n",
            "        \"which is distributed approximately as chi-square with 1 degree of\n",
            "        freedom. The ordinary use of :math:`\\chi^2` tables would be\n",
            "        equivalent to a two-sided test of agreement. If a one-sided test\n",
            "        is desired, *as will almost always be the case*, the probability\n",
            "        discovered in the chi-square table should be *halved*.\"\n",
            "        \n",
            "        However, this standardized statistic does not distinguish between the\n",
            "        observed values being well correlated with the predicted ranks and being\n",
            "        _anti_-correlated with the predicted ranks. Instead, we follow [2]_\n",
            "        and calculate the standardized statistic\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\Lambda = \\frac{L - E_0}{\\sqrt{V_0}},\n",
            "        \n",
            "        where :math:`E_0 = \\frac{1}{4} mn(n+1)^2` and\n",
            "        :math:`V_0 = \\frac{1}{144} mn^2(n+1)(n^2-1)`, \"which is asymptotically\n",
            "        normal under the null hypothesis\".\n",
            "        \n",
            "        The *p*-value for ``method='exact'`` is generated by comparing the observed\n",
            "        value of :math:`L` against the :math:`L` values generated for all\n",
            "        :math:`(n!)^m` possible permutations of ranks. The calculation is performed\n",
            "        using the recursive method of [5].\n",
            "        \n",
            "        The *p*-values are not adjusted for the possibility of ties. When\n",
            "        ties are present, the reported  ``'exact'`` *p*-values may be somewhat\n",
            "        larger (i.e. more conservative) than the true *p*-value [2]_. The\n",
            "        ``'asymptotic'``` *p*-values, however, tend to be smaller (i.e. less\n",
            "        conservative) than the ``'exact'`` *p*-values.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Ellis Batten Page, \"Ordered hypotheses for multiple treatments:\n",
            "           a significant test for linear ranks\", *Journal of the American\n",
            "           Statistical Association* 58(301), p. 216--230, 1963.\n",
            "        \n",
            "        .. [2] Markus Neuhauser, *Nonparametric Statistical Test: A computational\n",
            "           approach*, CRC Press, p. 150--152, 2012.\n",
            "        \n",
            "        .. [3] Statext LLC, \"Page's L Trend Test - Easy Statistics\", *Statext -\n",
            "           Statistics Study*, https://www.statext.com/practice/PageTrendTest03.php,\n",
            "           Accessed July 12, 2020.\n",
            "        \n",
            "        .. [4] \"Page's Trend Test\", *Wikipedia*, WikimediaFoundation,\n",
            "           https://en.wikipedia.org/wiki/Page%27s_trend_test,\n",
            "           Accessed July 12, 2020.\n",
            "        \n",
            "        .. [5] Robert E. Odeh, \"The exact distribution of Page's L-statistic in\n",
            "           the two-way layout\", *Communications in Statistics - Simulation and\n",
            "           Computation*,  6(1), p. 49--61, 1977.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We use the example from [3]_: 10 students are asked to rate three\n",
            "        teaching methods - tutorial, lecture, and seminar - on a scale of 1-5,\n",
            "        with 1 being the lowest and 5 being the highest. We have decided that\n",
            "        a confidence level of 99% is required to reject the null hypothesis in\n",
            "        favor of our alternative: that the seminar will have the highest ratings\n",
            "        and the tutorial will have the lowest. Initially, the data have been\n",
            "        tabulated with each row representing an individual student's ratings of\n",
            "        the three methods in the following order: tutorial, lecture, seminar.\n",
            "        \n",
            "        >>> table = [[3, 4, 3],\n",
            "        ...          [2, 2, 4],\n",
            "        ...          [3, 3, 5],\n",
            "        ...          [1, 3, 2],\n",
            "        ...          [2, 3, 2],\n",
            "        ...          [2, 4, 5],\n",
            "        ...          [1, 2, 4],\n",
            "        ...          [3, 4, 4],\n",
            "        ...          [2, 4, 5],\n",
            "        ...          [1, 3, 4]]\n",
            "        \n",
            "        Because the tutorial is hypothesized to have the lowest ratings, the\n",
            "        column corresponding with tutorial rankings should be first; the seminar\n",
            "        is hypothesized to have the highest ratings, so its column should be last.\n",
            "        Since the columns are already arranged in this order of increasing\n",
            "        predicted mean, we can pass the table directly into `page_trend_test`.\n",
            "        \n",
            "        >>> from scipy.stats import page_trend_test\n",
            "        >>> res = page_trend_test(table)\n",
            "        >>> res\n",
            "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
            "                            method='exact')\n",
            "        \n",
            "        This *p*-value indicates that there is a 0.1819% chance that\n",
            "        the :math:`L` statistic would reach such an extreme value under the null\n",
            "        hypothesis. Because 0.1819% is less than 1%, we have evidence to reject\n",
            "        the null hypothesis in favor of our alternative at a 99% confidence level.\n",
            "        \n",
            "        The value of the :math:`L` statistic is 133.5. To check this manually,\n",
            "        we rank the data such that high scores correspond with high ranks, settling\n",
            "        ties with an average rank:\n",
            "        \n",
            "        >>> from scipy.stats import rankdata\n",
            "        >>> ranks = rankdata(table, axis=1)\n",
            "        >>> ranks\n",
            "        array([[1.5, 3. , 1.5],\n",
            "               [1.5, 1.5, 3. ],\n",
            "               [1.5, 1.5, 3. ],\n",
            "               [1. , 3. , 2. ],\n",
            "               [1.5, 3. , 1.5],\n",
            "               [1. , 2. , 3. ],\n",
            "               [1. , 2. , 3. ],\n",
            "               [1. , 2.5, 2.5],\n",
            "               [1. , 2. , 3. ],\n",
            "               [1. , 2. , 3. ]])\n",
            "        \n",
            "        We add the ranks within each column, multiply the sums by the\n",
            "        predicted ranks, and sum the products.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> m, n = ranks.shape\n",
            "        >>> predicted_ranks = np.arange(1, n+1)\n",
            "        >>> L = (predicted_ranks * np.sum(ranks, axis=0)).sum()\n",
            "        >>> res.statistic == L\n",
            "        True\n",
            "        \n",
            "        As presented in [3]_, the asymptotic approximation of the *p*-value is the\n",
            "        survival function of the normal distribution evaluated at the standardized\n",
            "        test statistic:\n",
            "        \n",
            "        >>> from scipy.stats import norm\n",
            "        >>> E0 = (m*n*(n+1)**2)/4\n",
            "        >>> V0 = (m*n**2*(n+1)*(n**2-1))/144\n",
            "        >>> Lambda = (L-E0)/np.sqrt(V0)\n",
            "        >>> p = norm.sf(Lambda)\n",
            "        >>> p\n",
            "        0.0012693433690751756\n",
            "        \n",
            "        This does not precisely match the *p*-value reported by `page_trend_test`\n",
            "        above. The asymptotic distribution is not very accurate, nor conservative,\n",
            "        for :math:`m \\leq 12` and :math:`n \\leq 8`, so `page_trend_test` chose to\n",
            "        use ``method='exact'`` based on the dimensions of the table and the\n",
            "        recommendations in Page's original paper [1]_. To override\n",
            "        `page_trend_test`'s choice, provide the `method` argument.\n",
            "        \n",
            "        >>> res = page_trend_test(table, method=\"asymptotic\")\n",
            "        >>> res\n",
            "        PageTrendTestResult(statistic=133.5, pvalue=0.0012693433690751756,\n",
            "                            method='asymptotic')\n",
            "        \n",
            "        If the data are already ranked, we can pass in the ``ranks`` instead of\n",
            "        the ``table`` to save computation time.\n",
            "        \n",
            "        >>> res = page_trend_test(ranks,             # ranks of data\n",
            "        ...                       ranked=True,       # data is already ranked\n",
            "        ...                       )\n",
            "        >>> res\n",
            "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
            "                            method='exact')\n",
            "        \n",
            "        Suppose the raw data had been tabulated in an order different from the\n",
            "        order of predicted means, say lecture, seminar, tutorial.\n",
            "        \n",
            "        >>> table = np.asarray(table)[:, [1, 2, 0]]\n",
            "        \n",
            "        Since the arrangement of this table is not consistent with the assumed\n",
            "        ordering, we can either rearrange the table or provide the\n",
            "        `predicted_ranks`. Remembering that the lecture is predicted\n",
            "        to have the middle rank, the seminar the highest, and tutorial the lowest,\n",
            "        we pass:\n",
            "        \n",
            "        >>> res = page_trend_test(table,             # data as originally tabulated\n",
            "        ...                       predicted_ranks=[2, 3, 1],  # our predicted order\n",
            "        ...                       )\n",
            "        >>> res\n",
            "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
            "                            method='exact')\n",
            "    \n",
            "    pearsonr(x, y, *, alternative='two-sided', method=None)\n",
            "        Pearson correlation coefficient and p-value for testing non-correlation.\n",
            "        \n",
            "        The Pearson correlation coefficient [1]_ measures the linear relationship\n",
            "        between two datasets. Like other correlation\n",
            "        coefficients, this one varies between -1 and +1 with 0 implying no\n",
            "        correlation. Correlations of -1 or +1 imply an exact linear relationship.\n",
            "        Positive correlations imply that as x increases, so does y. Negative\n",
            "        correlations imply that as x increases, y decreases.\n",
            "        \n",
            "        This function also performs a test of the null hypothesis that the\n",
            "        distributions underlying the samples are uncorrelated and normally\n",
            "        distributed. (See Kowalski [3]_\n",
            "        for a discussion of the effects of non-normality of the input on the\n",
            "        distribution of the correlation coefficient.)\n",
            "        The p-value roughly indicates the probability of an uncorrelated system\n",
            "        producing datasets that have a Pearson correlation at least as extreme\n",
            "        as the one computed from these datasets.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : (N,) array_like\n",
            "            Input array.\n",
            "        y : (N,) array_like\n",
            "            Input array.\n",
            "        alternative : {'two-sided', 'greater', 'less'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "        \n",
            "            * 'two-sided': the correlation is nonzero\n",
            "            * 'less': the correlation is negative (less than zero)\n",
            "            * 'greater':  the correlation is positive (greater than zero)\n",
            "        \n",
            "            .. versionadded:: 1.9.0\n",
            "        method : ResamplingMethod, optional\n",
            "            Defines the method used to compute the p-value. If `method` is an\n",
            "            instance of `PermutationMethod`/`MonteCarloMethod`, the p-value is\n",
            "            computed using\n",
            "            `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n",
            "            provided configuration options and other appropriate settings.\n",
            "            Otherwise, the p-value is computed as documented in the notes.\n",
            "        \n",
            "            .. versionadded:: 1.11.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.PearsonRResult`\n",
            "            An object with the following attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                Pearson product-moment correlation coefficient.\n",
            "            pvalue : float\n",
            "                The p-value associated with the chosen alternative.\n",
            "        \n",
            "            The object has the following method:\n",
            "        \n",
            "            confidence_interval(confidence_level, method)\n",
            "                This computes the confidence interval of the correlation\n",
            "                coefficient `statistic` for the given confidence level.\n",
            "                The confidence interval is returned in a ``namedtuple`` with\n",
            "                fields `low` and `high`. If `method` is not provided, the\n",
            "                confidence interval is computed using the Fisher transformation\n",
            "                [1]_. If `method` is an instance of `BootstrapMethod`, the\n",
            "                confidence interval is computed using `scipy.stats.bootstrap` with\n",
            "                the provided configuration options and other appropriate settings.\n",
            "                In some cases, confidence limits may be NaN due to a degenerate\n",
            "                resample, and this is typical for very small samples (~6\n",
            "                observations).\n",
            "        \n",
            "        Warns\n",
            "        -----\n",
            "        `~scipy.stats.ConstantInputWarning`\n",
            "            Raised if an input is a constant array.  The correlation coefficient\n",
            "            is not defined in this case, so ``np.nan`` is returned.\n",
            "        \n",
            "        `~scipy.stats.NearConstantInputWarning`\n",
            "            Raised if an input is \"nearly\" constant.  The array ``x`` is considered\n",
            "            nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.\n",
            "            Numerical errors in the calculation ``x - mean(x)`` in this case might\n",
            "            result in an inaccurate calculation of r.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        spearmanr : Spearman rank-order correlation coefficient.\n",
            "        kendalltau : Kendall's tau, a correlation measure for ordinal data.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The correlation coefficient is calculated as follows:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            r = \\frac{\\sum (x - m_x) (y - m_y)}\n",
            "                     {\\sqrt{\\sum (x - m_x)^2 \\sum (y - m_y)^2}}\n",
            "        \n",
            "        where :math:`m_x` is the mean of the vector x and :math:`m_y` is\n",
            "        the mean of the vector y.\n",
            "        \n",
            "        Under the assumption that x and y are drawn from\n",
            "        independent normal distributions (so the population correlation coefficient\n",
            "        is 0), the probability density function of the sample correlation\n",
            "        coefficient r is ([1]_, [2]_):\n",
            "        \n",
            "        .. math::\n",
            "            f(r) = \\frac{{(1-r^2)}^{n/2-2}}{\\mathrm{B}(\\frac{1}{2},\\frac{n}{2}-1)}\n",
            "        \n",
            "        where n is the number of samples, and B is the beta function.  This\n",
            "        is sometimes referred to as the exact distribution of r.  This is\n",
            "        the distribution that is used in `pearsonr` to compute the p-value when\n",
            "        the `method` parameter is left at its default value (None).\n",
            "        The distribution is a beta distribution on the interval [-1, 1],\n",
            "        with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's\n",
            "        implementation of the beta distribution, the distribution of r is::\n",
            "        \n",
            "            dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
            "        \n",
            "        The default p-value returned by `pearsonr` is a two-sided p-value. For a\n",
            "        given sample with correlation coefficient r, the p-value is\n",
            "        the probability that abs(r') of a random sample x' and y' drawn from\n",
            "        the population with zero correlation would be greater than or equal\n",
            "        to abs(r). In terms of the object ``dist`` shown above, the p-value\n",
            "        for a given r and length n can be computed as::\n",
            "        \n",
            "            p = 2*dist.cdf(-abs(r))\n",
            "        \n",
            "        When n is 2, the above continuous distribution is not well-defined.\n",
            "        One can interpret the limit of the beta distribution as the shape\n",
            "        parameters a and b approach a = b = 0 as a discrete distribution with\n",
            "        equal probability masses at r = 1 and r = -1.  More directly, one\n",
            "        can observe that, given the data x = [x1, x2] and y = [y1, y2], and\n",
            "        assuming x1 != x2 and y1 != y2, the only possible values for r are 1\n",
            "        and -1.  Because abs(r') for any sample x' and y' with length 2 will\n",
            "        be 1, the two-sided p-value for a sample of length 2 is always 1.\n",
            "        \n",
            "        For backwards compatibility, the object that is returned also behaves\n",
            "        like a tuple of length two that holds the statistic and the p-value.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
            "        .. [2] Student, \"Probable error of a correlation coefficient\",\n",
            "               Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.\n",
            "        .. [3] C. J. Kowalski, \"On the Effects of Non-Normality on the Distribution\n",
            "               of the Sample Product-Moment Correlation Coefficient\"\n",
            "               Journal of the Royal Statistical Society. Series C (Applied\n",
            "               Statistics), Vol. 21, No. 1 (1972), pp. 1-12.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x, y = [1, 2, 3, 4, 5, 6, 7], [10, 9, 2.5, 6, 4, 3, 2]\n",
            "        >>> res = stats.pearsonr(x, y)\n",
            "        >>> res\n",
            "        PearsonRResult(statistic=-0.828503883588428, pvalue=0.021280260007523286)\n",
            "        \n",
            "        To perform an exact permutation version of the test:\n",
            "        \n",
            "        >>> rng = np.random.default_rng(7796654889291491997)\n",
            "        >>> method = stats.PermutationMethod(n_resamples=np.inf, random_state=rng)\n",
            "        >>> stats.pearsonr(x, y, method=method)\n",
            "        PearsonRResult(statistic=-0.828503883588428, pvalue=0.028174603174603175)\n",
            "        \n",
            "        To perform the test under the null hypothesis that the data were drawn from\n",
            "        *uniform* distributions:\n",
            "        \n",
            "        >>> method = stats.MonteCarloMethod(rvs=(rng.uniform, rng.uniform))\n",
            "        >>> stats.pearsonr(x, y, method=method)\n",
            "        PearsonRResult(statistic=-0.828503883588428, pvalue=0.0188)\n",
            "        \n",
            "        To produce an asymptotic 90% confidence interval:\n",
            "        \n",
            "        >>> res.confidence_interval(confidence_level=0.9)\n",
            "        ConfidenceInterval(low=-0.9644331982722841, high=-0.3460237473272273)\n",
            "        \n",
            "        And for a bootstrap confidence interval:\n",
            "        \n",
            "        >>> method = stats.BootstrapMethod(method='BCa', random_state=rng)\n",
            "        >>> res.confidence_interval(confidence_level=0.9, method=method)\n",
            "        ConfidenceInterval(low=-0.9983163756488651, high=-0.22771001702132443)  # may vary\n",
            "        \n",
            "        There is a linear dependence between x and y if y = a + b*x + e, where\n",
            "        a,b are constants and e is a random error term, assumed to be independent\n",
            "        of x. For simplicity, assume that x is standard normal, a=0, b=1 and let\n",
            "        e follow a normal distribution with mean zero and standard deviation s>0.\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> s = 0.5\n",
            "        >>> x = stats.norm.rvs(size=500, random_state=rng)\n",
            "        >>> e = stats.norm.rvs(scale=s, size=500, random_state=rng)\n",
            "        >>> y = x + e\n",
            "        >>> stats.pearsonr(x, y).statistic\n",
            "        0.9001942438244763\n",
            "        \n",
            "        This should be close to the exact value given by\n",
            "        \n",
            "        >>> 1/np.sqrt(1 + s**2)\n",
            "        0.8944271909999159\n",
            "        \n",
            "        For s=0.5, we observe a high level of correlation. In general, a large\n",
            "        variance of the noise reduces the correlation, while the correlation\n",
            "        approaches one as the variance of the error goes to zero.\n",
            "        \n",
            "        It is important to keep in mind that no correlation does not imply\n",
            "        independence unless (x, y) is jointly normal. Correlation can even be zero\n",
            "        when there is a very simple dependence structure: if X follows a\n",
            "        standard normal distribution, let y = abs(x). Note that the correlation\n",
            "        between x and y is zero. Indeed, since the expectation of x is zero,\n",
            "        cov(x, y) = E[x*y]. By definition, this equals E[x*abs(x)] which is zero\n",
            "        by symmetry. The following lines of code illustrate this observation:\n",
            "        \n",
            "        >>> y = np.abs(x)\n",
            "        >>> stats.pearsonr(x, y)\n",
            "        PearsonRResult(statistic=-0.05444919272687482, pvalue=0.22422294836207743)\n",
            "        \n",
            "        A non-zero correlation coefficient can be misleading. For example, if X has\n",
            "        a standard normal distribution, define y = x if x < 0 and y = 0 otherwise.\n",
            "        A simple calculation shows that corr(x, y) = sqrt(2/Pi) = 0.797...,\n",
            "        implying a high level of correlation:\n",
            "        \n",
            "        >>> y = np.where(x < 0, x, 0)\n",
            "        >>> stats.pearsonr(x, y)\n",
            "        PearsonRResult(statistic=0.861985781588, pvalue=4.813432002751103e-149)\n",
            "        \n",
            "        This is unintuitive since there is no dependence of x and y if x is larger\n",
            "        than zero which happens in about half of the cases if we sample x and y.\n",
            "    \n",
            "    percentileofscore(a, score, kind='rank', nan_policy='propagate')\n",
            "        Compute the percentile rank of a score relative to a list of scores.\n",
            "        \n",
            "        A `percentileofscore` of, for example, 80% means that 80% of the\n",
            "        scores in `a` are below the given score. In the case of gaps or\n",
            "        ties, the exact definition depends on the optional keyword, `kind`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            A 1-D array to which `score` is compared.\n",
            "        score : array_like\n",
            "            Scores to compute percentiles for.\n",
            "        kind : {'rank', 'weak', 'strict', 'mean'}, optional\n",
            "            Specifies the interpretation of the resulting score.\n",
            "            The following options are available (default is 'rank'):\n",
            "        \n",
            "              * 'rank': Average percentage ranking of score.  In case of multiple\n",
            "                matches, average the percentage rankings of all matching scores.\n",
            "              * 'weak': This kind corresponds to the definition of a cumulative\n",
            "                distribution function.  A percentileofscore of 80% means that 80%\n",
            "                of values are less than or equal to the provided score.\n",
            "              * 'strict': Similar to \"weak\", except that only values that are\n",
            "                strictly less than the given score are counted.\n",
            "              * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n",
            "                in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Specifies how to treat `nan` values in `a`.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "              * 'propagate': returns nan (for each value in `score`).\n",
            "              * 'raise': throws an error\n",
            "              * 'omit': performs the calculations ignoring nan values\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        pcos : float\n",
            "            Percentile-position of score (0-100) relative to `a`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.percentile\n",
            "        scipy.stats.scoreatpercentile, scipy.stats.rankdata\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Three-quarters of the given values lie below a given score:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> stats.percentileofscore([1, 2, 3, 4], 3)\n",
            "        75.0\n",
            "        \n",
            "        With multiple matches, note how the scores of the two matches, 0.6\n",
            "        and 0.8 respectively, are averaged:\n",
            "        \n",
            "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n",
            "        70.0\n",
            "        \n",
            "        Only 2/5 values are strictly less than 3:\n",
            "        \n",
            "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n",
            "        40.0\n",
            "        \n",
            "        But 4/5 values are less than or equal to 3:\n",
            "        \n",
            "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n",
            "        80.0\n",
            "        \n",
            "        The average between the weak and the strict scores is:\n",
            "        \n",
            "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n",
            "        60.0\n",
            "        \n",
            "        Score arrays (of any dimensionality) are supported:\n",
            "        \n",
            "        >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n",
            "        array([40., 70.])\n",
            "        \n",
            "        The inputs can be infinite:\n",
            "        \n",
            "        >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n",
            "        array([75., 75., 100.])\n",
            "        \n",
            "        If `a` is empty, then the resulting percentiles are all `nan`:\n",
            "        \n",
            "        >>> stats.percentileofscore([], [1, 2])\n",
            "        array([nan, nan])\n",
            "    \n",
            "    permutation_test(data, statistic, *, permutation_type='independent', vectorized=None, n_resamples=9999, batch=None, alternative='two-sided', axis=0, random_state=None)\n",
            "        Performs a permutation test of a given statistic on provided data.\n",
            "        \n",
            "        For independent sample statistics, the null hypothesis is that the data are\n",
            "        randomly sampled from the same distribution.\n",
            "        For paired sample statistics, two null hypothesis can be tested:\n",
            "        that the data are paired at random or that the data are assigned to samples\n",
            "        at random.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        data : iterable of array-like\n",
            "            Contains the samples, each of which is an array of observations.\n",
            "            Dimensions of sample arrays must be compatible for broadcasting except\n",
            "            along `axis`.\n",
            "        statistic : callable\n",
            "            Statistic for which the p-value of the hypothesis test is to be\n",
            "            calculated. `statistic` must be a callable that accepts samples\n",
            "            as separate arguments (e.g. ``statistic(*data)``) and returns the\n",
            "            resulting statistic.\n",
            "            If `vectorized` is set ``True``, `statistic` must also accept a keyword\n",
            "            argument `axis` and be vectorized to compute the statistic along the\n",
            "            provided `axis` of the sample arrays.\n",
            "        permutation_type : {'independent', 'samples', 'pairings'}, optional\n",
            "            The type of permutations to be performed, in accordance with the\n",
            "            null hypothesis. The first two permutation types are for paired sample\n",
            "            statistics, in which all samples contain the same number of\n",
            "            observations and observations with corresponding indices along `axis`\n",
            "            are considered to be paired; the third is for independent sample\n",
            "            statistics.\n",
            "        \n",
            "            - ``'samples'`` : observations are assigned to different samples\n",
            "              but remain paired with the same observations from other samples.\n",
            "              This permutation type is appropriate for paired sample hypothesis\n",
            "              tests such as the Wilcoxon signed-rank test and the paired t-test.\n",
            "            - ``'pairings'`` : observations are paired with different observations,\n",
            "              but they remain within the same sample. This permutation type is\n",
            "              appropriate for association/correlation tests with statistics such\n",
            "              as Spearman's :math:`\\rho`, Kendall's :math:`\\tau`, and Pearson's\n",
            "              :math:`r`.\n",
            "            - ``'independent'`` (default) : observations are assigned to different\n",
            "              samples. Samples may contain different numbers of observations. This\n",
            "              permutation type is appropriate for independent sample hypothesis\n",
            "              tests such as the Mann-Whitney :math:`U` test and the independent\n",
            "              sample t-test.\n",
            "        \n",
            "              Please see the Notes section below for more detailed descriptions\n",
            "              of the permutation types.\n",
            "        \n",
            "        vectorized : bool, optional\n",
            "            If `vectorized` is set ``False``, `statistic` will not be passed\n",
            "            keyword argument `axis` and is expected to calculate the statistic\n",
            "            only for 1D samples. If ``True``, `statistic` will be passed keyword\n",
            "            argument `axis` and is expected to calculate the statistic along `axis`\n",
            "            when passed an ND sample array. If ``None`` (default), `vectorized`\n",
            "            will be set ``True`` if ``axis`` is a parameter of `statistic`. Use\n",
            "            of a vectorized statistic typically reduces computation time.\n",
            "        n_resamples : int or np.inf, default: 9999\n",
            "            Number of random permutations (resamples) used to approximate the null\n",
            "            distribution. If greater than or equal to the number of distinct\n",
            "            permutations, the exact null distribution will be computed.\n",
            "            Note that the number of distinct permutations grows very rapidly with\n",
            "            the sizes of samples, so exact tests are feasible only for very small\n",
            "            data sets.\n",
            "        batch : int, optional\n",
            "            The number of permutations to process in each call to `statistic`.\n",
            "            Memory usage is O( `batch` * ``n`` ), where ``n`` is the total size\n",
            "            of all samples, regardless of the value of `vectorized`. Default is\n",
            "            ``None``, in which case ``batch`` is the number of permutations.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            The alternative hypothesis for which the p-value is calculated.\n",
            "            For each alternative, the p-value is defined for exact tests as\n",
            "            follows.\n",
            "        \n",
            "            - ``'greater'`` : the percentage of the null distribution that is\n",
            "              greater than or equal to the observed value of the test statistic.\n",
            "            - ``'less'`` : the percentage of the null distribution that is\n",
            "              less than or equal to the observed value of the test statistic.\n",
            "            - ``'two-sided'`` (default) : twice the smaller of the p-values above.\n",
            "        \n",
            "            Note that p-values for randomized tests are calculated according to the\n",
            "            conservative (over-estimated) approximation suggested in [2]_ and [3]_\n",
            "            rather than the unbiased estimator suggested in [4]_. That is, when\n",
            "            calculating the proportion of the randomized null distribution that is\n",
            "            as extreme as the observed value of the test statistic, the values in\n",
            "            the numerator and denominator are both increased by one. An\n",
            "            interpretation of this adjustment is that the observed value of the\n",
            "            test statistic is always included as an element of the randomized\n",
            "            null distribution.\n",
            "            The convention used for two-sided p-values is not universal;\n",
            "            the observed test statistic and null distribution are returned in\n",
            "            case a different definition is preferred.\n",
            "        \n",
            "        axis : int, default: 0\n",
            "            The axis of the (broadcasted) samples over which to calculate the\n",
            "            statistic. If samples have a different number of dimensions,\n",
            "            singleton dimensions are prepended to samples with fewer dimensions\n",
            "            before `axis` is considered.\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                        `numpy.random.RandomState`}, optional\n",
            "        \n",
            "            Pseudorandom number generator state used to generate permutations.\n",
            "        \n",
            "            If `random_state` is ``None`` (default), the\n",
            "            `numpy.random.RandomState` singleton is used.\n",
            "            If `random_state` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `random_state`.\n",
            "            If `random_state` is already a ``Generator`` or ``RandomState``\n",
            "            instance then that instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : PermutationTestResult\n",
            "            An object with attributes:\n",
            "        \n",
            "            statistic : float or ndarray\n",
            "                The observed test statistic of the data.\n",
            "            pvalue : float or ndarray\n",
            "                The p-value for the given alternative.\n",
            "            null_distribution : ndarray\n",
            "                The values of the test statistic generated under the null\n",
            "                hypothesis.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        The three types of permutation tests supported by this function are\n",
            "        described below.\n",
            "        \n",
            "        **Unpaired statistics** (``permutation_type='independent'``):\n",
            "        \n",
            "        The null hypothesis associated with this permutation type is that all\n",
            "        observations are sampled from the same underlying distribution and that\n",
            "        they have been assigned to one of the samples at random.\n",
            "        \n",
            "        Suppose ``data`` contains two samples; e.g. ``a, b = data``.\n",
            "        When ``1 < n_resamples < binom(n, k)``, where\n",
            "        \n",
            "        * ``k`` is the number of observations in ``a``,\n",
            "        * ``n`` is the total number of observations in ``a`` and ``b``, and\n",
            "        * ``binom(n, k)`` is the binomial coefficient (``n`` choose ``k``),\n",
            "        \n",
            "        the data are pooled (concatenated), randomly assigned to either the first\n",
            "        or second sample, and the statistic is calculated. This process is\n",
            "        performed repeatedly, `permutation` times, generating a distribution of the\n",
            "        statistic under the null hypothesis. The statistic of the original\n",
            "        data is compared to this distribution to determine the p-value.\n",
            "        \n",
            "        When ``n_resamples >= binom(n, k)``, an exact test is performed: the data\n",
            "        are *partitioned* between the samples in each distinct way exactly once,\n",
            "        and the exact null distribution is formed.\n",
            "        Note that for a given partitioning of the data between the samples,\n",
            "        only one ordering/permutation of the data *within* each sample is\n",
            "        considered. For statistics that do not depend on the order of the data\n",
            "        within samples, this dramatically reduces computational cost without\n",
            "        affecting the shape of the null distribution (because the frequency/count\n",
            "        of each value is affected by the same factor).\n",
            "        \n",
            "        For ``a = [a1, a2, a3, a4]`` and ``b = [b1, b2, b3]``, an example of this\n",
            "        permutation type is ``x = [b3, a1, a2, b2]`` and ``y = [a4, b1, a3]``.\n",
            "        Because only one ordering/permutation of the data *within* each sample\n",
            "        is considered in an exact test, a resampling like ``x = [b3, a1, b2, a2]``\n",
            "        and ``y = [a4, a3, b1]`` would *not* be considered distinct from the\n",
            "        example above.\n",
            "        \n",
            "        ``permutation_type='independent'`` does not support one-sample statistics,\n",
            "        but it can be applied to statistics with more than two samples. In this\n",
            "        case, if ``n`` is an array of the number of observations within each\n",
            "        sample, the number of distinct partitions is::\n",
            "        \n",
            "            np.prod([binom(sum(n[i:]), sum(n[i+1:])) for i in range(len(n)-1)])\n",
            "        \n",
            "        **Paired statistics, permute pairings** (``permutation_type='pairings'``):\n",
            "        \n",
            "        The null hypothesis associated with this permutation type is that\n",
            "        observations within each sample are drawn from the same underlying\n",
            "        distribution and that pairings with elements of other samples are\n",
            "        assigned at random.\n",
            "        \n",
            "        Suppose ``data`` contains only one sample; e.g. ``a, = data``, and we\n",
            "        wish to consider all possible pairings of elements of ``a`` with elements\n",
            "        of a second sample, ``b``. Let ``n`` be the number of observations in\n",
            "        ``a``, which must also equal the number of observations in ``b``.\n",
            "        \n",
            "        When ``1 < n_resamples < factorial(n)``, the elements of ``a`` are\n",
            "        randomly permuted. The user-supplied statistic accepts one data argument,\n",
            "        say ``a_perm``, and calculates the statistic considering ``a_perm`` and\n",
            "        ``b``. This process is performed repeatedly, `permutation` times,\n",
            "        generating a distribution of the statistic under the null hypothesis.\n",
            "        The statistic of the original data is compared to this distribution to\n",
            "        determine the p-value.\n",
            "        \n",
            "        When ``n_resamples >= factorial(n)``, an exact test is performed:\n",
            "        ``a`` is permuted in each distinct way exactly once. Therefore, the\n",
            "        `statistic` is computed for each unique pairing of samples between ``a``\n",
            "        and ``b`` exactly once.\n",
            "        \n",
            "        For ``a = [a1, a2, a3]`` and ``b = [b1, b2, b3]``, an example of this\n",
            "        permutation type is ``a_perm = [a3, a1, a2]`` while ``b`` is left\n",
            "        in its original order.\n",
            "        \n",
            "        ``permutation_type='pairings'`` supports ``data`` containing any number\n",
            "        of samples, each of which must contain the same number of observations.\n",
            "        All samples provided in ``data`` are permuted *independently*. Therefore,\n",
            "        if ``m`` is the number of samples and ``n`` is the number of observations\n",
            "        within each sample, then the number of permutations in an exact test is::\n",
            "        \n",
            "            factorial(n)**m\n",
            "        \n",
            "        Note that if a two-sample statistic, for example, does not inherently\n",
            "        depend on the order in which observations are provided - only on the\n",
            "        *pairings* of observations - then only one of the two samples should be\n",
            "        provided in ``data``. This dramatically reduces computational cost without\n",
            "        affecting the shape of the null distribution (because the frequency/count\n",
            "        of each value is affected by the same factor).\n",
            "        \n",
            "        **Paired statistics, permute samples** (``permutation_type='samples'``):\n",
            "        \n",
            "        The null hypothesis associated with this permutation type is that\n",
            "        observations within each pair are drawn from the same underlying\n",
            "        distribution and that the sample to which they are assigned is random.\n",
            "        \n",
            "        Suppose ``data`` contains two samples; e.g. ``a, b = data``.\n",
            "        Let ``n`` be the number of observations in ``a``, which must also equal\n",
            "        the number of observations in ``b``.\n",
            "        \n",
            "        When ``1 < n_resamples < 2**n``, the elements of ``a`` are ``b`` are\n",
            "        randomly swapped between samples (maintaining their pairings) and the\n",
            "        statistic is calculated. This process is performed repeatedly,\n",
            "        `permutation` times,  generating a distribution of the statistic under the\n",
            "        null hypothesis. The statistic of the original data is compared to this\n",
            "        distribution to determine the p-value.\n",
            "        \n",
            "        When ``n_resamples >= 2**n``, an exact test is performed: the observations\n",
            "        are assigned to the two samples in each distinct way (while maintaining\n",
            "        pairings) exactly once.\n",
            "        \n",
            "        For ``a = [a1, a2, a3]`` and ``b = [b1, b2, b3]``, an example of this\n",
            "        permutation type is ``x = [b1, a2, b3]`` and ``y = [a1, b2, a3]``.\n",
            "        \n",
            "        ``permutation_type='samples'`` supports ``data`` containing any number\n",
            "        of samples, each of which must contain the same number of observations.\n",
            "        If ``data`` contains more than one sample, paired observations within\n",
            "        ``data`` are exchanged between samples *independently*. Therefore, if ``m``\n",
            "        is the number of samples and ``n`` is the number of observations within\n",
            "        each sample, then the number of permutations in an exact test is::\n",
            "        \n",
            "            factorial(m)**n\n",
            "        \n",
            "        Several paired-sample statistical tests, such as the Wilcoxon signed rank\n",
            "        test and paired-sample t-test, can be performed considering only the\n",
            "        *difference* between two paired elements. Accordingly, if ``data`` contains\n",
            "        only one sample, then the null distribution is formed by independently\n",
            "        changing the *sign* of each observation.\n",
            "        \n",
            "        .. warning::\n",
            "            The p-value is calculated by counting the elements of the null\n",
            "            distribution that are as extreme or more extreme than the observed\n",
            "            value of the statistic. Due to the use of finite precision arithmetic,\n",
            "            some statistic functions return numerically distinct values when the\n",
            "            theoretical values would be exactly equal. In some cases, this could\n",
            "            lead to a large error in the calculated p-value. `permutation_test`\n",
            "            guards against this by considering elements in the null distribution\n",
            "            that are \"close\" (within a relative tolerance of 100 times the\n",
            "            floating point epsilon of inexact dtypes) to the observed\n",
            "            value of the test statistic as equal to the observed value of the\n",
            "            test statistic. However, the user is advised to inspect the null\n",
            "            distribution to assess whether this method of comparison is\n",
            "            appropriate, and if not, calculate the p-value manually. See example\n",
            "            below.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] R. A. Fisher. The Design of Experiments, 6th Ed (1951).\n",
            "        .. [2] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "           Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n",
            "           Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n",
            "        .. [3] M. D. Ernst. \"Permutation Methods: A Basis for Exact Inference\".\n",
            "           Statistical Science (2004).\n",
            "        .. [4] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap\n",
            "           (1993).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        Suppose we wish to test whether two samples are drawn from the same\n",
            "        distribution. Assume that the underlying distributions are unknown to us,\n",
            "        and that before observing the data, we hypothesized that the mean of the\n",
            "        first sample would be less than that of the second sample. We decide that\n",
            "        we will use the difference between the sample means as a test statistic,\n",
            "        and we will consider a p-value of 0.05 to be statistically significant.\n",
            "        \n",
            "        For efficiency, we write the function defining the test statistic in a\n",
            "        vectorized fashion: the samples ``x`` and ``y`` can be ND arrays, and the\n",
            "        statistic will be calculated for each axis-slice along `axis`.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> def statistic(x, y, axis):\n",
            "        ...     return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n",
            "        \n",
            "        After collecting our data, we calculate the observed value of the test\n",
            "        statistic.\n",
            "        \n",
            "        >>> from scipy.stats import norm\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = norm.rvs(size=5, random_state=rng)\n",
            "        >>> y = norm.rvs(size=6, loc = 3, random_state=rng)\n",
            "        >>> statistic(x, y, 0)\n",
            "        -3.5411688580987266\n",
            "        \n",
            "        Indeed, the test statistic is negative, suggesting that the true mean of\n",
            "        the distribution underlying ``x`` is less than that of the distribution\n",
            "        underlying ``y``. To determine the probability of this occurring by chance\n",
            "        if the two samples were drawn from the same distribution, we perform\n",
            "        a permutation test.\n",
            "        \n",
            "        >>> from scipy.stats import permutation_test\n",
            "        >>> # because our statistic is vectorized, we pass `vectorized=True`\n",
            "        >>> # `n_resamples=np.inf` indicates that an exact test is to be performed\n",
            "        >>> res = permutation_test((x, y), statistic, vectorized=True,\n",
            "        ...                        n_resamples=np.inf, alternative='less')\n",
            "        >>> print(res.statistic)\n",
            "        -3.5411688580987266\n",
            "        >>> print(res.pvalue)\n",
            "        0.004329004329004329\n",
            "        \n",
            "        The probability of obtaining a test statistic less than or equal to the\n",
            "        observed value under the null hypothesis is 0.4329%. This is less than our\n",
            "        chosen threshold of 5%, so we consider this to be significant evidence\n",
            "        against the null hypothesis in favor of the alternative.\n",
            "        \n",
            "        Because the size of the samples above was small, `permutation_test` could\n",
            "        perform an exact test. For larger samples, we resort to a randomized\n",
            "        permutation test.\n",
            "        \n",
            "        >>> x = norm.rvs(size=100, random_state=rng)\n",
            "        >>> y = norm.rvs(size=120, loc=0.3, random_state=rng)\n",
            "        >>> res = permutation_test((x, y), statistic, n_resamples=100000,\n",
            "        ...                        vectorized=True, alternative='less',\n",
            "        ...                        random_state=rng)\n",
            "        >>> print(res.statistic)\n",
            "        -0.5230459671240913\n",
            "        >>> print(res.pvalue)\n",
            "        0.00016999830001699983\n",
            "        \n",
            "        The approximate probability of obtaining a test statistic less than or\n",
            "        equal to the observed value under the null hypothesis is 0.0225%. This is\n",
            "        again less than our chosen threshold of 5%, so again we have significant\n",
            "        evidence to reject the null hypothesis in favor of the alternative.\n",
            "        \n",
            "        For large samples and number of permutations, the result is comparable to\n",
            "        that of the corresponding asymptotic test, the independent sample t-test.\n",
            "        \n",
            "        >>> from scipy.stats import ttest_ind\n",
            "        >>> res_asymptotic = ttest_ind(x, y, alternative='less')\n",
            "        >>> print(res_asymptotic.pvalue)\n",
            "        0.00012688101537979522\n",
            "        \n",
            "        The permutation distribution of the test statistic is provided for\n",
            "        further investigation.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> plt.hist(res.null_distribution, bins=50)\n",
            "        >>> plt.title(\"Permutation distribution of test statistic\")\n",
            "        >>> plt.xlabel(\"Value of Statistic\")\n",
            "        >>> plt.ylabel(\"Frequency\")\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Inspection of the null distribution is essential if the statistic suffers\n",
            "        from inaccuracy due to limited machine precision. Consider the following\n",
            "        case:\n",
            "        \n",
            "        >>> from scipy.stats import pearsonr\n",
            "        >>> x = [1, 2, 4, 3]\n",
            "        >>> y = [2, 4, 6, 8]\n",
            "        >>> def statistic(x, y):\n",
            "        ...     return pearsonr(x, y).statistic\n",
            "        >>> res = permutation_test((x, y), statistic, vectorized=False,\n",
            "        ...                        permutation_type='pairings',\n",
            "        ...                        alternative='greater')\n",
            "        >>> r, pvalue, null = res.statistic, res.pvalue, res.null_distribution\n",
            "        \n",
            "        In this case, some elements of the null distribution differ from the\n",
            "        observed value of the correlation coefficient ``r`` due to numerical noise.\n",
            "        We manually inspect the elements of the null distribution that are nearly\n",
            "        the same as the observed value of the test statistic.\n",
            "        \n",
            "        >>> r\n",
            "        0.8\n",
            "        >>> unique = np.unique(null)\n",
            "        >>> unique\n",
            "        array([-1. , -0.8, -0.8, -0.6, -0.4, -0.2, -0.2,  0. ,  0.2,  0.2,  0.4,\n",
            "                0.6,  0.8,  0.8,  1. ]) # may vary\n",
            "        >>> unique[np.isclose(r, unique)].tolist()\n",
            "        [0.7999999999999999, 0.8]\n",
            "        \n",
            "        If `permutation_test` were to perform the comparison naively, the\n",
            "        elements of the null distribution with value ``0.7999999999999999`` would\n",
            "        not be considered as extreme or more extreme as the observed value of the\n",
            "        statistic, so the calculated p-value would be too small.\n",
            "        \n",
            "        >>> incorrect_pvalue = np.count_nonzero(null >= r) / len(null)\n",
            "        >>> incorrect_pvalue\n",
            "        0.1111111111111111  # may vary\n",
            "        \n",
            "        Instead, `permutation_test` treats elements of the null distribution that\n",
            "        are within ``max(1e-14, abs(r)*1e-14)`` of the observed value of the\n",
            "        statistic ``r`` to be equal to ``r``.\n",
            "        \n",
            "        >>> correct_pvalue = np.count_nonzero(null >= r - 1e-14) / len(null)\n",
            "        >>> correct_pvalue\n",
            "        0.16666666666666666\n",
            "        >>> res.pvalue == correct_pvalue\n",
            "        True\n",
            "        \n",
            "        This method of comparison is expected to be accurate in most practical\n",
            "        situations, but the user is advised to assess this by inspecting the\n",
            "        elements of the null distribution that are close to the observed value\n",
            "        of the statistic. Also, consider the use of statistics that can be\n",
            "        calculated using exact arithmetic (e.g. integer statistics).\n",
            "    \n",
            "    pmean(a, p, *, axis=0, dtype=None, weights=None, nan_policy='propagate', keepdims=False)\n",
            "        Calculate the weighted power mean along the specified axis.\n",
            "        \n",
            "        The weighted power mean of the array :math:`a_i` associated to weights\n",
            "        :math:`w_i` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\left( \\frac{ \\sum_{i=1}^n w_i a_i^p }{ \\sum_{i=1}^n w_i }\n",
            "                  \\right)^{ 1 / p } \\, ,\n",
            "        \n",
            "        and, with equal weights, it gives:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\left( \\frac{ 1 }{ n } \\sum_{i=1}^n a_i^p \\right)^{ 1 / p }  \\, .\n",
            "        \n",
            "        When ``p=0``, it returns the geometric mean.\n",
            "        \n",
            "        This mean is also called generalized mean or Hlder mean, and must not be\n",
            "        confused with the Kolmogorov generalized mean, also called\n",
            "        quasi-arithmetic mean or generalized f-mean [3]_.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array, masked array or object that can be converted to an array.\n",
            "        p : int or float\n",
            "            Exponent.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        dtype : dtype, optional\n",
            "            Type of the returned array and of the accumulator in which the\n",
            "            elements are summed. If `dtype` is not specified, it defaults to the\n",
            "            dtype of `a`, unless `a` has an integer `dtype` with a precision less\n",
            "            than that of the default platform integer. In that case, the default\n",
            "            platform integer is used.\n",
            "        weights : array_like, optional\n",
            "            The weights array can either be 1-D (in which case its length must be\n",
            "            the size of `a` along the given `axis`) or of the same shape as `a`.\n",
            "            Default is None, which gives each value a weight of 1.0.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        pmean : ndarray, see `dtype` parameter above.\n",
            "            Output array containing the power mean values.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`numpy.average`\n",
            "            Weighted average\n",
            "        :func:`gmean`\n",
            "            Geometric mean\n",
            "        :func:`hmean`\n",
            "            Harmonic mean\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The power mean is computed over a single dimension of the input\n",
            "        array, ``axis=0`` by default, or all values in the array if ``axis=None``.\n",
            "        float64 intermediate and return values are used for integer inputs.\n",
            "        \n",
            "        .. versionadded:: 1.9\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Generalized Mean\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Generalized_mean\n",
            "        .. [2] Norris, N., \"Convexity properties of generalized mean value\n",
            "               functions\", The Annals of Mathematical Statistics, vol. 8,\n",
            "               pp. 118-120, 1937\n",
            "        .. [3] Bullen, P.S., Handbook of Means and Their Inequalities, 2003\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import pmean, hmean, gmean\n",
            "        >>> pmean([1, 4], 1.3)\n",
            "        2.639372938300652\n",
            "        >>> pmean([1, 2, 3, 4, 5, 6, 7], 1.3)\n",
            "        4.157111214492084\n",
            "        >>> pmean([1, 4, 7], -2, weights=[3, 1, 3])\n",
            "        1.4969684896631954\n",
            "        \n",
            "        For p=-1, power mean is equal to harmonic mean:\n",
            "        \n",
            "        >>> pmean([1, 4, 7], -1, weights=[3, 1, 3])\n",
            "        1.9029126213592233\n",
            "        >>> hmean([1, 4, 7], weights=[3, 1, 3])\n",
            "        1.9029126213592233\n",
            "        \n",
            "        For p=0, power mean is defined as the geometric mean:\n",
            "        \n",
            "        >>> pmean([1, 4, 7], 0, weights=[3, 1, 3])\n",
            "        2.80668351922014\n",
            "        >>> gmean([1, 4, 7], weights=[3, 1, 3])\n",
            "        2.80668351922014\n",
            "    \n",
            "    pointbiserialr(x, y)\n",
            "        Calculate a point biserial correlation coefficient and its p-value.\n",
            "        \n",
            "        The point biserial correlation is used to measure the relationship\n",
            "        between a binary variable, x, and a continuous variable, y. Like other\n",
            "        correlation coefficients, this one varies between -1 and +1 with 0\n",
            "        implying no correlation. Correlations of -1 or +1 imply a determinative\n",
            "        relationship.\n",
            "        \n",
            "        This function may be computed using a shortcut formula but produces the\n",
            "        same result as `pearsonr`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like of bools\n",
            "            Input array.\n",
            "        y : array_like\n",
            "            Input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: SignificanceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                The R value.\n",
            "            pvalue : float\n",
            "                The two-sided p-value.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.\n",
            "        It is equivalent to `pearsonr`.\n",
            "        \n",
            "        The value of the point-biserial correlation can be calculated from:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            r_{pb} = \\frac{\\overline{Y_1} - \\overline{Y_0}}\n",
            "                          {s_y}\n",
            "                     \\sqrt{\\frac{N_0 N_1}\n",
            "                                {N (N - 1)}}\n",
            "        \n",
            "        Where :math:`\\overline{Y_{0}}` and :math:`\\overline{Y_{1}}` are means\n",
            "        of the metric observations coded 0 and 1 respectively; :math:`N_{0}` and\n",
            "        :math:`N_{1}` are number of observations coded 0 and 1 respectively;\n",
            "        :math:`N` is the total number of observations and :math:`s_{y}` is the\n",
            "        standard deviation of all the metric observations.\n",
            "        \n",
            "        A value of :math:`r_{pb}` that is significantly different from zero is\n",
            "        completely equivalent to a significant difference in means between the two\n",
            "        groups. Thus, an independent groups t Test with :math:`N-2` degrees of\n",
            "        freedom may be used to test whether :math:`r_{pb}` is nonzero. The\n",
            "        relation between the t-statistic for comparing two independent groups and\n",
            "        :math:`r_{pb}` is given by:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            t = \\sqrt{N - 2}\\frac{r_{pb}}{\\sqrt{1 - r^{2}_{pb}}}\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] J. Lev, \"The Point Biserial Coefficient of Correlation\", Ann. Math.\n",
            "               Statist., Vol. 20, no.1, pp. 125-126, 1949.\n",
            "        \n",
            "        .. [2] R.F. Tate, \"Correlation Between a Discrete and a Continuous\n",
            "               Variable. Point-Biserial Correlation.\", Ann. Math. Statist., Vol. 25,\n",
            "               np. 3, pp. 603-607, 1954.\n",
            "        \n",
            "        .. [3] D. Kornbrot \"Point Biserial Correlation\", In Wiley StatsRef:\n",
            "               Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.\n",
            "               :doi:`10.1002/9781118445112.stat06227`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
            "        >>> b = np.arange(7)\n",
            "        >>> stats.pointbiserialr(a, b)\n",
            "        (0.8660254037844386, 0.011724811003954652)\n",
            "        >>> stats.pearsonr(a, b)\n",
            "        (0.86602540378443871, 0.011724811003954626)\n",
            "        >>> np.corrcoef(a, b)\n",
            "        array([[ 1.       ,  0.8660254],\n",
            "               [ 0.8660254,  1.       ]])\n",
            "    \n",
            "    poisson_means_test(k1, n1, k2, n2, *, diff=0, alternative='two-sided')\n",
            "        Performs the Poisson means test, AKA the \"E-test\".\n",
            "        \n",
            "        This is a test of the null hypothesis that the difference between means of\n",
            "        two Poisson distributions is `diff`. The samples are provided as the\n",
            "        number of events `k1` and `k2` observed within measurement intervals\n",
            "        (e.g. of time, space, number of observations) of sizes `n1` and `n2`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        k1 : int\n",
            "            Number of events observed from distribution 1.\n",
            "        n1: float\n",
            "            Size of sample from distribution 1.\n",
            "        k2 : int\n",
            "            Number of events observed from distribution 2.\n",
            "        n2 : float\n",
            "            Size of sample from distribution 2.\n",
            "        diff : float, default=0\n",
            "            The hypothesized difference in means between the distributions\n",
            "            underlying the samples.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "        \n",
            "              * 'two-sided': the difference between distribution means is not\n",
            "                equal to `diff`\n",
            "              * 'less': the difference between distribution means is less than\n",
            "                `diff`\n",
            "              * 'greater': the difference between distribution means is greater\n",
            "                than `diff`\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic (see [1]_ equation 3.3).\n",
            "        pvalue : float\n",
            "            The probability of achieving such an extreme value of the test\n",
            "            statistic under the null hypothesis.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Let:\n",
            "        \n",
            "        .. math:: X_1 \\sim \\mbox{Poisson}(\\mathtt{n1}\\lambda_1)\n",
            "        \n",
            "        be a random variable independent of\n",
            "        \n",
            "        .. math:: X_2  \\sim \\mbox{Poisson}(\\mathtt{n2}\\lambda_2)\n",
            "        \n",
            "        and let ``k1`` and ``k2`` be the observed values of :math:`X_1`\n",
            "        and :math:`X_2`, respectively. Then `poisson_means_test` uses the number\n",
            "        of observed events ``k1`` and ``k2`` from samples of size ``n1`` and\n",
            "        ``n2``, respectively, to test the null hypothesis that\n",
            "        \n",
            "        .. math::\n",
            "           H_0: \\lambda_1 - \\lambda_2 = \\mathtt{diff}\n",
            "        \n",
            "        A benefit of the E-test is that it has good power for small sample sizes,\n",
            "        which can reduce sampling costs [1]_. It has been evaluated and determined\n",
            "        to be more powerful than the comparable C-test, sometimes referred to as\n",
            "        the Poisson exact test.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1]  Krishnamoorthy, K., & Thomson, J. (2004). A more powerful test for\n",
            "           comparing two Poisson means. Journal of Statistical Planning and\n",
            "           Inference, 119(1), 23-35.\n",
            "        \n",
            "        .. [2]  Przyborowski, J., & Wilenski, H. (1940). Homogeneity of results in\n",
            "           testing samples from Poisson series: With an application to testing\n",
            "           clover seed for dodder. Biometrika, 31(3/4), 313-323.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        Suppose that a gardener wishes to test the number of dodder (weed) seeds\n",
            "        in a sack of clover seeds that they buy from a seed company. It has\n",
            "        previously been established that the number of dodder seeds in clover\n",
            "        follows the Poisson distribution.\n",
            "        \n",
            "        A 100 gram sample is drawn from the sack before being shipped to the\n",
            "        gardener. The sample is analyzed, and it is found to contain no dodder\n",
            "        seeds; that is, `k1` is 0. However, upon arrival, the gardener draws\n",
            "        another 100 gram sample from the sack. This time, three dodder seeds are\n",
            "        found in the sample; that is, `k2` is 3. The gardener would like to\n",
            "        know if the difference is significant and not due to chance. The\n",
            "        null hypothesis is that the difference between the two samples is merely\n",
            "        due to chance, or that :math:`\\lambda_1 - \\lambda_2 = \\mathtt{diff}`\n",
            "        where :math:`\\mathtt{diff} = 0`. The alternative hypothesis is that the\n",
            "        difference is not due to chance, or :math:`\\lambda_1 - \\lambda_2 \\ne 0`.\n",
            "        The gardener selects a significance level of 5% to reject the null\n",
            "        hypothesis in favor of the alternative [2]_.\n",
            "        \n",
            "        >>> import scipy.stats as stats\n",
            "        >>> res = stats.poisson_means_test(0, 100, 3, 100)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (-1.7320508075688772, 0.08837900929018157)\n",
            "        \n",
            "        The p-value is .088, indicating a near 9% chance of observing a value of\n",
            "        the test statistic under the null hypothesis. This exceeds 5%, so the\n",
            "        gardener does not reject the null hypothesis as the difference cannot be\n",
            "        regarded as significant at this level.\n",
            "    \n",
            "    power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None)\n",
            "        Cressie-Read power divergence statistic and goodness of fit test.\n",
            "        \n",
            "        This function tests the null hypothesis that the categorical data\n",
            "        has the given frequencies, using the Cressie-Read power divergence\n",
            "        statistic.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        f_obs : array_like\n",
            "            Observed frequencies in each category.\n",
            "        f_exp : array_like, optional\n",
            "            Expected frequencies in each category.  By default the categories are\n",
            "            assumed to be equally likely.\n",
            "        ddof : int, optional\n",
            "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
            "            for the p-value.  The p-value is computed using a chi-squared\n",
            "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
            "            is the number of observed frequencies.  The default value of `ddof`\n",
            "            is 0.\n",
            "        axis : int or None, optional\n",
            "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
            "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
            "            as a single data set.  Default is 0.\n",
            "        lambda_ : float or str, optional\n",
            "            The power in the Cressie-Read power divergence statistic.  The default\n",
            "            is 1.  For convenience, `lambda_` may be assigned one of the following\n",
            "            strings, in which case the corresponding numerical value is used:\n",
            "        \n",
            "            * ``\"pearson\"`` (value 1)\n",
            "                Pearson's chi-squared statistic. In this case, the function is\n",
            "                equivalent to `chisquare`.\n",
            "            * ``\"log-likelihood\"`` (value 0)\n",
            "                Log-likelihood ratio. Also known as the G-test [3]_.\n",
            "            * ``\"freeman-tukey\"`` (value -1/2)\n",
            "                Freeman-Tukey statistic.\n",
            "            * ``\"mod-log-likelihood\"`` (value -1)\n",
            "                Modified log-likelihood ratio.\n",
            "            * ``\"neyman\"`` (value -2)\n",
            "                Neyman's statistic.\n",
            "            * ``\"cressie-read\"`` (value 2/3)\n",
            "                The power recommended in [5]_.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: Power_divergenceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float or ndarray\n",
            "                The Cressie-Read power divergence test statistic.  The value is\n",
            "                a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n",
            "            pvalue : float or ndarray\n",
            "                The p-value of the test.  The value is a float if `ddof` and the\n",
            "                return value `stat` are scalars.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        chisquare\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This test is invalid when the observed or expected frequencies in each\n",
            "        category are too small.  A typical rule is that all of the observed\n",
            "        and expected frequencies should be at least 5.\n",
            "        \n",
            "        Also, the sum of the observed and expected frequencies must be the same\n",
            "        for the test to be valid; `power_divergence` raises an error if the sums\n",
            "        do not agree within a relative tolerance of ``1e-8``.\n",
            "        \n",
            "        When `lambda_` is less than zero, the formula for the statistic involves\n",
            "        dividing by `f_obs`, so a warning or error may be generated if any value\n",
            "        in `f_obs` is 0.\n",
            "        \n",
            "        Similarly, a warning or error may be generated if any value in `f_exp` is\n",
            "        zero when `lambda_` >= 0.\n",
            "        \n",
            "        The default degrees of freedom, k-1, are for the case when no parameters\n",
            "        of the distribution are estimated. If p parameters are estimated by\n",
            "        efficient maximum likelihood then the correct degrees of freedom are\n",
            "        k-1-p. If the parameters are estimated in a different way, then the\n",
            "        dof can be between k-1-p and k-1. However, it is also possible that\n",
            "        the asymptotic distribution is not a chisquare, in which case this\n",
            "        test is not appropriate.\n",
            "        \n",
            "        This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n",
            "        is masked, then data at that position is ignored, and does not count\n",
            "        towards the size of the data set.\n",
            "        \n",
            "        .. versionadded:: 0.13.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
            "               Statistics\". Chapter 8.\n",
            "               https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n",
            "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
            "        .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n",
            "        .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n",
            "               practice of statistics in biological research\", New York: Freeman\n",
            "               (1981)\n",
            "        .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
            "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
            "               pp. 440-464.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        (See `chisquare` for more examples.)\n",
            "        \n",
            "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
            "        are uniform and given by the mean of the observed frequencies.  Here we\n",
            "        perform a G-test (i.e. use the log-likelihood ratio statistic):\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import power_divergence\n",
            "        >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n",
            "        (2.006573162632538, 0.84823476779463769)\n",
            "        \n",
            "        The expected frequencies can be given with the `f_exp` argument:\n",
            "        \n",
            "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
            "        ...                  f_exp=[16, 16, 16, 16, 16, 8],\n",
            "        ...                  lambda_='log-likelihood')\n",
            "        (3.3281031458963746, 0.6495419288047497)\n",
            "        \n",
            "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
            "        \n",
            "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
            "        >>> obs.shape\n",
            "        (6, 2)\n",
            "        >>> power_divergence(obs, lambda_=\"log-likelihood\")\n",
            "        (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n",
            "        \n",
            "        By setting ``axis=None``, the test is applied to all data in the array,\n",
            "        which is equivalent to applying the test to the flattened array.\n",
            "        \n",
            "        >>> power_divergence(obs, axis=None)\n",
            "        (23.31034482758621, 0.015975692534127565)\n",
            "        >>> power_divergence(obs.ravel())\n",
            "        (23.31034482758621, 0.015975692534127565)\n",
            "        \n",
            "        `ddof` is the change to make to the default degrees of freedom.\n",
            "        \n",
            "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n",
            "        (2.0, 0.73575888234288467)\n",
            "        \n",
            "        The calculation of the p-values is done by broadcasting the\n",
            "        test statistic with `ddof`.\n",
            "        \n",
            "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
            "        (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n",
            "        \n",
            "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
            "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
            "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
            "        statistics, we must use ``axis=1``:\n",
            "        \n",
            "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
            "        ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n",
            "        ...                         [8, 20, 20, 16, 12, 12]],\n",
            "        ...                  axis=1)\n",
            "        (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n",
            "    \n",
            "    ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda')\n",
            "        Calculate the shape parameter that maximizes the PPCC.\n",
            "        \n",
            "        The probability plot correlation coefficient (PPCC) plot can be used\n",
            "        to determine the optimal shape parameter for a one-parameter family\n",
            "        of distributions. ``ppcc_max`` returns the shape parameter that would\n",
            "        maximize the probability plot correlation coefficient for the given\n",
            "        data to a one-parameter family of distributions.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array.\n",
            "        brack : tuple, optional\n",
            "            Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n",
            "            then they are assumed to be a starting interval for a downhill bracket\n",
            "            search (see `scipy.optimize.brent`).\n",
            "        dist : str or stats.distributions instance, optional\n",
            "            Distribution or distribution function name.  Objects that look enough\n",
            "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
            "            are also accepted.  The default is ``'tukeylambda'``.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        shape_value : float\n",
            "            The shape parameter at which the probability plot correlation\n",
            "            coefficient reaches its max value.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ppcc_plot, probplot, boxcox\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The brack keyword serves as a starting point which is useful in corner\n",
            "        cases. One can use a plot to obtain a rough visual estimate of the location\n",
            "        for the maximum to start the search near it.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test\n",
            "               for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
            "        .. [2] Engineering Statistics Handbook, NIST/SEMATEC,\n",
            "               https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        First we generate some random data from a Weibull distribution\n",
            "        with shape parameter 2.5:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> c = 2.5\n",
            "        >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n",
            "        \n",
            "        Generate the PPCC plot for this data with the Weibull distribution.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 6))\n",
            "        >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax)\n",
            "        \n",
            "        We calculate the value where the shape should reach its maximum and a\n",
            "        red line is drawn there. The line should coincide with the highest\n",
            "        point in the PPCC graph.\n",
            "        \n",
            "        >>> cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min')\n",
            "        >>> ax.axvline(cmax, color='r')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    ppcc_plot(x, a, b, dist='tukeylambda', plot=None, N=80)\n",
            "        Calculate and optionally plot probability plot correlation coefficient.\n",
            "        \n",
            "        The probability plot correlation coefficient (PPCC) plot can be used to\n",
            "        determine the optimal shape parameter for a one-parameter family of\n",
            "        distributions.  It cannot be used for distributions without shape\n",
            "        parameters\n",
            "        (like the normal distribution) or with multiple shape parameters.\n",
            "        \n",
            "        By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n",
            "        Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n",
            "        distributions via an approximately normal one, and is therefore\n",
            "        particularly useful in practice.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array.\n",
            "        a, b : scalar\n",
            "            Lower and upper bounds of the shape parameter to use.\n",
            "        dist : str or stats.distributions instance, optional\n",
            "            Distribution or distribution function name.  Objects that look enough\n",
            "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
            "            are also accepted.  The default is ``'tukeylambda'``.\n",
            "        plot : object, optional\n",
            "            If given, plots PPCC against the shape parameter.\n",
            "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
            "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
            "            or a custom object with the same methods.\n",
            "            Default is None, which means that no plot is created.\n",
            "        N : int, optional\n",
            "            Number of points on the horizontal axis (equally distributed from\n",
            "            `a` to `b`).\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        svals : ndarray\n",
            "            The shape values for which `ppcc` was calculated.\n",
            "        ppcc : ndarray\n",
            "            The calculated probability plot correlation coefficient values.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ppcc_max, probplot, boxcox_normplot, tukeylambda\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n",
            "        Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        First we generate some random data from a Weibull distribution\n",
            "        with shape parameter 2.5, and plot the histogram of the data:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> c = 2.5\n",
            "        >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n",
            "        \n",
            "        Take a look at the histogram of the data.\n",
            "        \n",
            "        >>> fig1, ax = plt.subplots(figsize=(9, 4))\n",
            "        >>> ax.hist(x, bins=50)\n",
            "        >>> ax.set_title('Histogram of x')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Now we explore this data with a PPCC plot as well as the related\n",
            "        probability plot and Box-Cox normplot.  A red line is drawn where we\n",
            "        expect the PPCC value to be maximal (at the shape parameter ``c``\n",
            "        used above):\n",
            "        \n",
            "        >>> fig2 = plt.figure(figsize=(12, 4))\n",
            "        >>> ax1 = fig2.add_subplot(1, 3, 1)\n",
            "        >>> ax2 = fig2.add_subplot(1, 3, 2)\n",
            "        >>> ax3 = fig2.add_subplot(1, 3, 3)\n",
            "        >>> res = stats.probplot(x, plot=ax1)\n",
            "        >>> res = stats.boxcox_normplot(x, -4, 4, plot=ax2)\n",
            "        >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3)\n",
            "        >>> ax3.axvline(c, color='r')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False)\n",
            "        Calculate quantiles for a probability plot, and optionally show the plot.\n",
            "        \n",
            "        Generates a probability plot of sample data against the quantiles of a\n",
            "        specified theoretical distribution (the normal distribution by default).\n",
            "        `probplot` optionally calculates a best-fit line for the data and plots the\n",
            "        results using Matplotlib or a given plot function.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Sample/response data from which `probplot` creates the plot.\n",
            "        sparams : tuple, optional\n",
            "            Distribution-specific shape parameters (shape parameters plus location\n",
            "            and scale).\n",
            "        dist : str or stats.distributions instance, optional\n",
            "            Distribution or distribution function name. The default is 'norm' for a\n",
            "            normal probability plot.  Objects that look enough like a\n",
            "            stats.distributions instance (i.e. they have a ``ppf`` method) are also\n",
            "            accepted.\n",
            "        fit : bool, optional\n",
            "            Fit a least-squares regression (best-fit) line to the sample data if\n",
            "            True (default).\n",
            "        plot : object, optional\n",
            "            If given, plots the quantiles.\n",
            "            If given and `fit` is True, also plots the least squares fit.\n",
            "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
            "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
            "            or a custom object with the same methods.\n",
            "            Default is None, which means that no plot is created.\n",
            "        rvalue : bool, optional\n",
            "            If `plot` is provided and `fit` is True, setting `rvalue` to True\n",
            "            includes the coefficient of determination on the plot.\n",
            "            Default is False.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        (osm, osr) : tuple of ndarrays\n",
            "            Tuple of theoretical quantiles (osm, or order statistic medians) and\n",
            "            ordered responses (osr).  `osr` is simply sorted input `x`.\n",
            "            For details on how `osm` is calculated see the Notes section.\n",
            "        (slope, intercept, r) : tuple of floats, optional\n",
            "            Tuple  containing the result of the least-squares fit, if that is\n",
            "            performed by `probplot`. `r` is the square root of the coefficient of\n",
            "            determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n",
            "            returned.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Even if `plot` is given, the figure is not shown or saved by `probplot`;\n",
            "        ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n",
            "        calling `probplot`.\n",
            "        \n",
            "        `probplot` generates a probability plot, which should not be confused with\n",
            "        a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n",
            "        type, see ``statsmodels.api.ProbPlot``.\n",
            "        \n",
            "        The formula used for the theoretical quantiles (horizontal axis of the\n",
            "        probability plot) is Filliben's estimate::\n",
            "        \n",
            "            quantiles = dist.ppf(val), for\n",
            "        \n",
            "                    0.5**(1/n),                  for i = n\n",
            "              val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n",
            "                    1 - 0.5**(1/n),              for i = 1\n",
            "        \n",
            "        where ``i`` indicates the i-th ordered value and ``n`` is the total number\n",
            "        of values.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> nsample = 100\n",
            "        >>> rng = np.random.default_rng()\n",
            "        \n",
            "        A t distribution with small degrees of freedom:\n",
            "        \n",
            "        >>> ax1 = plt.subplot(221)\n",
            "        >>> x = stats.t.rvs(3, size=nsample, random_state=rng)\n",
            "        >>> res = stats.probplot(x, plot=plt)\n",
            "        \n",
            "        A t distribution with larger degrees of freedom:\n",
            "        \n",
            "        >>> ax2 = plt.subplot(222)\n",
            "        >>> x = stats.t.rvs(25, size=nsample, random_state=rng)\n",
            "        >>> res = stats.probplot(x, plot=plt)\n",
            "        \n",
            "        A mixture of two normal distributions with broadcasting:\n",
            "        \n",
            "        >>> ax3 = plt.subplot(223)\n",
            "        >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n",
            "        ...                    size=(nsample//2,2), random_state=rng).ravel()\n",
            "        >>> res = stats.probplot(x, plot=plt)\n",
            "        \n",
            "        A standard normal distribution:\n",
            "        \n",
            "        >>> ax4 = plt.subplot(224)\n",
            "        >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng)\n",
            "        >>> res = stats.probplot(x, plot=plt)\n",
            "        \n",
            "        Produce a new figure with a loggamma distribution, using the ``dist`` and\n",
            "        ``sparams`` keywords:\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng)\n",
            "        >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n",
            "        >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n",
            "        \n",
            "        Show the results with Matplotlib:\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    quantile_test(x, *, q=0, p=0.5, alternative='two-sided')\n",
            "        Perform a quantile test and compute a confidence interval of the quantile.\n",
            "        \n",
            "        This function tests the null hypothesis that `q` is the value of the\n",
            "        quantile associated with probability `p` of the population underlying\n",
            "        sample `x`. For example, with default parameters, it tests that the\n",
            "        median of the population underlying `x` is zero. The function returns an\n",
            "        object including the test statistic, a p-value, and a method for computing\n",
            "        the confidence interval around the quantile.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            A one-dimensional sample.\n",
            "        q : float, default: 0\n",
            "            The hypothesized value of the quantile.\n",
            "        p : float, default: 0.5\n",
            "            The probability associated with the quantile; i.e. the proportion of\n",
            "            the population less than `q` is `p`. Must be strictly between 0 and\n",
            "            1.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "        \n",
            "            * 'two-sided': the quantile associated with the probability `p`\n",
            "              is not `q`.\n",
            "            * 'less': the quantile associated with the probability `p` is less\n",
            "              than `q`.\n",
            "            * 'greater': the quantile associated with the probability `p` is\n",
            "              greater than `q`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : QuantileTestResult\n",
            "            An object with the following attributes:\n",
            "        \n",
            "            statistic : float\n",
            "                One of two test statistics that may be used in the quantile test.\n",
            "                The first test statistic, ``T1``, is the proportion of samples in\n",
            "                `x` that are less than or equal to the hypothesized quantile\n",
            "                `q`. The second test statistic, ``T2``, is the proportion of\n",
            "                samples in `x` that are strictly less than the hypothesized\n",
            "                quantile `q`.\n",
            "        \n",
            "                When ``alternative = 'greater'``, ``T1`` is used to calculate the\n",
            "                p-value and ``statistic`` is set to ``T1``.\n",
            "        \n",
            "                When ``alternative = 'less'``, ``T2`` is used to calculate the\n",
            "                p-value and ``statistic`` is set to ``T2``.\n",
            "        \n",
            "                When ``alternative = 'two-sided'``, both ``T1`` and ``T2`` are\n",
            "                considered, and the one that leads to the smallest p-value is used.\n",
            "        \n",
            "            statistic_type : int\n",
            "                Either `1` or `2` depending on which of ``T1`` or ``T2`` was\n",
            "                used to calculate the p-value.\n",
            "        \n",
            "            pvalue : float\n",
            "                The p-value associated with the given alternative.\n",
            "        \n",
            "            The object also has the following method:\n",
            "        \n",
            "            confidence_interval(confidence_level=0.95)\n",
            "                Computes a confidence interval around the the\n",
            "                population quantile associated with the probability `p`. The\n",
            "                confidence interval is returned in a ``namedtuple`` with\n",
            "                fields `low` and `high`.  Values are `nan` when there are\n",
            "                not enough observations to compute the confidence interval at\n",
            "                the desired confidence.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This test and its method for computing confidence intervals are\n",
            "        non-parametric. They are valid if and only if the observations are i.i.d.\n",
            "        \n",
            "        The implementation of the test follows Conover [1]_. Two test statistics\n",
            "        are considered.\n",
            "        \n",
            "        ``T1``: The number of observations in `x` less than or equal to `q`.\n",
            "        \n",
            "            ``T1 = (x <= q).sum()``\n",
            "        \n",
            "        ``T2``: The number of observations in `x` strictly less than `q`.\n",
            "        \n",
            "            ``T2 = (x < q).sum()``\n",
            "        \n",
            "        The use of two test statistics is necessary to handle the possibility that\n",
            "        `x` was generated from a discrete or mixed distribution.\n",
            "        \n",
            "        The null hypothesis for the test is:\n",
            "        \n",
            "            H0: The :math:`p^{\\mathrm{th}}` population quantile is `q`.\n",
            "        \n",
            "        and the null distribution for each test statistic is\n",
            "        :math:`\\mathrm{binom}\\left(n, p\\right)`. When ``alternative='less'``,\n",
            "        the alternative hypothesis is:\n",
            "        \n",
            "            H1: The :math:`p^{\\mathrm{th}}` population quantile is less than `q`.\n",
            "        \n",
            "        and the p-value is the probability that the binomial random variable\n",
            "        \n",
            "        .. math::\n",
            "            Y \\sim \\mathrm{binom}\\left(n, p\\right)\n",
            "        \n",
            "        is greater than or equal to the observed value ``T2``.\n",
            "        \n",
            "        When ``alternative='greater'``, the alternative hypothesis is:\n",
            "        \n",
            "            H1: The :math:`p^{\\mathrm{th}}` population quantile is greater than `q`\n",
            "        \n",
            "        and the p-value is the probability that the binomial random variable Y\n",
            "        is less than or equal to the observed value ``T1``.\n",
            "        \n",
            "        When ``alternative='two-sided'``, the alternative hypothesis is\n",
            "        \n",
            "            H1: `q` is not the :math:`p^{\\mathrm{th}}` population quantile.\n",
            "        \n",
            "        and the p-value is twice the smaller of the p-values for the ``'less'``\n",
            "        and ``'greater'`` cases. Both of these p-values can exceed 0.5 for the same\n",
            "        data, so the value is clipped into the interval :math:`[0, 1]`.\n",
            "        \n",
            "        The approach for confidence intervals is attributed to Thompson [2]_ and\n",
            "        later proven to be applicable to any set of i.i.d. samples [3]_. The\n",
            "        computation is based on the observation that the probability of a quantile\n",
            "        :math:`q` to be larger than any observations :math:`x_m (1\\leq m \\leq N)`\n",
            "        can be computed as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\mathbb{P}(x_m \\leq q) = 1 - \\sum_{k=0}^{m-1} \\binom{N}{k}\n",
            "            q^k(1-q)^{N-k}\n",
            "        \n",
            "        By default, confidence intervals are computed for a 95% confidence level.\n",
            "        A common interpretation of a 95% confidence intervals is that if i.i.d.\n",
            "        samples are drawn repeatedly from the same population and confidence\n",
            "        intervals are formed each time, the confidence interval will contain the\n",
            "        true value of the specified quantile in approximately 95% of trials.\n",
            "        \n",
            "        A similar function is available in the QuantileNPCI R package [4]_. The\n",
            "        foundation is the same, but it computes the confidence interval bounds by\n",
            "        doing interpolations between the sample values, whereas this function uses\n",
            "        only sample values as bounds. Thus, ``quantile_test.confidence_interval``\n",
            "        returns more conservative intervals (i.e., larger).\n",
            "        \n",
            "        The same computation of confidence intervals for quantiles is included in\n",
            "        the confintr package [5]_.\n",
            "        \n",
            "        Two-sided confidence intervals are not guaranteed to be optimal; i.e.,\n",
            "        there may exist a tighter interval that may contain the quantile of\n",
            "        interest with probability larger than the confidence level.\n",
            "        Without further assumption on the samples (e.g., the nature of the\n",
            "        underlying distribution), the one-sided intervals are optimally tight.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] W. J. Conover. Practical Nonparametric Statistics, 3rd Ed. 1999.\n",
            "        .. [2] W. R. Thompson, \"On Confidence Ranges for the Median and Other\n",
            "           Expectation Distributions for Populations of Unknown Distribution\n",
            "           Form,\" The Annals of Mathematical Statistics, vol. 7, no. 3,\n",
            "           pp. 122-128, 1936, Accessed: Sep. 18, 2019. [Online]. Available:\n",
            "           https://www.jstor.org/stable/2957563.\n",
            "        .. [3] H. A. David and H. N. Nagaraja, \"Order Statistics in Nonparametric\n",
            "           Inference\" in Order Statistics, John Wiley & Sons, Ltd, 2005, pp.\n",
            "           159-170. Available:\n",
            "           https://onlinelibrary.wiley.com/doi/10.1002/0471722162.ch7.\n",
            "        .. [4] N. Hutson, A. Hutson, L. Yan, \"QuantileNPCI: Nonparametric\n",
            "           Confidence Intervals for Quantiles,\" R package,\n",
            "           https://cran.r-project.org/package=QuantileNPCI\n",
            "        .. [5] M. Mayer, \"confintr: Confidence Intervals,\" R package,\n",
            "           https://cran.r-project.org/package=confintr\n",
            "        \n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        Suppose we wish to test the null hypothesis that the median of a population\n",
            "        is equal to 0.5. We choose a confidence level of 99%; that is, we will\n",
            "        reject the null hypothesis in favor of the alternative if the p-value is\n",
            "        less than 0.01.\n",
            "        \n",
            "        When testing random variates from the standard uniform distribution, which\n",
            "        has a median of 0.5, we expect the data to be consistent with the null\n",
            "        hypothesis most of the time.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng(6981396440634228121)\n",
            "        >>> rvs = stats.uniform.rvs(size=100, random_state=rng)\n",
            "        >>> stats.quantile_test(rvs, q=0.5, p=0.5)\n",
            "        QuantileTestResult(statistic=45, statistic_type=1, pvalue=0.36820161732669576)\n",
            "        \n",
            "        As expected, the p-value is not below our threshold of 0.01, so\n",
            "        we cannot reject the null hypothesis.\n",
            "        \n",
            "        When testing data from the standard *normal* distribution, which has a\n",
            "        median of 0, we would expect the null hypothesis to be rejected.\n",
            "        \n",
            "        >>> rvs = stats.norm.rvs(size=100, random_state=rng)\n",
            "        >>> stats.quantile_test(rvs, q=0.5, p=0.5)\n",
            "        QuantileTestResult(statistic=67, statistic_type=2, pvalue=0.0008737198369123724)\n",
            "        \n",
            "        Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n",
            "        null hypothesis in favor of the default \"two-sided\" alternative: the median\n",
            "        of the population is *not* equal to 0.5.\n",
            "        \n",
            "        However, suppose we were to test the null hypothesis against the\n",
            "        one-sided alternative that the median of the population is *greater* than\n",
            "        0.5. Since the median of the standard normal is less than 0.5, we would not\n",
            "        expect the null hypothesis to be rejected.\n",
            "        \n",
            "        >>> stats.quantile_test(rvs, q=0.5, p=0.5, alternative='greater')\n",
            "        QuantileTestResult(statistic=67, statistic_type=1, pvalue=0.9997956114162866)\n",
            "        \n",
            "        Unsurprisingly, with a p-value greater than our threshold, we would not\n",
            "        reject the null hypothesis in favor of the chosen alternative.\n",
            "        \n",
            "        The quantile test can be used for any quantile, not only the median. For\n",
            "        example, we can test whether the third quartile of the distribution\n",
            "        underlying the sample is greater than 0.6.\n",
            "        \n",
            "        >>> rvs = stats.uniform.rvs(size=100, random_state=rng)\n",
            "        >>> stats.quantile_test(rvs, q=0.6, p=0.75, alternative='greater')\n",
            "        QuantileTestResult(statistic=64, statistic_type=1, pvalue=0.00940696592998271)\n",
            "        \n",
            "        The p-value is lower than the threshold. We reject the null hypothesis in\n",
            "        favor of the alternative: the third quartile of the distribution underlying\n",
            "        our sample is greater than 0.6.\n",
            "        \n",
            "        `quantile_test` can also compute confidence intervals for any quantile.\n",
            "        \n",
            "        >>> rvs = stats.norm.rvs(size=100, random_state=rng)\n",
            "        >>> res = stats.quantile_test(rvs, q=0.6, p=0.75)\n",
            "        >>> ci = res.confidence_interval(confidence_level=0.95)\n",
            "        >>> ci\n",
            "        ConfidenceInterval(low=0.284491604437432, high=0.8912531024914844)\n",
            "        \n",
            "        When testing a one-sided alternative, the confidence interval contains\n",
            "        all observations such that if passed as `q`, the p-value of the\n",
            "        test would be greater than 0.05, and therefore the null hypothesis\n",
            "        would not be rejected. For example:\n",
            "        \n",
            "        >>> rvs.sort()\n",
            "        >>> q, p, alpha = 0.6, 0.75, 0.95\n",
            "        >>> res = stats.quantile_test(rvs, q=q, p=p, alternative='less')\n",
            "        >>> ci = res.confidence_interval(confidence_level=alpha)\n",
            "        >>> for x in rvs[rvs <= ci.high]:\n",
            "        ...     res = stats.quantile_test(rvs, q=x, p=p, alternative='less')\n",
            "        ...     assert res.pvalue > 1-alpha\n",
            "        >>> for x in rvs[rvs > ci.high]:\n",
            "        ...     res = stats.quantile_test(rvs, q=x, p=p, alternative='less')\n",
            "        ...     assert res.pvalue < 1-alpha\n",
            "        \n",
            "        Also, if a 95% confidence interval is repeatedly generated for random\n",
            "        samples, the confidence interval will contain the true quantile value in\n",
            "        approximately 95% of replications.\n",
            "        \n",
            "        >>> dist = stats.rayleigh() # our \"unknown\" distribution\n",
            "        >>> p = 0.2\n",
            "        >>> true_stat = dist.ppf(p) # the true value of the statistic\n",
            "        >>> n_trials = 1000\n",
            "        >>> quantile_ci_contains_true_stat = 0\n",
            "        >>> for i in range(n_trials):\n",
            "        ...     data = dist.rvs(size=100, random_state=rng)\n",
            "        ...     res = stats.quantile_test(data, p=p)\n",
            "        ...     ci = res.confidence_interval(0.95)\n",
            "        ...     if ci[0] < true_stat < ci[1]:\n",
            "        ...         quantile_ci_contains_true_stat += 1\n",
            "        >>> quantile_ci_contains_true_stat >= 950\n",
            "        True\n",
            "        \n",
            "        This works with any distribution and any quantile, as long as the samples\n",
            "        are i.i.d.\n",
            "    \n",
            "    rankdata(a, method='average', *, axis=None, nan_policy='propagate')\n",
            "        Assign ranks to data, dealing with ties appropriately.\n",
            "        \n",
            "        By default (``axis=None``), the data array is first flattened, and a flat\n",
            "        array of ranks is returned. Separately reshape the rank array to the\n",
            "        shape of the data array if desired (see Examples).\n",
            "        \n",
            "        Ranks begin at 1.  The `method` argument controls how ranks are assigned\n",
            "        to equal values.  See [1]_ for further discussion of ranking methods.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            The array of values to be ranked.\n",
            "        method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n",
            "            The method used to assign ranks to tied elements.\n",
            "            The following methods are available (default is 'average'):\n",
            "        \n",
            "              * 'average': The average of the ranks that would have been assigned to\n",
            "                all the tied values is assigned to each value.\n",
            "              * 'min': The minimum of the ranks that would have been assigned to all\n",
            "                the tied values is assigned to each value.  (This is also\n",
            "                referred to as \"competition\" ranking.)\n",
            "              * 'max': The maximum of the ranks that would have been assigned to all\n",
            "                the tied values is assigned to each value.\n",
            "              * 'dense': Like 'min', but the rank of the next highest element is\n",
            "                assigned the rank immediately after those assigned to the tied\n",
            "                elements.\n",
            "              * 'ordinal': All values are given a distinct rank, corresponding to\n",
            "                the order that the values occur in `a`.\n",
            "        axis : {None, int}, optional\n",
            "            Axis along which to perform the ranking. If ``None``, the data array\n",
            "            is first flattened.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}, optional\n",
            "            Defines how to handle when input contains nan.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "              * 'propagate': propagates nans through the rank calculation\n",
            "              * 'omit': performs the calculations ignoring nan values\n",
            "              * 'raise': raises an error\n",
            "        \n",
            "            .. note::\n",
            "        \n",
            "                When `nan_policy` is 'propagate', the output is an array of *all*\n",
            "                nans because ranks relative to nans in the input are undefined.\n",
            "                When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n",
            "                the other values, and the corresponding locations of the output\n",
            "                are nan.\n",
            "        \n",
            "            .. versionadded:: 1.10\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        ranks : ndarray\n",
            "             An array of size equal to the size of `a`, containing rank\n",
            "             scores.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import rankdata\n",
            "        >>> rankdata([0, 2, 3, 2])\n",
            "        array([ 1. ,  2.5,  4. ,  2.5])\n",
            "        >>> rankdata([0, 2, 3, 2], method='min')\n",
            "        array([ 1,  2,  4,  2])\n",
            "        >>> rankdata([0, 2, 3, 2], method='max')\n",
            "        array([ 1,  3,  4,  3])\n",
            "        >>> rankdata([0, 2, 3, 2], method='dense')\n",
            "        array([ 1,  2,  3,  2])\n",
            "        >>> rankdata([0, 2, 3, 2], method='ordinal')\n",
            "        array([ 1,  2,  4,  3])\n",
            "        >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n",
            "        array([[1. , 2.5],\n",
            "              [4. , 2.5]])\n",
            "        >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n",
            "        array([[1. , 2.5, 2.5],\n",
            "               [2. , 1. , 3. ]])\n",
            "        >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n",
            "        array([nan, nan, nan, nan, nan, nan])\n",
            "        >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n",
            "        array([ 2.,  3.,  4., nan,  1., nan])\n",
            "    \n",
            "    ranksums(x, y, alternative='two-sided', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Compute the Wilcoxon rank-sum statistic for two samples.\n",
            "        \n",
            "        The Wilcoxon rank-sum test tests the null hypothesis that two sets\n",
            "        of measurements are drawn from the same distribution.  The alternative\n",
            "        hypothesis is that values in one sample are more likely to be\n",
            "        larger than the values in the other sample.\n",
            "        \n",
            "        This test should be used to compare two samples from continuous\n",
            "        distributions.  It does not handle ties between measurements\n",
            "        in x and y.  For tie-handling and an optional continuity correction\n",
            "        see `scipy.stats.mannwhitneyu`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x,y : array_like\n",
            "            The data from the two samples.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "            \n",
            "            * 'two-sided': one of the distributions (underlying `x` or `y`) is\n",
            "              stochastically greater than the other.\n",
            "            * 'less': the distribution underlying `x` is stochastically less\n",
            "              than the distribution underlying `y`.\n",
            "            * 'greater': the distribution underlying `x` is stochastically greater\n",
            "              than the distribution underlying `y`.\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic under the large-sample approximation that the\n",
            "            rank sum statistic is normally distributed.\n",
            "        pvalue : float\n",
            "            The p-value of the test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We can test the hypothesis that two independent unequal-sized samples are\n",
            "        drawn from the same distribution with computing the Wilcoxon rank-sum\n",
            "        statistic.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ranksums\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> sample1 = rng.uniform(-1, 1, 200)\n",
            "        >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n",
            "        >>> ranksums(sample1, sample2)\n",
            "        RanksumsResult(statistic=-7.887059,\n",
            "                       pvalue=3.09390448e-15) # may vary\n",
            "        >>> ranksums(sample1, sample2, alternative='less')\n",
            "        RanksumsResult(statistic=-7.750585297581713,\n",
            "                       pvalue=4.573497606342543e-15) # may vary\n",
            "        >>> ranksums(sample1, sample2, alternative='greater')\n",
            "        RanksumsResult(statistic=-7.750585297581713,\n",
            "                       pvalue=0.9999999999999954) # may vary\n",
            "        \n",
            "        The p-value of less than ``0.05`` indicates that this test rejects the\n",
            "        hypothesis at the 5% significance level.\n",
            "    \n",
            "    relfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
            "        Return a relative frequency histogram, using the histogram function.\n",
            "        \n",
            "        A relative frequency  histogram is a mapping of the number of\n",
            "        observations in each of the bins relative to the total of observations.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        numbins : int, optional\n",
            "            The number of bins to use for the histogram. Default is 10.\n",
            "        defaultreallimits : tuple (lower, upper), optional\n",
            "            The lower and upper values for the range of the histogram.\n",
            "            If no value is given, a range slightly larger than the range of the\n",
            "            values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
            "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
            "        weights : array_like, optional\n",
            "            The weights for each value in `a`. Default is None, which gives each\n",
            "            value a weight of 1.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        frequency : ndarray\n",
            "            Binned values of relative frequency.\n",
            "        lowerlimit : float\n",
            "            Lower real limit.\n",
            "        binsize : float\n",
            "            Width of each bin.\n",
            "        extrapoints : int\n",
            "            Extra points.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> a = np.array([2, 4, 1, 2, 3, 2])\n",
            "        >>> res = stats.relfreq(a, numbins=4)\n",
            "        >>> res.frequency\n",
            "        array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n",
            "        >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n",
            "        1.0\n",
            "        \n",
            "        Create a normal distribution with 1000 random values\n",
            "        \n",
            "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
            "        \n",
            "        Calculate relative frequencies\n",
            "        \n",
            "        >>> res = stats.relfreq(samples, numbins=25)\n",
            "        \n",
            "        Calculate space of values for x\n",
            "        \n",
            "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n",
            "        ...                                  res.frequency.size)\n",
            "        \n",
            "        Plot relative frequency histogram\n",
            "        \n",
            "        >>> fig = plt.figure(figsize=(5, 4))\n",
            "        >>> ax = fig.add_subplot(1, 1, 1)\n",
            "        >>> ax.bar(x, res.frequency, width=res.binsize)\n",
            "        >>> ax.set_title('Relative frequency histogram')\n",
            "        >>> ax.set_xlim([x.min(), x.max()])\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    rvs_ratio_uniforms(pdf, umax, vmin, vmax, size=1, c=0, random_state=None)\n",
            "        Generate random samples from a probability density function using the\n",
            "        ratio-of-uniforms method.\n",
            "        \n",
            "        .. deprecated:: 1.12.0\n",
            "            `rvs_ratio_uniforms` is deprecated in favour of\n",
            "            `scipy.stats.sampling.RatioUniforms` from version 1.12.0 and will\n",
            "            be removed in SciPy 1.15.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        pdf : callable\n",
            "            A function with signature `pdf(x)` that is proportional to the\n",
            "            probability density function of the distribution.\n",
            "        umax : float\n",
            "            The upper bound of the bounding rectangle in the u-direction.\n",
            "        vmin : float\n",
            "            The lower bound of the bounding rectangle in the v-direction.\n",
            "        vmax : float\n",
            "            The upper bound of the bounding rectangle in the v-direction.\n",
            "        size : int or tuple of ints, optional\n",
            "            Defining number of random variates (default is 1).\n",
            "        c : float, optional.\n",
            "            Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                        `numpy.random.RandomState`}, optional\n",
            "        \n",
            "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "            singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `seed`.\n",
            "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "            that instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        rvs : ndarray\n",
            "            The random variates distributed according to the probability\n",
            "            distribution defined by the pdf.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Please refer to `scipy.stats.sampling.RatioUniforms` for the documentation.\n",
            "    \n",
            "    scoreatpercentile(a, per, limit=(), interpolation_method='fraction', axis=None)\n",
            "        Calculate the score at a given percentile of the input sequence.\n",
            "        \n",
            "        For example, the score at `per=50` is the median. If the desired quantile\n",
            "        lies between two data points, we interpolate between them, according to\n",
            "        the value of `interpolation`. If the parameter `limit` is provided, it\n",
            "        should be a tuple (lower, upper) of two values.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            A 1-D array of values from which to extract score.\n",
            "        per : array_like\n",
            "            Percentile(s) at which to extract score.  Values should be in range\n",
            "            [0,100].\n",
            "        limit : tuple, optional\n",
            "            Tuple of two scalars, the lower and upper limits within which to\n",
            "            compute the percentile. Values of `a` outside\n",
            "            this (closed) interval will be ignored.\n",
            "        interpolation_method : {'fraction', 'lower', 'higher'}, optional\n",
            "            Specifies the interpolation method to use,\n",
            "            when the desired quantile lies between two data points `i` and `j`\n",
            "            The following options are available (default is 'fraction'):\n",
            "        \n",
            "              * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the\n",
            "                fractional part of the index surrounded by ``i`` and ``j``\n",
            "              * 'lower': ``i``\n",
            "              * 'higher': ``j``\n",
            "        \n",
            "        axis : int, optional\n",
            "            Axis along which the percentiles are computed. Default is None. If\n",
            "            None, compute over the whole array `a`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        score : float or ndarray\n",
            "            Score at percentile(s).\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        percentileofscore, numpy.percentile\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function will become obsolete in the future.\n",
            "        For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality\n",
            "        that `scoreatpercentile` provides.  And it's significantly faster.\n",
            "        Therefore it's recommended to use `numpy.percentile` for users that have\n",
            "        numpy >= 1.9.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = np.arange(100)\n",
            "        >>> stats.scoreatpercentile(a, 50)\n",
            "        49.5\n",
            "    \n",
            "    sem(a, axis=0, ddof=1, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute standard error of the mean.\n",
            "        \n",
            "        Calculate the standard error of the mean (or standard error of\n",
            "        measurement) of the values in the input array.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            An array containing the values for which the standard error is\n",
            "            returned.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        ddof : int, optional\n",
            "            Delta degrees-of-freedom. How many degrees of freedom to adjust\n",
            "            for bias in limited samples relative to the population estimate\n",
            "            of variance. Defaults to 1.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        s : ndarray or float\n",
            "            The standard error of the mean in the sample(s), along the input axis.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The default value for `ddof` is different to the default (0) used by other\n",
            "        ddof containing routines, such as np.std and np.nanstd.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Find standard error along the first axis:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = np.arange(20).reshape(5,4)\n",
            "        >>> stats.sem(a)\n",
            "        array([ 2.8284,  2.8284,  2.8284,  2.8284])\n",
            "        \n",
            "        Find standard error across the whole array, using n degrees of freedom:\n",
            "        \n",
            "        >>> stats.sem(a, axis=None, ddof=0)\n",
            "        1.2893796958227628\n",
            "    \n",
            "    shapiro(x, *, axis=None, nan_policy='propagate', keepdims=False)\n",
            "        Perform the Shapiro-Wilk test for normality.\n",
            "        \n",
            "        The Shapiro-Wilk test tests the null hypothesis that the\n",
            "        data was drawn from a normal distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Array of sample data.\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The test statistic.\n",
            "        p-value : float\n",
            "            The p-value for the hypothesis test.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`anderson`\n",
            "            The Anderson-Darling test for normality\n",
            "        :func:`kstest`\n",
            "            The Kolmogorov-Smirnov test for goodness of fit.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The algorithm used is described in [4]_ but censoring parameters as\n",
            "        described are not implemented. For N > 5000 the W test statistic is\n",
            "        accurate, but the p-value may not be.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
            "               :doi:`10.18434/M32189`\n",
            "        .. [2] Shapiro, S. S. & Wilk, M.B, \"An analysis of variance test for\n",
            "               normality (complete samples)\", Biometrika, 1965, Vol. 52,\n",
            "               pp. 591-611, :doi:`10.2307/2333709`\n",
            "        .. [3] Razali, N. M. & Wah, Y. B., \"Power comparisons of Shapiro-Wilk,\n",
            "               Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests\", Journal\n",
            "               of Statistical Modeling and Analytics, 2011, Vol. 2, pp. 21-33.\n",
            "        .. [4] Royston P., \"Remark AS R94: A Remark on Algorithm AS 181: The\n",
            "               W-test for Normality\", 1995, Applied Statistics, Vol. 44,\n",
            "               :doi:`10.2307/2986146`\n",
            "        .. [5] Phipson B., and Smyth, G. K., \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn\", Statistical Applications in Genetics and Molecular Biology,\n",
            "               2010, Vol.9, :doi:`10.2202/1544-6115.1585`\n",
            "        .. [6] Panagiotakos, D. B., \"The value of p-value in biomedical\n",
            "               research\", The Open Cardiovascular Medicine Journal, 2008, Vol.2,\n",
            "               pp. 97-99, :doi:`10.2174/1874192400802010097`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to infer from measurements whether the weights of adult\n",
            "        human males in a medical study are not normally distributed [2]_.\n",
            "        The weights (lbs) are recorded in the array ``x`` below.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n",
            "        \n",
            "        The normality test of [1]_ and [2]_ begins by computing a statistic based\n",
            "        on the relationship between the observations and the expected order\n",
            "        statistics of a normal distribution.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.shapiro(x)\n",
            "        >>> res.statistic\n",
            "        0.7888147830963135\n",
            "        \n",
            "        The value of this statistic tends to be high (close to 1) for samples drawn\n",
            "        from a normal distribution.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the statistic\n",
            "        against the null distribution: the distribution of statistic values formed\n",
            "        under the null hypothesis that the weights were drawn from a normal\n",
            "        distribution. For this normality test, the null distribution is not easy to\n",
            "        calculate exactly, so it is usually approximated by Monte Carlo methods,\n",
            "        that is, drawing many samples of the same size as ``x`` from a normal\n",
            "        distribution and computing the values of the statistic for each.\n",
            "        \n",
            "        >>> def statistic(x):\n",
            "        ...     # Get only the `shapiro` statistic; ignore its p-value\n",
            "        ...     return stats.shapiro(x).statistic\n",
            "        >>> ref = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n",
            "        ...                              alternative='less')\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> bins = np.linspace(0.65, 1, 50)\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.hist(ref.null_distribution, density=True, bins=bins)\n",
            "        ...     ax.set_title(\"Shapiro-Wilk Test Null Distribution \\n\"\n",
            "        ...                  \"(Monte Carlo Approximation, 11 Observations)\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution less than or equal to the observed value of the\n",
            "        statistic.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> annotation = (f'p-value={res.pvalue:.6f}\\n(highlighted area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (0.75, 0.1), (0.68, 0.7), arrowprops=props)\n",
            "        >>> i_extreme = np.where(bins <= res.statistic)[0]\n",
            "        >>> for i in i_extreme:\n",
            "        ...     ax.patches[i].set_color('C1')\n",
            "        >>> plt.xlim(0.65, 0.9)\n",
            "        >>> plt.ylim(0, 4)\n",
            "        >>> plt.show\n",
            "        >>> res.pvalue\n",
            "        0.006703833118081093\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from a normally distributed population that produces such an\n",
            "        extreme value of the statistic - this may be taken as evidence against\n",
            "        the null hypothesis in favor of the alternative: the weights were not\n",
            "        drawn from a normal distribution. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence *for* the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [5]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "    \n",
            "    siegelslopes(y, x=None, method='hierarchical')\n",
            "        Computes the Siegel estimator for a set of points (x, y).\n",
            "        \n",
            "        `siegelslopes` implements a method for robust linear regression\n",
            "        using repeated medians (see [1]_) to fit a line to the points (x, y).\n",
            "        The method is robust to outliers with an asymptotic breakdown point\n",
            "        of 50%.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        y : array_like\n",
            "            Dependent variable.\n",
            "        x : array_like or None, optional\n",
            "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
            "        method : {'hierarchical', 'separate'}\n",
            "            If 'hierarchical', estimate the intercept using the estimated\n",
            "            slope ``slope`` (default option).\n",
            "            If 'separate', estimate the intercept independent of the estimated\n",
            "            slope. See Notes for details.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : ``SiegelslopesResult`` instance\n",
            "            The return value is an object with the following attributes:\n",
            "        \n",
            "            slope : float\n",
            "                Estimate of the slope of the regression line.\n",
            "            intercept : float\n",
            "                Estimate of the intercept of the regression line.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        theilslopes : a similar technique without repeated medians\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        With ``n = len(y)``, compute ``m_j`` as the median of\n",
            "        the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\n",
            "        ``slope`` is then the median of all slopes ``m_j``.\n",
            "        Two ways are given to estimate the intercept in [1]_ which can be chosen\n",
            "        via the parameter ``method``.\n",
            "        The hierarchical approach uses the estimated slope ``slope``\n",
            "        and computes ``intercept`` as the median of ``y - slope*x``.\n",
            "        The other approach estimates the intercept separately as follows: for\n",
            "        each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\n",
            "        lines through the remaining points and take the median ``i_j``.\n",
            "        ``intercept`` is the median of the ``i_j``.\n",
            "        \n",
            "        The implementation computes `n` times the median of a vector of size `n`\n",
            "        which can be slow for large vectors. There are more efficient algorithms\n",
            "        (see [2]_) which are not implemented here.\n",
            "        \n",
            "        For compatibility with older versions of SciPy, the return value acts\n",
            "        like a ``namedtuple`` of length 2, with fields ``slope`` and\n",
            "        ``intercept``, so one can continue to write::\n",
            "        \n",
            "            slope, intercept = siegelslopes(y, x)\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\n",
            "               Biometrika, Vol. 69, pp. 242-244, 1982.\n",
            "        \n",
            "        .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\n",
            "               line\", Proceedings of the Third Annual ACM-SIAM Symposium on\n",
            "               Discrete Algorithms, pp. 409-413, 1992.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        >>> x = np.linspace(-5, 5, num=150)\n",
            "        >>> y = x + np.random.normal(size=x.size)\n",
            "        >>> y[11:15] += 10  # add outliers\n",
            "        >>> y[-5:] -= 7\n",
            "        \n",
            "        Compute the slope and intercept.  For comparison, also compute the\n",
            "        least-squares fit with `linregress`:\n",
            "        \n",
            "        >>> res = stats.siegelslopes(y, x)\n",
            "        >>> lsq_res = stats.linregress(x, y)\n",
            "        \n",
            "        Plot the results. The Siegel regression line is shown in red. The green\n",
            "        line shows the least-squares fit for comparison.\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(x, y, 'b.')\n",
            "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
            "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    sigmaclip(a, low=4.0, high=4.0)\n",
            "        Perform iterative sigma-clipping of array elements.\n",
            "        \n",
            "        Starting from the full sample, all elements outside the critical range are\n",
            "        removed, i.e. all elements of the input array `c` that satisfy either of\n",
            "        the following conditions::\n",
            "        \n",
            "            c < mean(c) - std(c)*low\n",
            "            c > mean(c) + std(c)*high\n",
            "        \n",
            "        The iteration continues with the updated sample until no\n",
            "        elements are outside the (updated) range.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Data array, will be raveled if not 1-D.\n",
            "        low : float, optional\n",
            "            Lower bound factor of sigma clipping. Default is 4.\n",
            "        high : float, optional\n",
            "            Upper bound factor of sigma clipping. Default is 4.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        clipped : ndarray\n",
            "            Input array with clipped elements removed.\n",
            "        lower : float\n",
            "            Lower threshold value use for clipping.\n",
            "        upper : float\n",
            "            Upper threshold value use for clipping.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import sigmaclip\n",
            "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n",
            "        ...                     np.linspace(0, 20, 5)))\n",
            "        >>> fact = 1.5\n",
            "        >>> c, low, upp = sigmaclip(a, fact, fact)\n",
            "        >>> c\n",
            "        array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n",
            "        >>> c.var(), c.std()\n",
            "        (0.00055555555555555165, 0.023570226039551501)\n",
            "        >>> low, c.mean() - fact*c.std(), c.min()\n",
            "        (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n",
            "        >>> upp, c.mean() + fact*c.std(), c.max()\n",
            "        (10.035355339059327, 10.035355339059327, 10.033333333333333)\n",
            "        \n",
            "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n",
            "        ...                     np.linspace(-100, -50, 3)))\n",
            "        >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n",
            "        >>> (c == np.linspace(9.5, 10.5, 11)).all()\n",
            "        True\n",
            "    \n",
            "    skew(a, axis=0, bias=True, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the sample skewness of a data set.\n",
            "        \n",
            "        For normally distributed data, the skewness should be about zero. For\n",
            "        unimodal continuous distributions, a skewness value greater than zero means\n",
            "        that there is more weight in the right tail of the distribution. The\n",
            "        function `skewtest` can be used to determine if the skewness value\n",
            "        is close enough to zero, statistically speaking.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : ndarray\n",
            "            Input array.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        bias : bool, optional\n",
            "            If False, then the calculations are corrected for statistical bias.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        skewness : ndarray\n",
            "            The skewness of values along an axis, returning NaN where all values\n",
            "            are equal.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The sample skewness is computed as the Fisher-Pearson coefficient\n",
            "        of skewness, i.e.\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            g_1=\\frac{m_3}{m_2^{3/2}}\n",
            "        \n",
            "        where\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\n",
            "        \n",
            "        is the biased sample :math:`i\\texttt{th}` central moment, and\n",
            "        :math:`\\bar{x}` is\n",
            "        the sample mean.  If ``bias`` is False, the calculations are\n",
            "        corrected for bias and the value computed is the adjusted\n",
            "        Fisher-Pearson standardized moment coefficient, i.e.\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            G_1=\\frac{k_3}{k_2^{3/2}}=\n",
            "                \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
            "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
            "           York. 2000.\n",
            "           Section 2.2.24.1\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import skew\n",
            "        >>> skew([1, 2, 3, 4, 5])\n",
            "        0.0\n",
            "        >>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n",
            "        0.2650554122698573\n",
            "    \n",
            "    skewtest(a, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
            "        Test whether the skew is different from the normal distribution.\n",
            "        \n",
            "        This function tests the null hypothesis that the skewness of\n",
            "        the population that the sample was drawn from is the same\n",
            "        as that of a corresponding normal distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array\n",
            "            The data to be tested.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "            \n",
            "            * 'two-sided': the skewness of the distribution underlying the sample\n",
            "              is different from that of the normal distribution (i.e. 0)\n",
            "            * 'less': the skewness of the distribution underlying the sample\n",
            "              is less than that of the normal distribution\n",
            "            * 'greater': the skewness of the distribution underlying the sample\n",
            "              is greater than that of the normal distribution\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float\n",
            "            The computed z-score for this test.\n",
            "        pvalue : float\n",
            "            The p-value for the hypothesis test.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The sample size must be at least 8.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,\n",
            "                \"A suggestion for using powerful and informative tests of\n",
            "                normality\", American Statistician 44, pp. 316-321, 1990.\n",
            "        .. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n",
            "               for normality (complete samples). Biometrika, 52(3/4), 591-611.\n",
            "        .. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "               Zero: Calculating Exact P-values When Permutations Are Randomly\n",
            "               Drawn.\" Statistical Applications in Genetics and Molecular Biology\n",
            "               9.1 (2010).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to infer from measurements whether the weights of adult\n",
            "        human males in a medical study are not normally distributed [2]_.\n",
            "        The weights (lbs) are recorded in the array ``x`` below.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n",
            "        \n",
            "        The skewness test from [1]_ begins by computing a statistic based on the\n",
            "        sample skewness.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.skewtest(x)\n",
            "        >>> res.statistic\n",
            "        2.7788579769903414\n",
            "        \n",
            "        Because normal distributions have zero skewness, the magnitude of this\n",
            "        statistic tends to be low for samples drawn from a normal distribution.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the\n",
            "        statistic against the null distribution: the distribution of statistic\n",
            "        values derived under the null hypothesis that the weights were drawn from\n",
            "        a normal distribution.\n",
            "        \n",
            "        For this test, the null distribution of the statistic for very large\n",
            "        samples is the standard normal distribution.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> dist = stats.norm()\n",
            "        >>> st_val = np.linspace(-5, 5, 100)\n",
            "        >>> pdf = dist.pdf(st_val)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def st_plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(st_val, pdf)\n",
            "        ...     ax.set_title(\"Skew Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> st_plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution as extreme or more extreme than the observed\n",
            "        value of the statistic. In a two-sided test, elements of the null\n",
            "        distribution greater than the observed statistic and elements of the null\n",
            "        distribution less than the negative of the observed statistic are both\n",
            "        considered \"more extreme\".\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> st_plot(ax)\n",
            "        >>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n",
            "        >>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (3, 0.005), (3.25, 0.02), arrowprops=props)\n",
            "        >>> i = st_val >= res.statistic\n",
            "        >>> ax.fill_between(st_val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> i = st_val <= -res.statistic\n",
            "        >>> ax.fill_between(st_val[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> ax.set_xlim(-5, 5)\n",
            "        >>> ax.set_ylim(0, 0.1)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.005455036974740185\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from a normally distributed population that produces such an\n",
            "        extreme value of the statistic - this may be taken as evidence against\n",
            "        the null hypothesis in favor of the alternative: the weights were not\n",
            "        drawn from a normal distribution. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [3]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        \n",
            "        Note that the standard normal distribution provides an asymptotic\n",
            "        approximation of the null distribution; it is only accurate for samples\n",
            "        with many observations. For small samples like ours,\n",
            "        `scipy.stats.monte_carlo_test` may provide a more accurate, albeit\n",
            "        stochastic, approximation of the exact p-value.\n",
            "        \n",
            "        >>> def statistic(x, axis):\n",
            "        ...     # get just the skewtest statistic; ignore the p-value\n",
            "        ...     return stats.skewtest(x, axis=axis).statistic\n",
            "        >>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> st_plot(ax)\n",
            "        >>> ax.hist(res.null_distribution, np.linspace(-5, 5, 50),\n",
            "        ...         density=True)\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            'Monte Carlo approximation\\n(11 observations)'])\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.0062  # may vary\n",
            "        \n",
            "        In this case, the asymptotic approximation and Monte Carlo approximation\n",
            "        agree fairly closely, even for our small sample.\n",
            "    \n",
            "    sobol_indices(*, func: \"Callable[[np.ndarray], npt.ArrayLike] | dict[Literal['f_A', 'f_B', 'f_AB'], np.ndarray]\", n: 'IntNumber', dists: 'list[PPFDist] | None' = None, method: \"Callable | Literal['saltelli_2010']\" = 'saltelli_2010', random_state: 'SeedType' = None) -> 'SobolResult'\n",
            "        Global sensitivity indices of Sobol'.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        func : callable or dict(str, array_like)\n",
            "            If `func` is a callable, function to compute the Sobol' indices from.\n",
            "            Its signature must be::\n",
            "        \n",
            "                func(x: ArrayLike) -> ArrayLike\n",
            "        \n",
            "            with ``x`` of shape ``(d, n)`` and output of shape ``(s, n)`` where:\n",
            "        \n",
            "            - ``d`` is the input dimensionality of `func`\n",
            "              (number of input variables),\n",
            "            - ``s`` is the output dimensionality of `func`\n",
            "              (number of output variables), and\n",
            "            - ``n`` is the number of samples (see `n` below).\n",
            "        \n",
            "            Function evaluation values must be finite.\n",
            "        \n",
            "            If `func` is a dictionary, contains the function evaluations from three\n",
            "            different arrays. Keys must be: ``f_A``, ``f_B`` and ``f_AB``.\n",
            "            ``f_A`` and ``f_B`` should have a shape ``(s, n)`` and ``f_AB``\n",
            "            should have a shape ``(d, s, n)``.\n",
            "            This is an advanced feature and misuse can lead to wrong analysis.\n",
            "        n : int\n",
            "            Number of samples used to generate the matrices ``A`` and ``B``.\n",
            "            Must be a power of 2. The total number of points at which `func` is\n",
            "            evaluated will be ``n*(d+2)``.\n",
            "        dists : list(distributions), optional\n",
            "            List of each parameter's distribution. The distribution of parameters\n",
            "            depends on the application and should be carefully chosen.\n",
            "            Parameters are assumed to be independently distributed, meaning there\n",
            "            is no constraint nor relationship between their values.\n",
            "        \n",
            "            Distributions must be an instance of a class with a ``ppf``\n",
            "            method.\n",
            "        \n",
            "            Must be specified if `func` is a callable, and ignored otherwise.\n",
            "        method : Callable or str, default: 'saltelli_2010'\n",
            "            Method used to compute the first and total Sobol' indices.\n",
            "        \n",
            "            If a callable, its signature must be::\n",
            "        \n",
            "                func(f_A: np.ndarray, f_B: np.ndarray, f_AB: np.ndarray)\n",
            "                -> Tuple[np.ndarray, np.ndarray]\n",
            "        \n",
            "            with ``f_A, f_B`` of shape ``(s, n)`` and ``f_AB`` of shape\n",
            "            ``(d, s, n)``.\n",
            "            These arrays contain the function evaluations from three different sets\n",
            "            of samples.\n",
            "            The output is a tuple of the first and total indices with\n",
            "            shape ``(s, d)``.\n",
            "            This is an advanced feature and misuse can lead to wrong analysis.\n",
            "        random_state : {None, int, `numpy.random.Generator`}, optional\n",
            "            If `random_state` is an int or None, a new `numpy.random.Generator` is\n",
            "            created using ``np.random.default_rng(random_state)``.\n",
            "            If `random_state` is already a ``Generator`` instance, then the\n",
            "            provided instance is used.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SobolResult\n",
            "            An object with attributes:\n",
            "        \n",
            "            first_order : ndarray of shape (s, d)\n",
            "                First order Sobol' indices.\n",
            "            total_order : ndarray of shape (s, d)\n",
            "                Total order Sobol' indices.\n",
            "        \n",
            "            And method:\n",
            "        \n",
            "            bootstrap(confidence_level: float, n_resamples: int)\n",
            "            -> BootstrapSobolResult\n",
            "        \n",
            "                A method providing confidence intervals on the indices.\n",
            "                See `scipy.stats.bootstrap` for more details.\n",
            "        \n",
            "                The bootstrapping is done on both first and total order indices,\n",
            "                and they are available in `BootstrapSobolResult` as attributes\n",
            "                ``first_order`` and ``total_order``.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Sobol' method [1]_, [2]_ is a variance-based Sensitivity Analysis which\n",
            "        obtains the contribution of each parameter to the variance of the\n",
            "        quantities of interest (QoIs; i.e., the outputs of `func`).\n",
            "        Respective contributions can be used to rank the parameters and\n",
            "        also gauge the complexity of the model by computing the\n",
            "        model's effective (or mean) dimension.\n",
            "        \n",
            "        .. note::\n",
            "        \n",
            "            Parameters are assumed to be independently distributed. Each\n",
            "            parameter can still follow any distribution. In fact, the distribution\n",
            "            is very important and should match the real distribution of the\n",
            "            parameters.\n",
            "        \n",
            "        It uses a functional decomposition of the variance of the function to\n",
            "        explore\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\mathbb{V}(Y) = \\sum_{i}^{d} \\mathbb{V}_i (Y) + \\sum_{i<j}^{d}\n",
            "            \\mathbb{V}_{ij}(Y) + ... + \\mathbb{V}_{1,2,...,d}(Y),\n",
            "        \n",
            "        introducing conditional variances:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\mathbb{V}_i(Y) = \\mathbb{\\mathbb{V}}[\\mathbb{E}(Y|x_i)]\n",
            "            \\qquad\n",
            "            \\mathbb{V}_{ij}(Y) = \\mathbb{\\mathbb{V}}[\\mathbb{E}(Y|x_i x_j)]\n",
            "            - \\mathbb{V}_i(Y) - \\mathbb{V}_j(Y),\n",
            "        \n",
            "        Sobol' indices are expressed as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            S_i = \\frac{\\mathbb{V}_i(Y)}{\\mathbb{V}[Y]}\n",
            "            \\qquad\n",
            "            S_{ij} =\\frac{\\mathbb{V}_{ij}(Y)}{\\mathbb{V}[Y]}.\n",
            "        \n",
            "        :math:`S_{i}` corresponds to the first-order term which apprises the\n",
            "        contribution of the i-th parameter, while :math:`S_{ij}` corresponds to the\n",
            "        second-order term which informs about the contribution of interactions\n",
            "        between the i-th and the j-th parameters. These equations can be\n",
            "        generalized to compute higher order terms; however, they are expensive to\n",
            "        compute and their interpretation is complex.\n",
            "        This is why only first order indices are provided.\n",
            "        \n",
            "        Total order indices represent the global contribution of the parameters\n",
            "        to the variance of the QoI and are defined as:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            S_{T_i} = S_i + \\sum_j S_{ij} + \\sum_{j,k} S_{ijk} + ...\n",
            "            = 1 - \\frac{\\mathbb{V}[\\mathbb{E}(Y|x_{\\sim i})]}{\\mathbb{V}[Y]}.\n",
            "        \n",
            "        First order indices sum to at most 1, while total order indices sum to at\n",
            "        least 1. If there are no interactions, then first and total order indices\n",
            "        are equal, and both first and total order indices sum to 1.\n",
            "        \n",
            "        .. warning::\n",
            "        \n",
            "            Negative Sobol' values are due to numerical errors. Increasing the\n",
            "            number of points `n` should help.\n",
            "        \n",
            "            The number of sample required to have a good analysis increases with\n",
            "            the dimensionality of the problem. e.g. for a 3 dimension problem,\n",
            "            consider at minima ``n >= 2**12``. The more complex the model is,\n",
            "            the more samples will be needed.\n",
            "        \n",
            "            Even for a purely addiditive model, the indices may not sum to 1 due\n",
            "            to numerical noise.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Sobol, I. M.. \"Sensitivity analysis for nonlinear mathematical\n",
            "           models.\" Mathematical Modeling and Computational Experiment, 1:407-414,\n",
            "           1993.\n",
            "        .. [2] Sobol, I. M. (2001). \"Global sensitivity indices for nonlinear\n",
            "           mathematical models and their Monte Carlo estimates.\" Mathematics\n",
            "           and Computers in Simulation, 55(1-3):271-280,\n",
            "           :doi:`10.1016/S0378-4754(00)00270-6`, 2001.\n",
            "        .. [3] Saltelli, A. \"Making best use of model evaluations to\n",
            "           compute sensitivity indices.\"  Computer Physics Communications,\n",
            "           145(2):280-297, :doi:`10.1016/S0010-4655(02)00280-1`, 2002.\n",
            "        .. [4] Saltelli, A., M. Ratto, T. Andres, F. Campolongo, J. Cariboni,\n",
            "           D. Gatelli, M. Saisana, and S. Tarantola. \"Global Sensitivity Analysis.\n",
            "           The Primer.\" 2007.\n",
            "        .. [5] Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and\n",
            "           S. Tarantola. \"Variance based sensitivity analysis of model\n",
            "           output. Design and estimator for the total sensitivity index.\"\n",
            "           Computer Physics Communications, 181(2):259-270,\n",
            "           :doi:`10.1016/j.cpc.2009.09.018`, 2010.\n",
            "        .. [6] Ishigami, T. and T. Homma. \"An importance quantification technique\n",
            "           in uncertainty analysis for computer models.\" IEEE,\n",
            "           :doi:`10.1109/ISUMA.1990.151285`, 1990.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        The following is an example with the Ishigami function [6]_\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            Y(\\mathbf{x}) = \\sin x_1 + 7 \\sin^2 x_2 + 0.1 x_3^4 \\sin x_1,\n",
            "        \n",
            "        with :math:`\\mathbf{x} \\in [-\\pi, \\pi]^3`. This function exhibits strong\n",
            "        non-linearity and non-monotonicity.\n",
            "        \n",
            "        Remember, Sobol' indices assumes that samples are independently\n",
            "        distributed. In this case we use a uniform distribution on each marginals.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import sobol_indices, uniform\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> def f_ishigami(x):\n",
            "        ...     f_eval = (\n",
            "        ...         np.sin(x[0])\n",
            "        ...         + 7 * np.sin(x[1])**2\n",
            "        ...         + 0.1 * (x[2]**4) * np.sin(x[0])\n",
            "        ...     )\n",
            "        ...     return f_eval\n",
            "        >>> indices = sobol_indices(\n",
            "        ...     func=f_ishigami, n=1024,\n",
            "        ...     dists=[\n",
            "        ...         uniform(loc=-np.pi, scale=2*np.pi),\n",
            "        ...         uniform(loc=-np.pi, scale=2*np.pi),\n",
            "        ...         uniform(loc=-np.pi, scale=2*np.pi)\n",
            "        ...     ],\n",
            "        ...     random_state=rng\n",
            "        ... )\n",
            "        >>> indices.first_order\n",
            "        array([0.31637954, 0.43781162, 0.00318825])\n",
            "        >>> indices.total_order\n",
            "        array([0.56122127, 0.44287857, 0.24229595])\n",
            "        \n",
            "        Confidence interval can be obtained using bootstrapping.\n",
            "        \n",
            "        >>> boot = indices.bootstrap()\n",
            "        \n",
            "        Then, this information can be easily visualized.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
            "        >>> _ = axs[0].errorbar(\n",
            "        ...     [1, 2, 3], indices.first_order, fmt='o',\n",
            "        ...     yerr=[\n",
            "        ...         indices.first_order - boot.first_order.confidence_interval.low,\n",
            "        ...         boot.first_order.confidence_interval.high - indices.first_order\n",
            "        ...     ],\n",
            "        ... )\n",
            "        >>> axs[0].set_ylabel(\"First order Sobol' indices\")\n",
            "        >>> axs[0].set_xlabel('Input parameters')\n",
            "        >>> axs[0].set_xticks([1, 2, 3])\n",
            "        >>> _ = axs[1].errorbar(\n",
            "        ...     [1, 2, 3], indices.total_order, fmt='o',\n",
            "        ...     yerr=[\n",
            "        ...         indices.total_order - boot.total_order.confidence_interval.low,\n",
            "        ...         boot.total_order.confidence_interval.high - indices.total_order\n",
            "        ...     ],\n",
            "        ... )\n",
            "        >>> axs[1].set_ylabel(\"Total order Sobol' indices\")\n",
            "        >>> axs[1].set_xlabel('Input parameters')\n",
            "        >>> axs[1].set_xticks([1, 2, 3])\n",
            "        >>> plt.tight_layout()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        .. note::\n",
            "        \n",
            "            By default, `scipy.stats.uniform` has support ``[0, 1]``.\n",
            "            Using the parameters ``loc`` and ``scale``, one obtains the uniform\n",
            "            distribution on ``[loc, loc + scale]``.\n",
            "        \n",
            "        This result is particularly interesting because the first order index\n",
            "        :math:`S_{x_3} = 0` whereas its total order is :math:`S_{T_{x_3}} = 0.244`.\n",
            "        This means that higher order interactions with :math:`x_3` are responsible\n",
            "        for the difference. Almost 25% of the observed variance\n",
            "        on the QoI is due to the correlations between :math:`x_3` and :math:`x_1`,\n",
            "        although :math:`x_3` by itself has no impact on the QoI.\n",
            "        \n",
            "        The following gives a visual explanation of Sobol' indices on this\n",
            "        function. Let's generate 1024 samples in :math:`[-\\pi, \\pi]^3` and\n",
            "        calculate the value of the output.\n",
            "        \n",
            "        >>> from scipy.stats import qmc\n",
            "        >>> n_dim = 3\n",
            "        >>> p_labels = ['$x_1$', '$x_2$', '$x_3$']\n",
            "        >>> sample = qmc.Sobol(d=n_dim, seed=rng).random(1024)\n",
            "        >>> sample = qmc.scale(\n",
            "        ...     sample=sample,\n",
            "        ...     l_bounds=[-np.pi, -np.pi, -np.pi],\n",
            "        ...     u_bounds=[np.pi, np.pi, np.pi]\n",
            "        ... )\n",
            "        >>> output = f_ishigami(sample.T)\n",
            "        \n",
            "        Now we can do scatter plots of the output with respect to each parameter.\n",
            "        This gives a visual way to understand how each parameter impacts the\n",
            "        output of the function.\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(1, n_dim, figsize=(12, 4))\n",
            "        >>> for i in range(n_dim):\n",
            "        ...     xi = sample[:, i]\n",
            "        ...     ax[i].scatter(xi, output, marker='+')\n",
            "        ...     ax[i].set_xlabel(p_labels[i])\n",
            "        >>> ax[0].set_ylabel('Y')\n",
            "        >>> plt.tight_layout()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Now Sobol' goes a step further:\n",
            "        by conditioning the output value by given values of the parameter\n",
            "        (black lines), the conditional output mean is computed. It corresponds to\n",
            "        the term :math:`\\mathbb{E}(Y|x_i)`. Taking the variance of this term gives\n",
            "        the numerator of the Sobol' indices.\n",
            "        \n",
            "        >>> mini = np.min(output)\n",
            "        >>> maxi = np.max(output)\n",
            "        >>> n_bins = 10\n",
            "        >>> bins = np.linspace(-np.pi, np.pi, num=n_bins, endpoint=False)\n",
            "        >>> dx = bins[1] - bins[0]\n",
            "        >>> fig, ax = plt.subplots(1, n_dim, figsize=(12, 4))\n",
            "        >>> for i in range(n_dim):\n",
            "        ...     xi = sample[:, i]\n",
            "        ...     ax[i].scatter(xi, output, marker='+')\n",
            "        ...     ax[i].set_xlabel(p_labels[i])\n",
            "        ...     for bin_ in bins:\n",
            "        ...         idx = np.where((bin_ <= xi) & (xi <= bin_ + dx))\n",
            "        ...         xi_ = xi[idx]\n",
            "        ...         y_ = output[idx]\n",
            "        ...         ave_y_ = np.mean(y_)\n",
            "        ...         ax[i].plot([bin_ + dx/2] * 2, [mini, maxi], c='k')\n",
            "        ...         ax[i].scatter(bin_ + dx/2, ave_y_, c='r')\n",
            "        >>> ax[0].set_ylabel('Y')\n",
            "        >>> plt.tight_layout()\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Looking at :math:`x_3`, the variance\n",
            "        of the mean is zero leading to :math:`S_{x_3} = 0`. But we can further\n",
            "        observe that the variance of the output is not constant along the parameter\n",
            "        values of :math:`x_3`. This heteroscedasticity is explained by higher order\n",
            "        interactions. Moreover, an heteroscedasticity is also noticeable on\n",
            "        :math:`x_1` leading to an interaction between :math:`x_3` and :math:`x_1`.\n",
            "        On :math:`x_2`, the variance seems to be constant and thus null interaction\n",
            "        with this parameter can be supposed.\n",
            "        \n",
            "        This case is fairly simple to analyse visually---although it is only a\n",
            "        qualitative analysis. Nevertheless, when the number of input parameters\n",
            "        increases such analysis becomes unrealistic as it would be difficult to\n",
            "        conclude on high-order terms. Hence the benefit of using Sobol' indices.\n",
            "    \n",
            "    somersd(x, y=None, alternative='two-sided')\n",
            "        Calculates Somers' D, an asymmetric measure of ordinal association.\n",
            "        \n",
            "        Like Kendall's :math:`\\tau`, Somers' :math:`D` is a measure of the\n",
            "        correspondence between two rankings. Both statistics consider the\n",
            "        difference between the number of concordant and discordant pairs in two\n",
            "        rankings :math:`X` and :math:`Y`, and both are normalized such that values\n",
            "        close  to 1 indicate strong agreement and values close to -1 indicate\n",
            "        strong disagreement. They differ in how they are normalized. To show the\n",
            "        relationship, Somers' :math:`D` can be defined in terms of Kendall's\n",
            "        :math:`\\tau_a`:\n",
            "        \n",
            "        .. math::\n",
            "            D(Y|X) = \\frac{\\tau_a(X, Y)}{\\tau_a(X, X)}\n",
            "        \n",
            "        Suppose the first ranking :math:`X` has :math:`r` distinct ranks and the\n",
            "        second ranking :math:`Y` has :math:`s` distinct ranks. These two lists of\n",
            "        :math:`n` rankings can also be viewed as an :math:`r \\times s` contingency\n",
            "        table in which element :math:`i, j` is the number of rank pairs with rank\n",
            "        :math:`i` in ranking :math:`X` and rank :math:`j` in ranking :math:`Y`.\n",
            "        Accordingly, `somersd` also allows the input data to be supplied as a\n",
            "        single, 2D contingency table instead of as two separate, 1D rankings.\n",
            "        \n",
            "        Note that the definition of Somers' :math:`D` is asymmetric: in general,\n",
            "        :math:`D(Y|X) \\neq D(X|Y)`. ``somersd(x, y)`` calculates Somers'\n",
            "        :math:`D(Y|X)`: the \"row\" variable :math:`X` is treated as an independent\n",
            "        variable, and the \"column\" variable :math:`Y` is dependent. For Somers'\n",
            "        :math:`D(X|Y)`, swap the input lists or transpose the input table.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            1D array of rankings, treated as the (row) independent variable.\n",
            "            Alternatively, a 2D contingency table.\n",
            "        y : array_like, optional\n",
            "            If `x` is a 1D array of rankings, `y` is a 1D array of rankings of the\n",
            "            same length, treated as the (column) dependent variable.\n",
            "            If `x` is 2D, `y` is ignored.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "            * 'two-sided': the rank correlation is nonzero\n",
            "            * 'less': the rank correlation is negative (less than zero)\n",
            "            * 'greater':  the rank correlation is positive (greater than zero)\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SomersDResult\n",
            "            A `SomersDResult` object with the following fields:\n",
            "        \n",
            "                statistic : float\n",
            "                   The Somers' :math:`D` statistic.\n",
            "                pvalue : float\n",
            "                   The p-value for a hypothesis test whose null\n",
            "                   hypothesis is an absence of association, :math:`D=0`.\n",
            "                   See notes for more information.\n",
            "                table : 2D array\n",
            "                   The contingency table formed from rankings `x` and `y` (or the\n",
            "                   provided contingency table, if `x` is a 2D array)\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kendalltau : Calculates Kendall's tau, another correlation measure.\n",
            "        weightedtau : Computes a weighted version of Kendall's tau.\n",
            "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
            "        pearsonr : Calculates a Pearson correlation coefficient.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function follows the contingency table approach of [2]_ and\n",
            "        [3]_. *p*-values are computed based on an asymptotic approximation of\n",
            "        the test statistic distribution under the null hypothesis :math:`D=0`.\n",
            "        \n",
            "        Theoretically, hypothesis tests based on Kendall's :math:`tau` and Somers'\n",
            "        :math:`D` should be identical.\n",
            "        However, the *p*-values returned by `kendalltau` are based\n",
            "        on the null hypothesis of *independence* between :math:`X` and :math:`Y`\n",
            "        (i.e. the population from which pairs in :math:`X` and :math:`Y` are\n",
            "        sampled contains equal numbers of all possible pairs), which is more\n",
            "        specific than the null hypothesis :math:`D=0` used here. If the null\n",
            "        hypothesis of independence is desired, it is acceptable to use the\n",
            "        *p*-value returned by `kendalltau` with the statistic returned by\n",
            "        `somersd` and vice versa. For more information, see [2]_.\n",
            "        \n",
            "        Contingency tables are formatted according to the convention used by\n",
            "        SAS and R: the first ranking supplied (``x``) is the \"row\" variable, and\n",
            "        the second ranking supplied (``y``) is the \"column\" variable. This is\n",
            "        opposite the convention of Somers' original paper [1]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Robert H. Somers, \"A New Asymmetric Measure of Association for\n",
            "               Ordinal Variables\", *American Sociological Review*, Vol. 27, No. 6,\n",
            "               pp. 799--811, 1962.\n",
            "        \n",
            "        .. [2] Morton B. Brown and Jacqueline K. Benedetti, \"Sampling Behavior of\n",
            "               Tests for Correlation in Two-Way Contingency Tables\", *Journal of\n",
            "               the American Statistical Association* Vol. 72, No. 358, pp.\n",
            "               309--315, 1977.\n",
            "        \n",
            "        .. [3] SAS Institute, Inc., \"The FREQ Procedure (Book Excerpt)\",\n",
            "               *SAS/STAT 9.2 User's Guide, Second Edition*, SAS Publishing, 2009.\n",
            "        \n",
            "        .. [4] Laerd Statistics, \"Somers' d using SPSS Statistics\", *SPSS\n",
            "               Statistics Tutorials and Statistical Guides*,\n",
            "               https://statistics.laerd.com/spss-tutorials/somers-d-using-spss-statistics.php,\n",
            "               Accessed July 31, 2020.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        We calculate Somers' D for the example given in [4]_, in which a hotel\n",
            "        chain owner seeks to determine the association between hotel room\n",
            "        cleanliness and customer satisfaction. The independent variable, hotel\n",
            "        room cleanliness, is ranked on an ordinal scale: \"below average (1)\",\n",
            "        \"average (2)\", or \"above average (3)\". The dependent variable, customer\n",
            "        satisfaction, is ranked on a second scale: \"very dissatisfied (1)\",\n",
            "        \"moderately dissatisfied (2)\", \"neither dissatisfied nor satisfied (3)\",\n",
            "        \"moderately satisfied (4)\", or \"very satisfied (5)\". 189 customers\n",
            "        respond to the survey, and the results are cast into a contingency table\n",
            "        with the hotel room cleanliness as the \"row\" variable and customer\n",
            "        satisfaction as the \"column\" variable.\n",
            "        \n",
            "        +-----+-----+-----+-----+-----+-----+\n",
            "        |     | (1) | (2) | (3) | (4) | (5) |\n",
            "        +=====+=====+=====+=====+=====+=====+\n",
            "        | (1) | 27  | 25  | 14  | 7   | 0   |\n",
            "        +-----+-----+-----+-----+-----+-----+\n",
            "        | (2) | 7   | 14  | 18  | 35  | 12  |\n",
            "        +-----+-----+-----+-----+-----+-----+\n",
            "        | (3) | 1   | 3   | 2   | 7   | 17  |\n",
            "        +-----+-----+-----+-----+-----+-----+\n",
            "        \n",
            "        For example, 27 customers assigned their room a cleanliness ranking of\n",
            "        \"below average (1)\" and a corresponding satisfaction of \"very\n",
            "        dissatisfied (1)\". We perform the analysis as follows.\n",
            "        \n",
            "        >>> from scipy.stats import somersd\n",
            "        >>> table = [[27, 25, 14, 7, 0], [7, 14, 18, 35, 12], [1, 3, 2, 7, 17]]\n",
            "        >>> res = somersd(table)\n",
            "        >>> res.statistic\n",
            "        0.6032766111513396\n",
            "        >>> res.pvalue\n",
            "        1.0007091191074533e-27\n",
            "        \n",
            "        The value of the Somers' D statistic is approximately 0.6, indicating\n",
            "        a positive correlation between room cleanliness and customer satisfaction\n",
            "        in the sample.\n",
            "        The *p*-value is very small, indicating a very small probability of\n",
            "        observing such an extreme value of the statistic under the null\n",
            "        hypothesis that the statistic of the entire population (from which\n",
            "        our sample of 189 customers is drawn) is zero. This supports the\n",
            "        alternative hypothesis that the true value of Somers' D for the population\n",
            "        is nonzero.\n",
            "    \n",
            "    spearmanr(a, b=None, axis=0, nan_policy='propagate', alternative='two-sided')\n",
            "        Calculate a Spearman correlation coefficient with associated p-value.\n",
            "        \n",
            "        The Spearman rank-order correlation coefficient is a nonparametric measure\n",
            "        of the monotonicity of the relationship between two datasets.\n",
            "        Like other correlation coefficients,\n",
            "        this one varies between -1 and +1 with 0 implying no correlation.\n",
            "        Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n",
            "        correlations imply that as x increases, so does y. Negative correlations\n",
            "        imply that as x increases, y decreases.\n",
            "        \n",
            "        The p-value roughly indicates the probability of an uncorrelated system\n",
            "        producing datasets that have a Spearman correlation at least as extreme\n",
            "        as the one computed from these datasets. Although calculation of the\n",
            "        p-value does not make strong assumptions about the distributions underlying\n",
            "        the samples, it is only accurate for very large samples (>500\n",
            "        observations). For smaller sample sizes, consider a permutation test (see\n",
            "        Examples section below).\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a, b : 1D or 2D array_like, b is optional\n",
            "            One or two 1-D or 2-D arrays containing multiple variables and\n",
            "            observations. When these are 1-D, each represents a vector of\n",
            "            observations of a single variable. For the behavior in the 2-D case,\n",
            "            see under ``axis``, below.\n",
            "            Both arrays need to have the same length in the ``axis`` dimension.\n",
            "        axis : int or None, optional\n",
            "            If axis=0 (default), then each column represents a variable, with\n",
            "            observations in the rows. If axis=1, the relationship is transposed:\n",
            "            each row represents a variable, while the columns contain observations.\n",
            "            If axis=None, then both arrays will be raveled.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan.\n",
            "            The following options are available (default is 'propagate'):\n",
            "        \n",
            "            * 'propagate': returns nan\n",
            "            * 'raise': throws an error\n",
            "            * 'omit': performs the calculations ignoring nan values\n",
            "        \n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            The following options are available:\n",
            "        \n",
            "            * 'two-sided': the correlation is nonzero\n",
            "            * 'less': the correlation is negative (less than zero)\n",
            "            * 'greater':  the correlation is positive (greater than zero)\n",
            "        \n",
            "            .. versionadded:: 1.7.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res : SignificanceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float or ndarray (2-D square)\n",
            "                Spearman correlation matrix or correlation coefficient (if only 2\n",
            "                variables are given as parameters). Correlation matrix is square\n",
            "                with length equal to total number of variables (columns or rows) in\n",
            "                ``a`` and ``b`` combined.\n",
            "            pvalue : float\n",
            "                The p-value for a hypothesis test whose null hypothesis\n",
            "                is that two samples have no ordinal correlation. See\n",
            "                `alternative` above for alternative hypotheses. `pvalue` has the\n",
            "                same shape as `statistic`.\n",
            "        \n",
            "        Warns\n",
            "        -----\n",
            "        `~scipy.stats.ConstantInputWarning`\n",
            "            Raised if an input is a constant array.  The correlation coefficient\n",
            "            is not defined in this case, so ``np.nan`` is returned.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
            "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
            "           York. 2000.\n",
            "           Section  14.7\n",
            "        .. [2] Kendall, M. G. and Stuart, A. (1973).\n",
            "           The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n",
            "           Griffin. 1973.\n",
            "           Section 31.18\n",
            "        .. [3] Kershenobich, D., Fierro, F. J., & Rojkind, M. (1970). The\n",
            "           relationship between the free pool of proline and collagen content in\n",
            "           human liver cirrhosis. The Journal of Clinical Investigation, 49(12),\n",
            "           2246-2249.\n",
            "        .. [4] Hollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric\n",
            "           statistical methods. John Wiley & Sons.\n",
            "        .. [5] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n",
            "           Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n",
            "           Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n",
            "        .. [6] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n",
            "           superior to t and F tests in biomedical research. The American\n",
            "           Statistician, 52(2), 127-132.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Consider the following data from [3]_, which studied the relationship\n",
            "        between free proline (an amino acid) and total collagen (a protein often\n",
            "        found in connective tissue) in unhealthy human livers.\n",
            "        \n",
            "        The ``x`` and ``y`` arrays below record measurements of the two compounds.\n",
            "        The observations are paired: each free proline measurement was taken from\n",
            "        the same liver as the total collagen measurement at the same index.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> # total collagen (mg/g dry weight of liver)\n",
            "        >>> x = np.array([7.1, 7.1, 7.2, 8.3, 9.4, 10.5, 11.4])\n",
            "        >>> # free proline ( mole/g dry weight of liver)\n",
            "        >>> y = np.array([2.8, 2.9, 2.8, 2.6, 3.5, 4.6, 5.0])\n",
            "        \n",
            "        These data were analyzed in [4]_ using Spearman's correlation coefficient,\n",
            "        a statistic sensitive to monotonic correlation between the samples.\n",
            "        \n",
            "        >>> from scipy import stats\n",
            "        >>> res = stats.spearmanr(x, y)\n",
            "        >>> res.statistic\n",
            "        0.7000000000000001\n",
            "        \n",
            "        The value of this statistic tends to be high (close to 1) for samples with\n",
            "        a strongly positive ordinal correlation, low (close to -1) for samples with\n",
            "        a strongly negative ordinal correlation, and small in magnitude (close to\n",
            "        zero) for samples with weak ordinal correlation.\n",
            "        \n",
            "        The test is performed by comparing the observed value of the\n",
            "        statistic against the null distribution: the distribution of statistic\n",
            "        values derived under the null hypothesis that total collagen and free\n",
            "        proline measurements are independent.\n",
            "        \n",
            "        For this test, the statistic can be transformed such that the null\n",
            "        distribution for large samples is Student's t distribution with\n",
            "        ``len(x) - 2`` degrees of freedom.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> dof = len(x)-2  # len(x) == len(y)\n",
            "        >>> dist = stats.t(df=dof)\n",
            "        >>> t_vals = np.linspace(-5, 5, 100)\n",
            "        >>> pdf = dist.pdf(t_vals)\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> def plot(ax):  # we'll reuse this\n",
            "        ...     ax.plot(t_vals, pdf)\n",
            "        ...     ax.set_title(\"Spearman's Rho Test Null Distribution\")\n",
            "        ...     ax.set_xlabel(\"statistic\")\n",
            "        ...     ax.set_ylabel(\"probability density\")\n",
            "        >>> plot(ax)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The comparison is quantified by the p-value: the proportion of values in\n",
            "        the null distribution as extreme or more extreme than the observed\n",
            "        value of the statistic. In a two-sided test in which the statistic is\n",
            "        positive, elements of the null distribution greater than the transformed\n",
            "        statistic and elements of the null distribution less than the negative of\n",
            "        the observed statistic are both considered \"more extreme\".\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> rs = res.statistic  # original statistic\n",
            "        >>> transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n",
            "        >>> pvalue = dist.cdf(-transformed) + dist.sf(transformed)\n",
            "        >>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (2.7, 0.025), (3, 0.03), arrowprops=props)\n",
            "        >>> i = t_vals >= transformed\n",
            "        >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> i = t_vals <= -transformed\n",
            "        >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> ax.set_xlim(-5, 5)\n",
            "        >>> ax.set_ylim(0, 0.1)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.07991669030889909  # two-sided p-value\n",
            "        \n",
            "        If the p-value is \"small\" - that is, if there is a low probability of\n",
            "        sampling data from independent distributions that produces such an extreme\n",
            "        value of the statistic - this may be taken as evidence against the null\n",
            "        hypothesis in favor of the alternative: the distribution of total collagen\n",
            "        and free proline are *not* independent. Note that:\n",
            "        \n",
            "        - The inverse is not true; that is, the test is not used to provide\n",
            "          evidence for the null hypothesis.\n",
            "        - The threshold for values that will be considered \"small\" is a choice that\n",
            "          should be made before the data is analyzed [5]_ with consideration of the\n",
            "          risks of both false positives (incorrectly rejecting the null hypothesis)\n",
            "          and false negatives (failure to reject a false null hypothesis).\n",
            "        - Small p-values are not evidence for a *large* effect; rather, they can\n",
            "          only provide evidence for a \"significant\" effect, meaning that they are\n",
            "          unlikely to have occurred under the null hypothesis.\n",
            "        \n",
            "        Suppose that before performing the experiment, the authors had reason\n",
            "        to predict a positive correlation between the total collagen and free\n",
            "        proline measurements, and that they had chosen to assess the plausibility\n",
            "        of the null hypothesis against a one-sided alternative: free proline has a\n",
            "        positive ordinal correlation with total collagen. In this case, only those\n",
            "        values in the null distribution that are as great or greater than the\n",
            "        observed statistic are considered to be more extreme.\n",
            "        \n",
            "        >>> res = stats.spearmanr(x, y, alternative='greater')\n",
            "        >>> res.statistic\n",
            "        0.7000000000000001  # same statistic\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> pvalue = dist.sf(transformed)\n",
            "        >>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n",
            "        >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n",
            "        >>> _ = ax.annotate(annotation, (3, 0.018), (3.5, 0.03), arrowprops=props)\n",
            "        >>> i = t_vals >= transformed\n",
            "        >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n",
            "        >>> ax.set_xlim(1, 5)\n",
            "        >>> ax.set_ylim(0, 0.1)\n",
            "        >>> plt.show()\n",
            "        >>> res.pvalue\n",
            "        0.03995834515444954  # one-sided p-value; half of the two-sided p-value\n",
            "        \n",
            "        Note that the t-distribution provides an asymptotic approximation of the\n",
            "        null distribution; it is only accurate for samples with many observations.\n",
            "        For small samples, it may be more appropriate to perform a permutation\n",
            "        test: Under the null hypothesis that total collagen and free proline are\n",
            "        independent, each of the free proline measurements were equally likely to\n",
            "        have been observed with any of the total collagen measurements. Therefore,\n",
            "        we can form an *exact* null distribution by calculating the statistic under\n",
            "        each possible pairing of elements between ``x`` and ``y``.\n",
            "        \n",
            "        >>> def statistic(x):  # explore all possible pairings by permuting `x`\n",
            "        ...     rs = stats.spearmanr(x, y).statistic  # ignore pvalue\n",
            "        ...     transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n",
            "        ...     return transformed\n",
            "        >>> ref = stats.permutation_test((x,), statistic, alternative='greater',\n",
            "        ...                              permutation_type='pairings')\n",
            "        >>> fig, ax = plt.subplots(figsize=(8, 5))\n",
            "        >>> plot(ax)\n",
            "        >>> ax.hist(ref.null_distribution, np.linspace(-5, 5, 26),\n",
            "        ...         density=True)\n",
            "        >>> ax.legend(['aymptotic approximation\\n(many observations)',\n",
            "        ...            f'exact \\n({len(ref.null_distribution)} permutations)'])\n",
            "        >>> plt.show()\n",
            "        >>> ref.pvalue\n",
            "        0.04563492063492063  # exact one-sided p-value\n",
            "    \n",
            "    theilslopes(y, x=None, alpha=0.95, method='separate')\n",
            "        Computes the Theil-Sen estimator for a set of points (x, y).\n",
            "        \n",
            "        `theilslopes` implements a method for robust linear regression.  It\n",
            "        computes the slope as the median of all slopes between paired values.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        y : array_like\n",
            "            Dependent variable.\n",
            "        x : array_like or None, optional\n",
            "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
            "        alpha : float, optional\n",
            "            Confidence degree between 0 and 1. Default is 95% confidence.\n",
            "            Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n",
            "            interpreted as \"find the 90% confidence interval\".\n",
            "        method : {'joint', 'separate'}, optional\n",
            "            Method to be used for computing estimate for intercept.\n",
            "            Following methods are supported,\n",
            "        \n",
            "                * 'joint': Uses np.median(y - slope * x) as intercept.\n",
            "                * 'separate': Uses np.median(y) - slope * np.median(x)\n",
            "                              as intercept.\n",
            "        \n",
            "            The default is 'separate'.\n",
            "        \n",
            "            .. versionadded:: 1.8.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : ``TheilslopesResult`` instance\n",
            "            The return value is an object with the following attributes:\n",
            "        \n",
            "            slope : float\n",
            "                Theil slope.\n",
            "            intercept : float\n",
            "                Intercept of the Theil line.\n",
            "            low_slope : float\n",
            "                Lower bound of the confidence interval on `slope`.\n",
            "            high_slope : float\n",
            "                Upper bound of the confidence interval on `slope`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        siegelslopes : a similar technique using repeated medians\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The implementation of `theilslopes` follows [1]_. The intercept is\n",
            "        not defined in [1]_, and here it is defined as ``median(y) -\n",
            "        slope*median(x)``, which is given in [3]_. Other definitions of\n",
            "        the intercept exist in the literature such as  ``median(y - slope*x)``\n",
            "        in [4]_. The approach to compute the intercept can be determined by the\n",
            "        parameter ``method``. A confidence interval for the intercept is not\n",
            "        given as this question is not addressed in [1]_.\n",
            "        \n",
            "        For compatibility with older versions of SciPy, the return value acts\n",
            "        like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\n",
            "        ``low_slope``, and ``high_slope``, so one can continue to write::\n",
            "        \n",
            "            slope, intercept, low_slope, high_slope = theilslopes(y, x)\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\n",
            "               Kendall's tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n",
            "        .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\n",
            "               regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\n",
            "               53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n",
            "        .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\n",
            "               John Wiley and Sons, New York, pp. 493.\n",
            "        .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        >>> x = np.linspace(-5, 5, num=150)\n",
            "        >>> y = x + np.random.normal(size=x.size)\n",
            "        >>> y[11:15] += 10  # add outliers\n",
            "        >>> y[-5:] -= 7\n",
            "        \n",
            "        Compute the slope, intercept and 90% confidence interval.  For comparison,\n",
            "        also compute the least-squares fit with `linregress`:\n",
            "        \n",
            "        >>> res = stats.theilslopes(y, x, 0.90, method='separate')\n",
            "        >>> lsq_res = stats.linregress(x, y)\n",
            "        \n",
            "        Plot the results. The Theil-Sen regression line is shown in red, with the\n",
            "        dashed red lines illustrating the confidence interval of the slope (note\n",
            "        that the dashed red lines are not the confidence interval of the regression\n",
            "        as the confidence interval of the intercept is not included). The green\n",
            "        line shows the least-squares fit for comparison.\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(x, y, 'b.')\n",
            "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
            "        >>> ax.plot(x, res[1] + res[2] * x, 'r--')\n",
            "        >>> ax.plot(x, res[1] + res[3] * x, 'r--')\n",
            "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
            "        >>> plt.show()\n",
            "    \n",
            "    tiecorrect(rankvals)\n",
            "        Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        rankvals : array_like\n",
            "            A 1-D sequence of ranks.  Typically this will be the array\n",
            "            returned by `~scipy.stats.rankdata`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        factor : float\n",
            "            Correction factor for U or H.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        rankdata : Assign ranks to the data\n",
            "        mannwhitneyu : Mann-Whitney rank test\n",
            "        kruskal : Kruskal-Wallis H test\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n",
            "               Sciences.  New York: McGraw-Hill.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import tiecorrect, rankdata\n",
            "        >>> tiecorrect([1, 2.5, 2.5, 4])\n",
            "        0.9\n",
            "        >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n",
            "        >>> ranks\n",
            "        array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n",
            "        >>> tiecorrect(ranks)\n",
            "        0.9833333333333333\n",
            "    \n",
            "    tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the trimmed maximum.\n",
            "        \n",
            "        This function computes the maximum value of an array along a given axis,\n",
            "        while ignoring values larger than a specified upper limit.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        upperlimit : None or float, optional\n",
            "            Values in the input array greater than the given limit will be ignored.\n",
            "            When upperlimit is None, then all values are used. The default value\n",
            "            is None.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        inclusive : {True, False}, optional\n",
            "            This flag determines whether values exactly equal to the upper limit\n",
            "            are included.  The default value is True.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tmax : float, int or ndarray\n",
            "            Trimmed maximum.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tmax(x)\n",
            "        19\n",
            "        \n",
            "        >>> stats.tmax(x, 13)\n",
            "        13\n",
            "        \n",
            "        >>> stats.tmax(x, 13, inclusive=False)\n",
            "        12\n",
            "    \n",
            "    tmean(a, limits=None, inclusive=(True, True), axis=None, *, nan_policy='propagate', keepdims=False)\n",
            "        Compute the trimmed mean.\n",
            "        \n",
            "        This function finds the arithmetic mean of given values, ignoring values\n",
            "        outside the given `limits`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        limits : None or (lower limit, upper limit), optional\n",
            "            Values in the input array less than the lower limit or greater than the\n",
            "            upper limit will be ignored.  When limits is None (default), then all\n",
            "            values are used.  Either of the limit values in the tuple can also be\n",
            "            None representing a half-open interval.\n",
            "        inclusive : (bool, bool), optional\n",
            "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
            "            determine whether values exactly equal to the lower or upper limits\n",
            "            are included.  The default value is (True, True).\n",
            "        axis : int or None, default: None\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tmean : ndarray\n",
            "            Trimmed mean.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`trim_mean`\n",
            "            Returns mean after trimming a proportion from both tails.\n",
            "        \n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tmean(x)\n",
            "        9.5\n",
            "        >>> stats.tmean(x, (3,17))\n",
            "        10.0\n",
            "    \n",
            "    tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate', *, keepdims=False)\n",
            "        Compute the trimmed minimum.\n",
            "        \n",
            "        This function finds the minimum value of an array `a` along the\n",
            "        specified axis, but only considering values greater than a specified\n",
            "        lower limit.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        lowerlimit : None or float, optional\n",
            "            Values in the input array less than the given limit will be ignored.\n",
            "            When lowerlimit is None, then all values are used. The default value\n",
            "            is None.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        inclusive : {True, False}, optional\n",
            "            This flag determines whether values exactly equal to the lower limit\n",
            "            are included.  The default value is True.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tmin : float, int or ndarray\n",
            "            Trimmed minimum.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tmin(x)\n",
            "        0\n",
            "        \n",
            "        >>> stats.tmin(x, 13)\n",
            "        13\n",
            "        \n",
            "        >>> stats.tmin(x, 13, inclusive=False)\n",
            "        14\n",
            "    \n",
            "    trim1(a, proportiontocut, tail='right', axis=0)\n",
            "        Slice off a proportion from ONE end of the passed array distribution.\n",
            "        \n",
            "        If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n",
            "        10% of scores. The lowest or highest values are trimmed (depending on\n",
            "        the tail).\n",
            "        Slice off less if proportion results in a non-integer slice index\n",
            "        (i.e. conservatively slices off `proportiontocut` ).\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        proportiontocut : float\n",
            "            Fraction to cut off of 'left' or 'right' of distribution.\n",
            "        tail : {'left', 'right'}, optional\n",
            "            Defaults to 'right'.\n",
            "        axis : int or None, optional\n",
            "            Axis along which to trim data. Default is 0. If None, compute over\n",
            "            the whole array `a`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        trim1 : ndarray\n",
            "            Trimmed version of array `a`. The order of the trimmed content is\n",
            "            undefined.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Create an array of 10 values and trim 20% of its lowest values:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "        >>> stats.trim1(a, 0.2, 'left')\n",
            "        array([2, 4, 3, 5, 6, 7, 8, 9])\n",
            "        \n",
            "        Note that the elements of the input array are trimmed by value, but the\n",
            "        output array is not necessarily sorted.\n",
            "        \n",
            "        The proportion to trim is rounded down to the nearest integer. For\n",
            "        instance, trimming 25% of the values from an array of 10 values will\n",
            "        return an array of 8 values:\n",
            "        \n",
            "        >>> b = np.arange(10)\n",
            "        >>> stats.trim1(b, 1/4).shape\n",
            "        (8,)\n",
            "        \n",
            "        Multidimensional arrays can be trimmed along any axis or across the entire\n",
            "        array:\n",
            "        \n",
            "        >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n",
            "        >>> d = np.array([a, b, c])\n",
            "        >>> stats.trim1(d, 0.8, axis=0).shape\n",
            "        (1, 10)\n",
            "        >>> stats.trim1(d, 0.8, axis=1).shape\n",
            "        (3, 2)\n",
            "        >>> stats.trim1(d, 0.8, axis=None).shape\n",
            "        (6,)\n",
            "    \n",
            "    trim_mean(a, proportiontocut, axis=0)\n",
            "        Return mean of array after trimming a specified fraction of extreme values\n",
            "        \n",
            "        Removes the specified proportion of elements from *each* end of the\n",
            "        sorted array, then computes the mean of the remaining elements.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        proportiontocut : float\n",
            "            Fraction of the most positive and most negative elements to remove.\n",
            "            When the specified proportion does not result in an integer number of\n",
            "            elements, the number of elements to trim is rounded down.\n",
            "        axis : int or None, default: 0\n",
            "            Axis along which the trimmed means are computed.\n",
            "            If None, compute over the raveled array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        trim_mean : ndarray\n",
            "            Mean of trimmed array.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        trimboth : Remove a proportion of elements from each end of an array.\n",
            "        tmean : Compute the mean after trimming values outside specified limits.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n",
            "        calculation::\n",
            "        \n",
            "            import numpy as np\n",
            "            a = np.sort(a)\n",
            "            m = int(proportiontocut * len(a))\n",
            "            np.mean(a[m: len(a) - m])\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = [1, 2, 3, 5]\n",
            "        >>> stats.trim_mean(x, 0.25)\n",
            "        2.5\n",
            "        \n",
            "        When the specified proportion does not result in an integer number of\n",
            "        elements, the number of elements to trim is rounded down.\n",
            "        \n",
            "        >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n",
            "        True\n",
            "        \n",
            "        Use `axis` to specify the axis along which the calculation is performed.\n",
            "        \n",
            "        >>> x2 = [[1, 2, 3, 5],\n",
            "        ...       [10, 20, 30, 50]]\n",
            "        >>> stats.trim_mean(x2, 0.25)\n",
            "        array([ 5.5, 11. , 16.5, 27.5])\n",
            "        >>> stats.trim_mean(x2, 0.25, axis=1)\n",
            "        array([ 2.5, 25. ])\n",
            "    \n",
            "    trimboth(a, proportiontocut, axis=0)\n",
            "        Slice off a proportion of items from both ends of an array.\n",
            "        \n",
            "        Slice off the passed proportion of items from both ends of the passed\n",
            "        array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n",
            "        rightmost 10% of scores). The trimmed values are the lowest and\n",
            "        highest ones.\n",
            "        Slice off less if proportion results in a non-integer slice index (i.e.\n",
            "        conservatively slices off `proportiontocut`).\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Data to trim.\n",
            "        proportiontocut : float\n",
            "            Proportion (in range 0-1) of total data set to trim of each end.\n",
            "        axis : int or None, optional\n",
            "            Axis along which to trim data. Default is 0. If None, compute over\n",
            "            the whole array `a`.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        out : ndarray\n",
            "            Trimmed version of array `a`. The order of the trimmed content\n",
            "            is undefined.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        trim_mean\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Create an array of 10 values and trim 10% of those values from each end:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "        >>> stats.trimboth(a, 0.1)\n",
            "        array([1, 3, 2, 4, 5, 6, 7, 8])\n",
            "        \n",
            "        Note that the elements of the input array are trimmed by value, but the\n",
            "        output array is not necessarily sorted.\n",
            "        \n",
            "        The proportion to trim is rounded down to the nearest integer. For\n",
            "        instance, trimming 25% of the values from each end of an array of 10\n",
            "        values will return an array of 6 values:\n",
            "        \n",
            "        >>> b = np.arange(10)\n",
            "        >>> stats.trimboth(b, 1/4).shape\n",
            "        (6,)\n",
            "        \n",
            "        Multidimensional arrays can be trimmed along any axis or across the entire\n",
            "        array:\n",
            "        \n",
            "        >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n",
            "        >>> d = np.array([a, b, c])\n",
            "        >>> stats.trimboth(d, 0.4, axis=0).shape\n",
            "        (1, 10)\n",
            "        >>> stats.trimboth(d, 0.4, axis=1).shape\n",
            "        (3, 2)\n",
            "        >>> stats.trimboth(d, 0.4, axis=None).shape\n",
            "        (6,)\n",
            "    \n",
            "    tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1, *, nan_policy='propagate', keepdims=False)\n",
            "        Compute the trimmed standard error of the mean.\n",
            "        \n",
            "        This function finds the standard error of the mean for given\n",
            "        values, ignoring values outside the given `limits`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        limits : None or (lower limit, upper limit), optional\n",
            "            Values in the input array less than the lower limit or greater than the\n",
            "            upper limit will be ignored. When limits is None, then all values are\n",
            "            used. Either of the limit values in the tuple can also be None\n",
            "            representing a half-open interval.  The default value is None.\n",
            "        inclusive : (bool, bool), optional\n",
            "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
            "            determine whether values exactly equal to the lower or upper limits\n",
            "            are included.  The default value is (True, True).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        ddof : int, optional\n",
            "            Delta degrees of freedom.  Default is 1.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tsem : float\n",
            "            Trimmed standard error of the mean.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `tsem` uses unbiased sample standard deviation, i.e. it uses a\n",
            "        correction factor ``n / (n - 1)``.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tsem(x)\n",
            "        1.3228756555322954\n",
            "        >>> stats.tsem(x, (3,17))\n",
            "        1.1547005383792515\n",
            "    \n",
            "    tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1, *, nan_policy='propagate', keepdims=False)\n",
            "        Compute the trimmed sample standard deviation.\n",
            "        \n",
            "        This function finds the sample standard deviation of given values,\n",
            "        ignoring values outside the given `limits`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        limits : None or (lower limit, upper limit), optional\n",
            "            Values in the input array less than the lower limit or greater than the\n",
            "            upper limit will be ignored. When limits is None, then all values are\n",
            "            used. Either of the limit values in the tuple can also be None\n",
            "            representing a half-open interval.  The default value is None.\n",
            "        inclusive : (bool, bool), optional\n",
            "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
            "            determine whether values exactly equal to the lower or upper limits\n",
            "            are included.  The default value is (True, True).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        ddof : int, optional\n",
            "            Delta degrees of freedom.  Default is 1.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tstd : float\n",
            "            Trimmed sample standard deviation.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n",
            "        correction factor ``n / (n - 1)``.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tstd(x)\n",
            "        5.9160797830996161\n",
            "        >>> stats.tstd(x, (3,17))\n",
            "        4.4721359549995796\n",
            "    \n",
            "    ttest_1samp(a, popmean, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
            "        Calculate the T-test for the mean of ONE group of scores.\n",
            "        \n",
            "        This is a test for the null hypothesis that the expected value\n",
            "        (mean) of a sample of independent observations `a` is equal to the given\n",
            "        population mean, `popmean`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Sample observations.\n",
            "        popmean : float or array_like\n",
            "            Expected value in null hypothesis. If array_like, then its length along\n",
            "            `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "            \n",
            "            * 'two-sided': the mean of the underlying distribution of the sample\n",
            "              is different than the given population mean (`popmean`)\n",
            "            * 'less': the mean of the underlying distribution of the sample is\n",
            "              less than the given population mean (`popmean`)\n",
            "            * 'greater': the mean of the underlying distribution of the sample is\n",
            "              greater than the given population mean (`popmean`)\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.TtestResult`\n",
            "            An object with the following attributes:\n",
            "            \n",
            "            statistic : float or array\n",
            "                The t-statistic.\n",
            "            pvalue : float or array\n",
            "                The p-value associated with the given alternative.\n",
            "            df : float or array\n",
            "                The number of degrees of freedom used in calculation of the\n",
            "                t-statistic; this is one less than the size of the sample\n",
            "                (``a.shape[axis]``).\n",
            "            \n",
            "                .. versionadded:: 1.10.0\n",
            "            \n",
            "            The object also has the following method:\n",
            "            \n",
            "            confidence_interval(confidence_level=0.95)\n",
            "                Computes a confidence interval around the population\n",
            "                mean for the given confidence level.\n",
            "                The confidence interval is returned in a ``namedtuple`` with\n",
            "                fields `low` and `high`.\n",
            "            \n",
            "                .. versionadded:: 1.10.0\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n",
            "        ``se`` is the standard error. Therefore, the statistic will be positive\n",
            "        when the sample mean is greater than the population mean and negative when\n",
            "        the sample mean is less than the population mean.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we wish to test the null hypothesis that the mean of a population\n",
            "        is equal to 0.5. We choose a confidence level of 99%; that is, we will\n",
            "        reject the null hypothesis in favor of the alternative if the p-value is\n",
            "        less than 0.01.\n",
            "        \n",
            "        When testing random variates from the standard uniform distribution, which\n",
            "        has a mean of 0.5, we expect the data to be consistent with the null\n",
            "        hypothesis most of the time.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n",
            "        >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
            "        TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n",
            "        \n",
            "        As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n",
            "        we cannot reject the null hypothesis.\n",
            "        \n",
            "        When testing data from the standard *normal* distribution, which has a mean\n",
            "        of 0, we would expect the null hypothesis to be rejected.\n",
            "        \n",
            "        >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
            "        >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
            "        TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n",
            "        \n",
            "        Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n",
            "        null hypothesis in favor of the default \"two-sided\" alternative: the mean\n",
            "        of the population is *not* equal to 0.5.\n",
            "        \n",
            "        However, suppose we were to test the null hypothesis against the\n",
            "        one-sided alternative that the mean of the population is *greater* than\n",
            "        0.5. Since the mean of the standard normal is less than 0.5, we would not\n",
            "        expect the null hypothesis to be rejected.\n",
            "        \n",
            "        >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n",
            "        TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n",
            "        \n",
            "        Unsurprisingly, with a p-value greater than our threshold, we would not\n",
            "        reject the null hypothesis.\n",
            "        \n",
            "        Note that when working with a confidence level of 99%, a true null\n",
            "        hypothesis will be rejected approximately 1% of the time.\n",
            "        \n",
            "        >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n",
            "        >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n",
            "        >>> np.sum(res.pvalue < 0.01)\n",
            "        1\n",
            "        \n",
            "        Indeed, even though all 100 samples above were drawn from the standard\n",
            "        uniform distribution, which *does* have a population mean of 0.5, we would\n",
            "        mistakenly reject the null hypothesis for one of them.\n",
            "        \n",
            "        `ttest_1samp` can also compute a confidence interval around the population\n",
            "        mean.\n",
            "        \n",
            "        >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
            "        >>> res = stats.ttest_1samp(rvs, popmean=0)\n",
            "        >>> ci = res.confidence_interval(confidence_level=0.95)\n",
            "        >>> ci\n",
            "        ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n",
            "        \n",
            "        The bounds of the 95% confidence interval are the\n",
            "        minimum and maximum values of the parameter `popmean` for which the\n",
            "        p-value of the test would be 0.05.\n",
            "        \n",
            "        >>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n",
            "        >>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
            "        >>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n",
            "        >>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
            "        \n",
            "        Under certain assumptions about the population from which a sample\n",
            "        is drawn, the confidence interval with confidence level 95% is expected\n",
            "        to contain the true population mean in 95% of sample replications.\n",
            "        \n",
            "        >>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n",
            "        >>> res = stats.ttest_1samp(rvs, popmean=0)\n",
            "        >>> ci = res.confidence_interval()\n",
            "        >>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n",
            "        >>> contains_pop_mean.sum()\n",
            "        953\n",
            "    \n",
            "    ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, alternative='two-sided', trim=0, *, keepdims=False)\n",
            "        Calculate the T-test for the means of *two independent* samples of scores.\n",
            "        \n",
            "        This is a test for the null hypothesis that 2 independent samples\n",
            "        have identical average (expected) values. This test assumes that the\n",
            "        populations have identical variances by default.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a, b : array_like\n",
            "            The arrays must have the same shape, except in the dimension\n",
            "            corresponding to `axis` (the first, by default).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        equal_var : bool, optional\n",
            "            If True (default), perform a standard independent 2 sample test\n",
            "            that assumes equal population variances [1]_.\n",
            "            If False, perform Welch's t-test, which does not assume equal\n",
            "            population variance [2]_.\n",
            "            \n",
            "            .. versionadded:: 0.11.0\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        permutations : non-negative int, np.inf, or None (default), optional\n",
            "            If 0 or None (default), use the t-distribution to calculate p-values.\n",
            "            Otherwise, `permutations` is  the number of random permutations that\n",
            "            will be used to estimate p-values using a permutation test. If\n",
            "            `permutations` equals or exceeds the number of distinct partitions of\n",
            "            the pooled data, an exact test is performed instead (i.e. each\n",
            "            distinct partition is used exactly once). See Notes for details.\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        random_state : {None, int, `numpy.random.Generator`,\n",
            "                `numpy.random.RandomState`}, optional\n",
            "            \n",
            "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
            "            singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
            "            seeded with `seed`.\n",
            "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
            "            that instance is used.\n",
            "            \n",
            "            Pseudorandom number generator state used to generate permutations\n",
            "            (used only when `permutations` is not None).\n",
            "            \n",
            "            .. versionadded:: 1.7.0\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "            \n",
            "            * 'two-sided': the means of the distributions underlying the samples\n",
            "              are unequal.\n",
            "            * 'less': the mean of the distribution underlying the first sample\n",
            "              is less than the mean of the distribution underlying the second\n",
            "              sample.\n",
            "            * 'greater': the mean of the distribution underlying the first\n",
            "              sample is greater than the mean of the distribution underlying\n",
            "              the second sample.\n",
            "            \n",
            "            .. versionadded:: 1.6.0\n",
            "        trim : float, optional\n",
            "            If nonzero, performs a trimmed (Yuen's) t-test.\n",
            "            Defines the fraction of elements to be trimmed from each end of the\n",
            "            input samples. If 0 (default), no elements will be trimmed from either\n",
            "            side. The number of trimmed elements from each tail is the floor of the\n",
            "            trim times the number of elements. Valid range is [0, .5).\n",
            "            \n",
            "            .. versionadded:: 1.7\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.TtestResult`\n",
            "            An object with the following attributes:\n",
            "            \n",
            "            statistic : float or ndarray\n",
            "                The t-statistic.\n",
            "            pvalue : float or ndarray\n",
            "                The p-value associated with the given alternative.\n",
            "            df : float or ndarray\n",
            "                The number of degrees of freedom used in calculation of the\n",
            "                t-statistic. This is always NaN for a permutation t-test.\n",
            "            \n",
            "                .. versionadded:: 1.11.0\n",
            "            \n",
            "            The object also has the following method:\n",
            "            \n",
            "            confidence_interval(confidence_level=0.95)\n",
            "                Computes a confidence interval around the difference in\n",
            "                population means for the given confidence level.\n",
            "                The confidence interval is returned in a ``namedtuple`` with\n",
            "                fields ``low`` and ``high``.\n",
            "                When a permutation t-test is performed, the confidence interval\n",
            "                is not computed, and fields ``low`` and ``high`` contain NaN.\n",
            "            \n",
            "                .. versionadded:: 1.11.0\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Suppose we observe two independent samples, e.g. flower petal lengths, and\n",
            "        we are considering whether the two samples were drawn from the same\n",
            "        population (e.g. the same species of flower or two species with similar\n",
            "        petal characteristics) or two different populations.\n",
            "        \n",
            "        The t-test quantifies the difference between the arithmetic means\n",
            "        of the two samples. The p-value quantifies the probability of observing\n",
            "        as or more extreme values assuming the null hypothesis, that the\n",
            "        samples are drawn from populations with the same population means, is true.\n",
            "        A p-value larger than a chosen threshold (e.g. 5% or 1%) indicates that\n",
            "        our observation is not so unlikely to have occurred by chance. Therefore,\n",
            "        we do not reject the null hypothesis of equal population means.\n",
            "        If the p-value is smaller than our threshold, then we have evidence\n",
            "        against the null hypothesis of equal population means.\n",
            "        \n",
            "        By default, the p-value is determined by comparing the t-statistic of the\n",
            "        observed data against a theoretical t-distribution.\n",
            "        When ``1 < permutations < binom(n, k)``, where\n",
            "        \n",
            "        * ``k`` is the number of observations in `a`,\n",
            "        * ``n`` is the total number of observations in `a` and `b`, and\n",
            "        * ``binom(n, k)`` is the binomial coefficient (``n`` choose ``k``),\n",
            "        \n",
            "        the data are pooled (concatenated), randomly assigned to either group `a`\n",
            "        or `b`, and the t-statistic is calculated. This process is performed\n",
            "        repeatedly (`permutation` times), generating a distribution of the\n",
            "        t-statistic under the null hypothesis, and the t-statistic of the observed\n",
            "        data is compared to this distribution to determine the p-value.\n",
            "        Specifically, the p-value reported is the \"achieved significance level\"\n",
            "        (ASL) as defined in 4.4 of [3]_. Note that there are other ways of\n",
            "        estimating p-values using randomized permutation tests; for other\n",
            "        options, see the more general `permutation_test`.\n",
            "        \n",
            "        When ``permutations >= binom(n, k)``, an exact test is performed: the data\n",
            "        are partitioned between the groups in each distinct way exactly once.\n",
            "        \n",
            "        The permutation test can be computationally expensive and not necessarily\n",
            "        more accurate than the analytical test, but it does not make strong\n",
            "        assumptions about the shape of the underlying distribution.\n",
            "        \n",
            "        Use of trimming is commonly referred to as the trimmed t-test. At times\n",
            "        called Yuen's t-test, this is an extension of Welch's t-test, with the\n",
            "        difference being the use of winsorized means in calculation of the variance\n",
            "        and the trimmed sample size in calculation of the statistic. Trimming is\n",
            "        recommended if the underlying distribution is long-tailed or contaminated\n",
            "        with outliers [4]_.\n",
            "        \n",
            "        The statistic is calculated as ``(np.mean(a) - np.mean(b))/se``, where\n",
            "        ``se`` is the standard error. Therefore, the statistic will be positive\n",
            "        when the sample mean of `a` is greater than the sample mean of `b` and\n",
            "        negative when the sample mean of `a` is less than the sample mean of\n",
            "        `b`.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
            "        \n",
            "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
            "        \n",
            "        .. [3] B. Efron and T. Hastie. Computer Age Statistical Inference. (2016).\n",
            "        \n",
            "        .. [4] Yuen, Karen K. \"The Two-Sample Trimmed t for Unequal Population\n",
            "               Variances.\" Biometrika, vol. 61, no. 1, 1974, pp. 165-170. JSTOR,\n",
            "               www.jstor.org/stable/2334299. Accessed 30 Mar. 2021.\n",
            "        \n",
            "        .. [5] Yuen, Karen K., and W. J. Dixon. \"The Approximate Behaviour and\n",
            "               Performance of the Two-Sample Trimmed t.\" Biometrika, vol. 60,\n",
            "               no. 2, 1973, pp. 369-374. JSTOR, www.jstor.org/stable/2334550.\n",
            "               Accessed 30 Mar. 2021.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        \n",
            "        Test with sample with identical means:\n",
            "        \n",
            "        >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
            "        >>> rvs2 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
            "        >>> stats.ttest_ind(rvs1, rvs2)\n",
            "        Ttest_indResult(statistic=-0.4390847099199348, pvalue=0.6606952038870015)\n",
            "        >>> stats.ttest_ind(rvs1, rvs2, equal_var=False)\n",
            "        Ttest_indResult(statistic=-0.4390847099199348, pvalue=0.6606952553131064)\n",
            "        \n",
            "        `ttest_ind` underestimates p for unequal variances:\n",
            "        \n",
            "        >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500, random_state=rng)\n",
            "        >>> stats.ttest_ind(rvs1, rvs3)\n",
            "        Ttest_indResult(statistic=-1.6370984482905417, pvalue=0.1019251574705033)\n",
            "        >>> stats.ttest_ind(rvs1, rvs3, equal_var=False)\n",
            "        Ttest_indResult(statistic=-1.637098448290542, pvalue=0.10202110497954867)\n",
            "        \n",
            "        When ``n1 != n2``, the equal variance t-statistic is no longer equal to the\n",
            "        unequal variance t-statistic:\n",
            "        \n",
            "        >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100, random_state=rng)\n",
            "        >>> stats.ttest_ind(rvs1, rvs4)\n",
            "        Ttest_indResult(statistic=-1.9481646859513422, pvalue=0.05186270935842703)\n",
            "        >>> stats.ttest_ind(rvs1, rvs4, equal_var=False)\n",
            "        Ttest_indResult(statistic=-1.3146566100751664, pvalue=0.1913495266513811)\n",
            "        \n",
            "        T-test with different means, variance, and n:\n",
            "        \n",
            "        >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100, random_state=rng)\n",
            "        >>> stats.ttest_ind(rvs1, rvs5)\n",
            "        Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0046418707568707885)\n",
            "        >>> stats.ttest_ind(rvs1, rvs5, equal_var=False)\n",
            "        Ttest_indResult(statistic=-1.8686598649188084, pvalue=0.06434714193919686)\n",
            "        \n",
            "        When performing a permutation test, more permutations typically yields\n",
            "        more accurate results. Use a ``np.random.Generator`` to ensure\n",
            "        reproducibility:\n",
            "        \n",
            "        >>> stats.ttest_ind(rvs1, rvs5, permutations=10000,\n",
            "        ...                 random_state=rng)\n",
            "        Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0052994700529947)\n",
            "        \n",
            "        Take these two samples, one of which has an extreme tail.\n",
            "        \n",
            "        >>> a = (56, 128.6, 12, 123.8, 64.34, 78, 763.3)\n",
            "        >>> b = (1.1, 2.9, 4.2)\n",
            "        \n",
            "        Use the `trim` keyword to perform a trimmed (Yuen) t-test. For example,\n",
            "        using 20% trimming, ``trim=.2``, the test will reduce the impact of one\n",
            "        (``np.floor(trim*len(a))``) element from each tail of sample `a`. It will\n",
            "        have no effect on sample `b` because ``np.floor(trim*len(b))`` is 0.\n",
            "        \n",
            "        >>> stats.ttest_ind(a, b, trim=.2)\n",
            "        Ttest_indResult(statistic=3.4463884028073513,\n",
            "                        pvalue=0.01369338726499547)\n",
            "    \n",
            "    ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2, equal_var=True, alternative='two-sided')\n",
            "        T-test for means of two independent samples from descriptive statistics.\n",
            "        \n",
            "        This is a test for the null hypothesis that two independent\n",
            "        samples have identical average (expected) values.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        mean1 : array_like\n",
            "            The mean(s) of sample 1.\n",
            "        std1 : array_like\n",
            "            The corrected sample standard deviation of sample 1 (i.e. ``ddof=1``).\n",
            "        nobs1 : array_like\n",
            "            The number(s) of observations of sample 1.\n",
            "        mean2 : array_like\n",
            "            The mean(s) of sample 2.\n",
            "        std2 : array_like\n",
            "            The corrected sample standard deviation of sample 2 (i.e. ``ddof=1``).\n",
            "        nobs2 : array_like\n",
            "            The number(s) of observations of sample 2.\n",
            "        equal_var : bool, optional\n",
            "            If True (default), perform a standard independent 2 sample test\n",
            "            that assumes equal population variances [1]_.\n",
            "            If False, perform Welch's t-test, which does not assume equal\n",
            "            population variance [2]_.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "        \n",
            "            * 'two-sided': the means of the distributions are unequal.\n",
            "            * 'less': the mean of the first distribution is less than the\n",
            "              mean of the second distribution.\n",
            "            * 'greater': the mean of the first distribution is greater than the\n",
            "              mean of the second distribution.\n",
            "        \n",
            "            .. versionadded:: 1.6.0\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        statistic : float or array\n",
            "            The calculated t-statistics.\n",
            "        pvalue : float or array\n",
            "            The two-tailed p-value.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.ttest_ind\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The statistic is calculated as ``(mean1 - mean2)/se``, where ``se`` is the\n",
            "        standard error. Therefore, the statistic will be positive when `mean1` is\n",
            "        greater than `mean2` and negative when `mean1` is less than `mean2`.\n",
            "        \n",
            "        This method does not check whether any of the elements of `std1` or `std2`\n",
            "        are negative. If any elements of the `std1` or `std2` parameters are\n",
            "        negative in a call to this method, this method will return the same result\n",
            "        as if it were passed ``numpy.abs(std1)`` and ``numpy.abs(std2)``,\n",
            "        respectively, instead; no exceptions or warnings will be emitted.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
            "        \n",
            "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Suppose we have the summary data for two samples, as follows (with the\n",
            "        Sample Variance being the corrected sample variance)::\n",
            "        \n",
            "                             Sample   Sample\n",
            "                       Size   Mean   Variance\n",
            "            Sample 1    13    15.0     87.5\n",
            "            Sample 2    11    12.0     39.0\n",
            "        \n",
            "        Apply the t-test to this data (with the assumption that the population\n",
            "        variances are equal):\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ttest_ind_from_stats\n",
            "        >>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,\n",
            "        ...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)\n",
            "        Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)\n",
            "        \n",
            "        For comparison, here is the data from which those summary statistics\n",
            "        were taken.  With this data, we can compute the same result using\n",
            "        `scipy.stats.ttest_ind`:\n",
            "        \n",
            "        >>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])\n",
            "        >>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])\n",
            "        >>> from scipy.stats import ttest_ind\n",
            "        >>> ttest_ind(a, b)\n",
            "        Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)\n",
            "        \n",
            "        Suppose we instead have binary data and would like to apply a t-test to\n",
            "        compare the proportion of 1s in two independent groups::\n",
            "        \n",
            "                              Number of    Sample     Sample\n",
            "                        Size    ones        Mean     Variance\n",
            "            Sample 1    150      30         0.2        0.161073\n",
            "            Sample 2    200      45         0.225      0.175251\n",
            "        \n",
            "        The sample mean :math:`\\hat{p}` is the proportion of ones in the sample\n",
            "        and the variance for a binary observation is estimated by\n",
            "        :math:`\\hat{p}(1-\\hat{p})`.\n",
            "        \n",
            "        >>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.161073), nobs1=150,\n",
            "        ...                      mean2=0.225, std2=np.sqrt(0.175251), nobs2=200)\n",
            "        Ttest_indResult(statistic=-0.5627187905196761, pvalue=0.5739887114209541)\n",
            "        \n",
            "        For comparison, we could compute the t statistic and p-value using\n",
            "        arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.\n",
            "        \n",
            "        >>> group1 = np.array([1]*30 + [0]*(150-30))\n",
            "        >>> group2 = np.array([1]*45 + [0]*(200-45))\n",
            "        >>> ttest_ind(group1, group2)\n",
            "        Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)\n",
            "    \n",
            "    ttest_rel(a, b, axis=0, nan_policy='propagate', alternative='two-sided', *, keepdims=False)\n",
            "        Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
            "        \n",
            "        This is a test for the null hypothesis that two related or\n",
            "        repeated samples have identical average (expected) values.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a, b : array_like\n",
            "            The arrays must have the same shape.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
            "            Defines the alternative hypothesis.\n",
            "            The following options are available (default is 'two-sided'):\n",
            "            \n",
            "            * 'two-sided': the means of the distributions underlying the samples\n",
            "              are unequal.\n",
            "            * 'less': the mean of the distribution underlying the first sample\n",
            "              is less than the mean of the distribution underlying the second\n",
            "              sample.\n",
            "            * 'greater': the mean of the distribution underlying the first\n",
            "              sample is greater than the mean of the distribution underlying\n",
            "              the second sample.\n",
            "            \n",
            "            .. versionadded:: 1.6.0\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.TtestResult`\n",
            "            An object with the following attributes:\n",
            "            \n",
            "            statistic : float or array\n",
            "                The t-statistic.\n",
            "            pvalue : float or array\n",
            "                The p-value associated with the given alternative.\n",
            "            df : float or array\n",
            "                The number of degrees of freedom used in calculation of the\n",
            "                t-statistic; this is one less than the size of the sample\n",
            "                (``a.shape[axis]``).\n",
            "            \n",
            "                .. versionadded:: 1.10.0\n",
            "            \n",
            "            The object also has the following method:\n",
            "            \n",
            "            confidence_interval(confidence_level=0.95)\n",
            "                Computes a confidence interval around the difference in\n",
            "                population means for the given confidence level.\n",
            "                The confidence interval is returned in a ``namedtuple`` with\n",
            "                fields `low` and `high`.\n",
            "            \n",
            "                .. versionadded:: 1.10.0\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Examples for use are scores of the same set of student in\n",
            "        different exams, or repeated sampling from the same units. The\n",
            "        test measures whether the average score differs significantly\n",
            "        across samples (e.g. exams). If we observe a large p-value, for\n",
            "        example greater than 0.05 or 0.1 then we cannot reject the null\n",
            "        hypothesis of identical average scores. If the p-value is smaller\n",
            "        than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n",
            "        hypothesis of equal averages. Small p-values are associated with\n",
            "        large t-statistics.\n",
            "        \n",
            "        The t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\n",
            "        standard error. Therefore, the t-statistic will be positive when the sample\n",
            "        mean of ``a - b`` is greater than zero and negative when the sample mean of\n",
            "        ``a - b`` is less than zero.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> rng = np.random.default_rng()\n",
            "        \n",
            "        >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
            "        >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
            "        ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
            "        >>> stats.ttest_rel(rvs1, rvs2)\n",
            "        TtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n",
            "        >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n",
            "        ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
            "        >>> stats.ttest_rel(rvs1, rvs3)\n",
            "        TtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n",
            "    \n",
            "    tukey_hsd(*args)\n",
            "        Perform Tukey's HSD test for equality of means over multiple treatments.\n",
            "        \n",
            "        Tukey's honestly significant difference (HSD) test performs pairwise\n",
            "        comparison of means for a set of samples. Whereas ANOVA (e.g. `f_oneway`)\n",
            "        assesses whether the true means underlying each sample are identical,\n",
            "        Tukey's HSD is a post hoc test used to compare the mean of each sample\n",
            "        to the mean of each other sample.\n",
            "        \n",
            "        The null hypothesis is that the distributions underlying the samples all\n",
            "        have the same mean. The test statistic, which is computed for every\n",
            "        possible pairing of samples, is simply the difference between the sample\n",
            "        means. For each pair, the p-value is the probability under the null\n",
            "        hypothesis (and other assumptions; see notes) of observing such an extreme\n",
            "        value of the statistic, considering that many pairwise comparisons are\n",
            "        being performed. Confidence intervals for the difference between each pair\n",
            "        of means are also available.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        sample1, sample2, ... : array_like\n",
            "            The sample measurements for each group. There must be at least\n",
            "            two arguments.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        result : `~scipy.stats._result_classes.TukeyHSDResult` instance\n",
            "            The return value is an object with the following attributes:\n",
            "        \n",
            "            statistic : float ndarray\n",
            "                The computed statistic of the test for each comparison. The element\n",
            "                at index ``(i, j)`` is the statistic for the comparison between\n",
            "                groups ``i`` and ``j``.\n",
            "            pvalue : float ndarray\n",
            "                The computed p-value of the test for each comparison. The element\n",
            "                at index ``(i, j)`` is the p-value for the comparison between\n",
            "                groups ``i`` and ``j``.\n",
            "        \n",
            "            The object has the following methods:\n",
            "        \n",
            "            confidence_interval(confidence_level=0.95):\n",
            "                Compute the confidence interval for the specified confidence level.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        dunnett : performs comparison of means against a control group.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The use of this test relies on several assumptions.\n",
            "        \n",
            "        1. The observations are independent within and among groups.\n",
            "        2. The observations within each group are normally distributed.\n",
            "        3. The distributions from which the samples are drawn have the same finite\n",
            "           variance.\n",
            "        \n",
            "        The original formulation of the test was for samples of equal size [6]_.\n",
            "        In case of unequal sample sizes, the test uses the Tukey-Kramer method\n",
            "        [4]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n",
            "               Method.\"\n",
            "               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n",
            "               28 November 2020.\n",
            "        .. [2] Abdi, Herve & Williams, Lynne. (2021). \"Tukey's Honestly Significant\n",
            "               Difference (HSD) Test.\"\n",
            "               https://personal.utdallas.edu/~herve/abdi-HSD2010-pretty.pdf\n",
            "        .. [3] \"One-Way ANOVA Using SAS PROC ANOVA & PROC GLM.\" SAS\n",
            "               Tutorials, 2007, www.stattutorials.com/SAS/TUTORIAL-PROC-GLM.htm.\n",
            "        .. [4] Kramer, Clyde Young. \"Extension of Multiple Range Tests to Group\n",
            "               Means with Unequal Numbers of Replications.\" Biometrics, vol. 12,\n",
            "               no. 3, 1956, pp. 307-310. JSTOR, www.jstor.org/stable/3001469.\n",
            "               Accessed 25 May 2021.\n",
            "        .. [5] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.3.3.\n",
            "               The ANOVA table and tests of hypotheses about means\"\n",
            "               https://www.itl.nist.gov/div898/handbook/prc/section4/prc433.htm,\n",
            "               2 June 2021.\n",
            "        .. [6] Tukey, John W. \"Comparing Individual Means in the Analysis of\n",
            "               Variance.\" Biometrics, vol. 5, no. 2, 1949, pp. 99-114. JSTOR,\n",
            "               www.jstor.org/stable/3001913. Accessed 14 June 2021.\n",
            "        \n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Here are some data comparing the time to relief of three brands of\n",
            "        headache medicine, reported in minutes. Data adapted from [3]_.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import tukey_hsd\n",
            "        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n",
            "        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n",
            "        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n",
            "        \n",
            "        We would like to see if the means between any of the groups are\n",
            "        significantly different. First, visually examine a box and whisker plot.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.boxplot([group0, group1, group2])\n",
            "        >>> ax.set_xticklabels([\"group0\", \"group1\", \"group2\"]) # doctest: +SKIP\n",
            "        >>> ax.set_ylabel(\"mean\") # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "        \n",
            "        From the box and whisker plot, we can see overlap in the interquartile\n",
            "        ranges group 1 to group 2 and group 3, but we can apply the ``tukey_hsd``\n",
            "        test to determine if the difference between means is significant. We\n",
            "        set a significance level of .05 to reject the null hypothesis.\n",
            "        \n",
            "        >>> res = tukey_hsd(group0, group1, group2)\n",
            "        >>> print(res)\n",
            "        Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\n",
            "        Comparison  Statistic  p-value   Lower CI   Upper CI\n",
            "        (0 - 1)     -4.600      0.014     -8.249     -0.951\n",
            "        (0 - 2)     -0.260      0.980     -3.909      3.389\n",
            "        (1 - 0)      4.600      0.014      0.951      8.249\n",
            "        (1 - 2)      4.340      0.020      0.691      7.989\n",
            "        (2 - 0)      0.260      0.980     -3.389      3.909\n",
            "        (2 - 1)     -4.340      0.020     -7.989     -0.691\n",
            "        \n",
            "        The null hypothesis is that each group has the same mean. The p-value for\n",
            "        comparisons between ``group0`` and ``group1`` as well as ``group1`` and\n",
            "        ``group2`` do not exceed .05, so we reject the null hypothesis that they\n",
            "        have the same means. The p-value of the comparison between ``group0``\n",
            "        and ``group2`` exceeds .05, so we accept the null hypothesis that there\n",
            "        is not a significant difference between their means.\n",
            "        \n",
            "        We can also compute the confidence interval associated with our chosen\n",
            "        confidence level.\n",
            "        \n",
            "        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n",
            "        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n",
            "        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n",
            "        >>> result = tukey_hsd(group0, group1, group2)\n",
            "        >>> conf = res.confidence_interval(confidence_level=.99)\n",
            "        >>> for ((i, j), l) in np.ndenumerate(conf.low):\n",
            "        ...     # filter out self comparisons\n",
            "        ...     if i != j:\n",
            "        ...         h = conf.high[i,j]\n",
            "        ...         print(f\"({i} - {j}) {l:>6.3f} {h:>6.3f}\")\n",
            "        (0 - 1) -9.480  0.280\n",
            "        (0 - 2) -5.140  4.620\n",
            "        (1 - 0) -0.280  9.480\n",
            "        (1 - 2) -0.540  9.220\n",
            "        (2 - 0) -4.620  5.140\n",
            "        (2 - 1) -9.220  0.540\n",
            "    \n",
            "    tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1, *, nan_policy='propagate', keepdims=False)\n",
            "        Compute the trimmed variance.\n",
            "        \n",
            "        This function computes the sample variance of an array of values,\n",
            "        while ignoring values which are outside of given `limits`.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Array of values.\n",
            "        limits : None or (lower limit, upper limit), optional\n",
            "            Values in the input array less than the lower limit or greater than the\n",
            "            upper limit will be ignored. When limits is None, then all values are\n",
            "            used. Either of the limit values in the tuple can also be None\n",
            "            representing a half-open interval.  The default value is None.\n",
            "        inclusive : (bool, bool), optional\n",
            "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
            "            determine whether values exactly equal to the lower or upper limits\n",
            "            are included.  The default value is (True, True).\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        ddof : int, optional\n",
            "            Delta degrees of freedom.  Default is 1.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        tvar : float\n",
            "            Trimmed variance.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `tvar` computes the unbiased sample variance, i.e. it uses a correction\n",
            "        factor ``n / (n - 1)``.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = np.arange(20)\n",
            "        >>> stats.tvar(x)\n",
            "        35.0\n",
            "        >>> stats.tvar(x, (3,17))\n",
            "        20.0\n",
            "    \n",
            "    variation(a, axis=0, nan_policy='propagate', ddof=0, *, keepdims=False)\n",
            "        Compute the coefficient of variation.\n",
            "        \n",
            "        The coefficient of variation is the standard deviation divided by the\n",
            "        mean.  This function is equivalent to::\n",
            "        \n",
            "            np.std(x, axis=axis, ddof=ddof) / np.mean(x)\n",
            "        \n",
            "        The default for ``ddof`` is 0, but many definitions of the coefficient\n",
            "        of variation use the square root of the unbiased sample variance\n",
            "        for the sample standard deviation, which corresponds to ``ddof=1``.\n",
            "        \n",
            "        The function does not take the absolute value of the mean of the data,\n",
            "        so the return value is negative if the mean is negative.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            Input array.\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        ddof : int, optional\n",
            "            Gives the \"Delta Degrees Of Freedom\" used when computing the\n",
            "            standard deviation.  The divisor used in the calculation of the\n",
            "            standard deviation is ``N - ddof``, where ``N`` is the number of\n",
            "            elements.  `ddof` must be less than ``N``; if it isn't, the result\n",
            "            will be ``nan`` or ``inf``, depending on ``N`` and the values in\n",
            "            the array.  By default `ddof` is zero for backwards compatibility,\n",
            "            but it is recommended to use ``ddof=1`` to ensure that the sample\n",
            "            standard deviation is computed as the square root of the unbiased\n",
            "            sample variance.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        variation : ndarray\n",
            "            The calculated variation along the requested axis.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        There are several edge cases that are handled without generating a\n",
            "        warning:\n",
            "        \n",
            "        * If both the mean and the standard deviation are zero, ``nan``\n",
            "          is returned.\n",
            "        * If the mean is zero and the standard deviation is nonzero, ``inf``\n",
            "          is returned.\n",
            "        * If the input has length zero (either because the array has zero\n",
            "          length, or all the input values are ``nan`` and ``nan_policy`` is\n",
            "          ``'omit'``), ``nan`` is returned.\n",
            "        * If the input contains ``inf``, ``nan`` is returned.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
            "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
            "           York. 2000.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import variation\n",
            "        >>> variation([1, 2, 3, 4, 5], ddof=1)\n",
            "        0.5270462766947299\n",
            "        \n",
            "        Compute the variation along a given dimension of an array that contains\n",
            "        a few ``nan`` values:\n",
            "        \n",
            "        >>> x = np.array([[  10.0, np.nan, 11.0, 19.0, 23.0, 29.0, 98.0],\n",
            "        ...               [  29.0,   30.0, 32.0, 33.0, 35.0, 56.0, 57.0],\n",
            "        ...               [np.nan, np.nan, 12.0, 13.0, 16.0, 16.0, 17.0]])\n",
            "        >>> variation(x, axis=1, ddof=1, nan_policy='omit')\n",
            "        array([1.05109361, 0.31428986, 0.146483  ])\n",
            "    \n",
            "    wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
            "        Compute the Wasserstein-1 distance between two 1D discrete distributions.\n",
            "        \n",
            "        The Wasserstein distance, also called the Earth mover's distance or the\n",
            "        optimal transport distance, is a similarity metric between two probability\n",
            "        distributions [1]_. In the discrete case, the Wasserstein distance can be\n",
            "        understood as the cost of an optimal transport plan to convert one\n",
            "        distribution into the other. The cost is calculated as the product of the\n",
            "        amount of probability mass being moved and the distance it is being moved.\n",
            "        A brief and intuitive introduction can be found at [2]_.\n",
            "        \n",
            "        .. versionadded:: 1.0.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        u_values : 1d array_like\n",
            "            A sample from a probability distribution or the support (set of all\n",
            "            possible values) of a probability distribution. Each element is an\n",
            "            observation or possible value.\n",
            "        \n",
            "        v_values : 1d array_like\n",
            "            A sample from or the support of a second distribution.\n",
            "        \n",
            "        u_weights, v_weights : 1d array_like, optional\n",
            "            Weights or counts corresponding with the sample or probability masses\n",
            "            corresponding with the support values. Sum of elements must be positive\n",
            "            and finite. If unspecified, each value is assigned the same weight.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        distance : float\n",
            "            The computed distance between the distributions.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Given two 1D probability mass functions, :math:`u` and :math:`v`, the first\n",
            "        Wasserstein distance between the distributions is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int_{\\mathbb{R} \\times\n",
            "            \\mathbb{R}} |x-y| \\mathrm{d} \\pi (x, y)\n",
            "        \n",
            "        where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n",
            "        :math:`\\mathbb{R} \\times \\mathbb{R}` whose marginals are :math:`u` and\n",
            "        :math:`v` on the first and second factors respectively. For a given value\n",
            "        :math:`x`, :math:`u(x)` gives the probabilty of :math:`u` at position\n",
            "        :math:`x`, and the same for :math:`v(x)`.\n",
            "        \n",
            "        If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and\n",
            "        :math:`v`, this distance also equals to:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            l_1(u, v) = \\int_{-\\infty}^{+\\infty} |U-V|\n",
            "        \n",
            "        See [3]_ for a proof of the equivalence of both definitions.\n",
            "        \n",
            "        The input distributions can be empirical, therefore coming from samples\n",
            "        whose values are effectively inputs of the function, or they can be seen as\n",
            "        generalized functions, in which case they are weighted sums of Dirac delta\n",
            "        functions located at the specified values.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Wasserstein metric\", https://en.wikipedia.org/wiki/Wasserstein_metric\n",
            "        .. [2] Lili Weng, \"What is Wasserstein distance?\", Lil'log,\n",
            "               https://lilianweng.github.io/posts/2017-08-20-gan/#what-is-wasserstein-distance.\n",
            "        .. [3] Ramdas, Garcia, Cuturi \"On Wasserstein Two Sample Testing and Related\n",
            "               Families of Nonparametric Tests\" (2015). :arXiv:`1509.02237`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        wasserstein_distance_nd: Compute the Wasserstein-1 distance between two N-D\n",
            "            discrete distributions.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import wasserstein_distance\n",
            "        >>> wasserstein_distance([0, 1, 3], [5, 6, 8])\n",
            "        5.0\n",
            "        >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])\n",
            "        0.25\n",
            "        >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],\n",
            "        ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])\n",
            "        4.0781331438047861\n",
            "    \n",
            "    wasserstein_distance_nd(u_values, v_values, u_weights=None, v_weights=None)\n",
            "        Compute the Wasserstein-1 distance between two N-D discrete distributions.\n",
            "        \n",
            "        The Wasserstein distance, also called the Earth mover's distance or the\n",
            "        optimal transport distance, is a similarity metric between two probability\n",
            "        distributions [1]_. In the discrete case, the Wasserstein distance can be\n",
            "        understood as the cost of an optimal transport plan to convert one\n",
            "        distribution into the other. The cost is calculated as the product of the\n",
            "        amount of probability mass being moved and the distance it is being moved.\n",
            "        A brief and intuitive introduction can be found at [2]_.\n",
            "        \n",
            "        .. versionadded:: 1.13.0\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        u_values : 2d array_like\n",
            "            A sample from a probability distribution or the support (set of all\n",
            "            possible values) of a probability distribution. Each element along\n",
            "            axis 0 is an observation or possible value, and axis 1 represents the\n",
            "            dimensionality of the distribution; i.e., each row is a vector\n",
            "            observation or possible value.\n",
            "        \n",
            "        v_values : 2d array_like\n",
            "            A sample from or the support of a second distribution.\n",
            "        \n",
            "        u_weights, v_weights : 1d array_like, optional\n",
            "            Weights or counts corresponding with the sample or probability masses\n",
            "            corresponding with the support values. Sum of elements must be positive\n",
            "            and finite. If unspecified, each value is assigned the same weight.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        distance : float\n",
            "            The computed distance between the distributions.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Given two probability mass functions, :math:`u`\n",
            "        and :math:`v`, the first Wasserstein distance between the distributions\n",
            "        using the Euclidean norm is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int \\| x-y \\|_2 \\mathrm{d} \\pi (x, y)\n",
            "        \n",
            "        where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n",
            "        :math:`\\mathbb{R}^n \\times \\mathbb{R}^n` whose marginals are :math:`u` and\n",
            "        :math:`v` on the first and second factors respectively. For a given value\n",
            "        :math:`x`, :math:`u(x)` gives the probabilty of :math:`u` at position\n",
            "        :math:`x`, and the same for :math:`v(x)`.\n",
            "        \n",
            "        This is also called the optimal transport problem or the Monge problem.\n",
            "        Let the finite point sets :math:`\\{x_i\\}` and :math:`\\{y_j\\}` denote\n",
            "        the support set of probability mass function :math:`u` and :math:`v`\n",
            "        respectively. The Monge problem can be expressed as follows,\n",
            "        \n",
            "        Let :math:`\\Gamma` denote the transport plan, :math:`D` denote the\n",
            "        distance matrix and,\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            x = \\text{vec}(\\Gamma)          \\\\\n",
            "            c = \\text{vec}(D)               \\\\\n",
            "            b = \\begin{bmatrix}\n",
            "                    u\\\\\n",
            "                    v\\\\\n",
            "                \\end{bmatrix}\n",
            "        \n",
            "        The :math:`\\text{vec}()` function denotes the Vectorization function\n",
            "        that transforms a matrix into a column vector by vertically stacking\n",
            "        the columns of the matrix.\n",
            "        The tranport plan :math:`\\Gamma` is a matrix :math:`[\\gamma_{ij}]` in\n",
            "        which :math:`\\gamma_{ij}` is a positive value representing the amount of\n",
            "        probability mass transported from :math:`u(x_i)` to :math:`v(y_i)`.\n",
            "        Summing over the rows of :math:`\\Gamma` should give the source distribution\n",
            "        :math:`u` : :math:`\\sum_j \\gamma_{ij} = u(x_i)` holds for all :math:`i`\n",
            "        and summing over the columns of :math:`\\Gamma` should give the target\n",
            "        distribution :math:`v`: :math:`\\sum_i \\gamma_{ij} = v(y_j)` holds for all\n",
            "        :math:`j`.\n",
            "        The distance matrix :math:`D` is a matrix :math:`[d_{ij}]`, in which\n",
            "        :math:`d_{ij} = d(x_i, y_j)`.\n",
            "        \n",
            "        Given :math:`\\Gamma`, :math:`D`, :math:`b`, the Monge problem can be\n",
            "        tranformed into a linear programming problem by\n",
            "        taking :math:`A x = b` as constraints and :math:`z = c^T x` as minimization\n",
            "        target (sum of costs) , where matrix :math:`A` has the form\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\begin{array} {rrrr|rrrr|r|rrrr}\n",
            "                1 & 1 & \\dots & 1 & 0 & 0 & \\dots & 0 & \\dots & 0 & 0 & \\dots &\n",
            "                    0 \\cr\n",
            "                0 & 0 & \\dots & 0 & 1 & 1 & \\dots & 1 & \\dots & 0 & 0 &\\dots &\n",
            "                    0 \\cr\n",
            "                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots\n",
            "                    & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\cr\n",
            "                0 & 0 & \\dots & 0 & 0 & 0 & \\dots & 0 & \\dots & 1 & 1 & \\dots &\n",
            "                    1 \\cr \\hline\n",
            "        \n",
            "                1 & 0 & \\dots & 0 & 1 & 0 & \\dots & \\dots & \\dots & 1 & 0 & \\dots &\n",
            "                    0 \\cr\n",
            "                0 & 1 & \\dots & 0 & 0 & 1 & \\dots & \\dots & \\dots & 0 & 1 & \\dots &\n",
            "                    0 \\cr\n",
            "                \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots &\n",
            "                    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\cr\n",
            "                0 & 0 & \\dots & 1 & 0 & 0 & \\dots & 1 & \\dots & 0 & 0 & \\dots & 1\n",
            "            \\end{array}\n",
            "        \n",
            "        By solving the dual form of the above linear programming problem (with\n",
            "        solution :math:`y^*`), the Wasserstein distance :math:`l_1 (u, v)` can\n",
            "        be computed as :math:`b^T y^*`.\n",
            "        \n",
            "        The above solution is inspired by Vincent Herrmann's blog [3]_ . For a\n",
            "        more thorough explanation, see [4]_ .\n",
            "        \n",
            "        The input distributions can be empirical, therefore coming from samples\n",
            "        whose values are effectively inputs of the function, or they can be seen as\n",
            "        generalized functions, in which case they are weighted sums of Dirac delta\n",
            "        functions located at the specified values.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Wasserstein metric\",\n",
            "               https://en.wikipedia.org/wiki/Wasserstein_metric\n",
            "        .. [2] Lili Weng, \"What is Wasserstein distance?\", Lil'log,\n",
            "               https://lilianweng.github.io/posts/2017-08-20-gan/#what-is-wasserstein-distance.\n",
            "        .. [3] Hermann, Vincent. \"Wasserstein GAN and the Kantorovich-Rubinstein\n",
            "               Duality\". https://vincentherrmann.github.io/blog/wasserstein/.\n",
            "        .. [4] Peyr, Gabriel, and Marco Cuturi. \"Computational optimal\n",
            "               transport.\" Center for Research in Economics and Statistics\n",
            "               Working Papers 2017-86 (2017).\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        wasserstein_distance: Compute the Wasserstein-1 distance between two\n",
            "            1D discrete distributions.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Compute the Wasserstein distance between two three-dimensional samples,\n",
            "        each with two observations.\n",
            "        \n",
            "        >>> from scipy.stats import wasserstein_distance_nd\n",
            "        >>> wasserstein_distance_nd([[0, 2, 3], [1, 2, 5]], [[3, 2, 3], [4, 2, 5]])\n",
            "        3.0\n",
            "        \n",
            "        Compute the Wasserstein distance between two two-dimensional distributions\n",
            "        with three and two weighted observations, respectively.\n",
            "        \n",
            "        >>> wasserstein_distance_nd([[0, 2.75], [2, 209.3], [0, 0]],\n",
            "        ...                      [[0.2, 0.322], [4.5, 25.1808]],\n",
            "        ...                      [0.4, 5.2, 0.114], [0.8, 1.5])\n",
            "        174.15840245217169\n",
            "    \n",
            "    weightedtau(x, y, rank=True, weigher=None, additive=True)\n",
            "        Compute a weighted version of Kendall's :math:`\\tau`.\n",
            "        \n",
            "        The weighted :math:`\\tau` is a weighted version of Kendall's\n",
            "        :math:`\\tau` in which exchanges of high weight are more influential than\n",
            "        exchanges of low weight. The default parameters compute the additive\n",
            "        hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n",
            "        been shown to provide the best balance between important and\n",
            "        unimportant elements [1]_.\n",
            "        \n",
            "        The weighting is defined by means of a rank array, which assigns a\n",
            "        nonnegative rank to each element (higher importance ranks being\n",
            "        associated with smaller values, e.g., 0 is the highest possible rank),\n",
            "        and a weigher function, which assigns a weight based on the rank to\n",
            "        each element. The weight of an exchange is then the sum or the product\n",
            "        of the weights of the ranks of the exchanged elements. The default\n",
            "        parameters compute :math:`\\tau_\\mathrm h`: an exchange between\n",
            "        elements with rank :math:`r` and :math:`s` (starting from zero) has\n",
            "        weight :math:`1/(r+1) + 1/(s+1)`.\n",
            "        \n",
            "        Specifying a rank array is meaningful only if you have in mind an\n",
            "        external criterion of importance. If, as it usually happens, you do\n",
            "        not have in mind a specific rank, the weighted :math:`\\tau` is\n",
            "        defined by averaging the values obtained using the decreasing\n",
            "        lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n",
            "        behavior with default parameters. Note that the convention used\n",
            "        here for ranking (lower values imply higher importance) is opposite\n",
            "        to that used by other SciPy statistical functions.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x, y : array_like\n",
            "            Arrays of scores, of the same shape. If arrays are not 1-D, they will\n",
            "            be flattened to 1-D.\n",
            "        rank : array_like of ints or bool, optional\n",
            "            A nonnegative rank assigned to each element. If it is None, the\n",
            "            decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n",
            "            higher rank will be those with larger `x`-values, using `y`-values to\n",
            "            break ties (in particular, swapping `x` and `y` will give a different\n",
            "            result). If it is False, the element indices will be used\n",
            "            directly as ranks. The default is True, in which case this\n",
            "            function returns the average of the values obtained using the\n",
            "            decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n",
            "        weigher : callable, optional\n",
            "            The weigher function. Must map nonnegative integers (zero\n",
            "            representing the most important element) to a nonnegative weight.\n",
            "            The default, None, provides hyperbolic weighing, that is,\n",
            "            rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n",
            "        additive : bool, optional\n",
            "            If True, the weight of an exchange is computed by adding the\n",
            "            weights of the ranks of the exchanged elements; otherwise, the weights\n",
            "            are multiplied. The default is True.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        res: SignificanceResult\n",
            "            An object containing attributes:\n",
            "        \n",
            "            statistic : float\n",
            "               The weighted :math:`\\tau` correlation index.\n",
            "            pvalue : float\n",
            "               Presently ``np.nan``, as the null distribution of the statistic is\n",
            "               unknown (even in the additive hyperbolic case).\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kendalltau : Calculates Kendall's tau.\n",
            "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
            "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n",
            "        [1]_ that is a weighted extension of Knight's algorithm for Kendall's\n",
            "        :math:`\\tau` [2]_. It can compute Shieh's weighted :math:`\\tau` [3]_\n",
            "        between rankings without ties (i.e., permutations) by setting\n",
            "        `additive` and `rank` to False, as the definition given in [1]_ is a\n",
            "        generalization of Shieh's.\n",
            "        \n",
            "        NaNs are considered the smallest possible score.\n",
            "        \n",
            "        .. versionadded:: 0.19.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Sebastiano Vigna, \"A weighted correlation index for rankings with\n",
            "               ties\", Proceedings of the 24th international conference on World\n",
            "               Wide Web, pp. 1166-1176, ACM, 2015.\n",
            "        .. [2] W.R. Knight, \"A Computer Method for Calculating Kendall's Tau with\n",
            "               Ungrouped Data\", Journal of the American Statistical Association,\n",
            "               Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n",
            "        .. [3] Grace S. Shieh. \"A weighted Kendall's tau statistic\", Statistics &\n",
            "               Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> x = [12, 2, 1, 12, 2]\n",
            "        >>> y = [1, 4, 7, 1, 0]\n",
            "        >>> res = stats.weightedtau(x, y)\n",
            "        >>> res.statistic\n",
            "        -0.56694968153682723\n",
            "        >>> res.pvalue\n",
            "        nan\n",
            "        >>> res = stats.weightedtau(x, y, additive=False)\n",
            "        >>> res.statistic\n",
            "        -0.62205716951801038\n",
            "        \n",
            "        NaNs are considered the smallest possible score:\n",
            "        \n",
            "        >>> x = [12, 2, 1, 12, 2]\n",
            "        >>> y = [1, 4, 7, 1, np.nan]\n",
            "        >>> res = stats.weightedtau(x, y)\n",
            "        >>> res.statistic\n",
            "        -0.56694968153682723\n",
            "        \n",
            "        This is exactly Kendall's tau:\n",
            "        \n",
            "        >>> x = [12, 2, 1, 12, 2]\n",
            "        >>> y = [1, 4, 7, 1, 0]\n",
            "        >>> res = stats.weightedtau(x, y, weigher=lambda x: 1)\n",
            "        >>> res.statistic\n",
            "        -0.47140452079103173\n",
            "        \n",
            "        >>> x = [12, 2, 1, 12, 2]\n",
            "        >>> y = [1, 4, 7, 1, 0]\n",
            "        >>> stats.weightedtau(x, y, rank=None)\n",
            "        SignificanceResult(statistic=-0.4157652301037516, pvalue=nan)\n",
            "        >>> stats.weightedtau(y, x, rank=None)\n",
            "        SignificanceResult(statistic=-0.7181341329699028, pvalue=nan)\n",
            "    \n",
            "    wilcoxon(x, y=None, zero_method='wilcox', correction=False, alternative='two-sided', method='auto', *, axis=0, nan_policy='propagate', keepdims=False)\n",
            "        Calculate the Wilcoxon signed-rank test.\n",
            "        \n",
            "        The Wilcoxon signed-rank test tests the null hypothesis that two\n",
            "        related paired samples come from the same distribution. In particular,\n",
            "        it tests whether the distribution of the differences ``x - y`` is symmetric\n",
            "        about zero. It is a non-parametric version of the paired T-test.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Either the first set of measurements (in which case ``y`` is the second\n",
            "            set of measurements), or the differences between two sets of\n",
            "            measurements (in which case ``y`` is not to be specified.)  Must be\n",
            "            one-dimensional.\n",
            "        y : array_like, optional\n",
            "            Either the second set of measurements (if ``x`` is the first set of\n",
            "            measurements), or not specified (if ``x`` is the differences between\n",
            "            two sets of measurements.)  Must be one-dimensional.\n",
            "            \n",
            "            .. warning::\n",
            "                When `y` is provided, `wilcoxon` calculates the test statistic\n",
            "                based on the ranks of the absolute values of ``d = x - y``.\n",
            "                Roundoff error in the subtraction can result in elements of ``d``\n",
            "                being assigned different ranks even when they would be tied with\n",
            "                exact arithmetic. Rather than passing `x` and `y` separately,\n",
            "                consider computing the difference ``x - y``, rounding as needed to\n",
            "                ensure that only truly unique elements are numerically distinct,\n",
            "                and passing the result as `x`, leaving `y` at the default (None).\n",
            "        zero_method : {\"wilcox\", \"pratt\", \"zsplit\"}, optional\n",
            "            There are different conventions for handling pairs of observations\n",
            "            with equal values (\"zero-differences\", or \"zeros\").\n",
            "            \n",
            "            * \"wilcox\": Discards all zero-differences (default); see [4]_.\n",
            "            * \"pratt\": Includes zero-differences in the ranking process,\n",
            "              but drops the ranks of the zeros (more conservative); see [3]_.\n",
            "              In this case, the normal approximation is adjusted as in [5]_.\n",
            "            * \"zsplit\": Includes zero-differences in the ranking process and\n",
            "              splits the zero rank between positive and negative ones.\n",
            "        correction : bool, optional\n",
            "            If True, apply continuity correction by adjusting the Wilcoxon rank\n",
            "            statistic by 0.5 towards the mean value when computing the\n",
            "            z-statistic if a normal approximation is used.  Default is False.\n",
            "        alternative : {\"two-sided\", \"greater\", \"less\"}, optional\n",
            "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
            "            In the following, let ``d`` represent the difference between the paired\n",
            "            samples: ``d = x - y`` if both ``x`` and ``y`` are provided, or\n",
            "            ``d = x`` otherwise.\n",
            "            \n",
            "            * 'two-sided': the distribution underlying ``d`` is not symmetric\n",
            "              about zero.\n",
            "            * 'less': the distribution underlying ``d`` is stochastically less\n",
            "              than a distribution symmetric about zero.\n",
            "            * 'greater': the distribution underlying ``d`` is stochastically\n",
            "              greater than a distribution symmetric about zero.\n",
            "        method : {\"auto\", \"exact\", \"approx\"} or `PermutationMethod` instance, optional\n",
            "            Method to calculate the p-value, see Notes. Default is \"auto\".\n",
            "        axis : int or None, default: 0\n",
            "            If an int, the axis of the input along which to compute the statistic.\n",
            "            The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
            "            corresponding element of the output.\n",
            "            If ``None``, the input will be raveled before computing the statistic.\n",
            "        nan_policy : {'propagate', 'omit', 'raise'}\n",
            "            Defines how to handle input NaNs.\n",
            "            \n",
            "            - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
            "              which the  statistic is computed, the corresponding entry of the output\n",
            "              will be NaN.\n",
            "            - ``omit``: NaNs will be omitted when performing the calculation.\n",
            "              If insufficient data remains in the axis slice along which the\n",
            "              statistic is computed, the corresponding entry of the output will be\n",
            "              NaN.\n",
            "            - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
            "        keepdims : bool, default: False\n",
            "            If this is set to True, the axes which are reduced are left\n",
            "            in the result as dimensions with size one. With this option,\n",
            "            the result will broadcast correctly against the input array.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        An object with the following attributes.\n",
            "        statistic : array_like\n",
            "            If `alternative` is \"two-sided\", the sum of the ranks of the\n",
            "            differences above or below zero, whichever is smaller.\n",
            "            Otherwise the sum of the ranks of the differences above zero.\n",
            "        pvalue : array_like\n",
            "            The p-value for the test depending on `alternative` and `method`.\n",
            "        zstatistic : array_like\n",
            "            When ``method = 'approx'``, this is the normalized z-statistic::\n",
            "            \n",
            "                z = (T - mn - d) / se\n",
            "            \n",
            "            where ``T`` is `statistic` as defined above, ``mn`` is the mean of the\n",
            "            distribution under the null hypothesis, ``d`` is a continuity\n",
            "            correction, and ``se`` is the standard error.\n",
            "            When ``method != 'approx'``, this attribute is not available.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        \n",
            "        :func:`kruskal`, :func:`mannwhitneyu`\n",
            "            ..\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        In the following, let ``d`` represent the difference between the paired\n",
            "        samples: ``d = x - y`` if both ``x`` and ``y`` are provided, or ``d = x``\n",
            "        otherwise. Assume that all elements of ``d`` are independent and\n",
            "        identically distributed observations, and all are distinct and nonzero.\n",
            "        \n",
            "        - When ``len(d)`` is sufficiently large, the null distribution of the\n",
            "          normalized test statistic (`zstatistic` above) is approximately normal,\n",
            "          and ``method = 'approx'`` can be used to compute the p-value.\n",
            "        \n",
            "        - When ``len(d)`` is small, the normal approximation may not be accurate,\n",
            "          and ``method='exact'`` is preferred (at the cost of additional\n",
            "          execution time).\n",
            "        \n",
            "        - The default, ``method='auto'``, selects between the two: when\n",
            "          ``len(d) <= 50`` and there are no zeros, the exact method is used;\n",
            "          otherwise, the approximate method is used.\n",
            "        \n",
            "        The presence of \"ties\" (i.e. not all elements of ``d`` are unique) or\n",
            "        \"zeros\" (i.e. elements of ``d`` are zero) changes the null distribution\n",
            "        of the test statistic, and ``method='exact'`` no longer calculates\n",
            "        the exact p-value. If ``method='approx'``, the z-statistic is adjusted\n",
            "        for more accurate comparison against the standard normal, but still,\n",
            "        for finite sample sizes, the standard normal is only an approximation of\n",
            "        the true null distribution of the z-statistic. For such situations, the\n",
            "        `method` parameter also accepts instances `PermutationMethod`. In this\n",
            "        case, the p-value is computed using `permutation_test` with the provided\n",
            "        configuration options and other appropriate settings.\n",
            "        \n",
            "        Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
            "        code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
            "        this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
            "        rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
            "        arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
            "        masked array with ``mask=False``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
            "        .. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.\n",
            "        .. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed\n",
            "           Rank Procedures, Journal of the American Statistical Association,\n",
            "           Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`\n",
            "        .. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,\n",
            "           Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`\n",
            "        .. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank\n",
            "           Sampling Distribution When Zero Differences are Present,\n",
            "           Journal of the American Statistical Association, Vol. 62, 1967,\n",
            "           pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        In [4]_, the differences in height between cross- and self-fertilized\n",
            "        corn plants is given as follows:\n",
            "        \n",
            "        >>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]\n",
            "        \n",
            "        Cross-fertilized plants appear to be higher. To test the null\n",
            "        hypothesis that there is no height difference, we can apply the\n",
            "        two-sided test:\n",
            "        \n",
            "        >>> from scipy.stats import wilcoxon\n",
            "        >>> res = wilcoxon(d)\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (24.0, 0.041259765625)\n",
            "        \n",
            "        Hence, we would reject the null hypothesis at a confidence level of 5%,\n",
            "        concluding that there is a difference in height between the groups.\n",
            "        To confirm that the median of the differences can be assumed to be\n",
            "        positive, we use:\n",
            "        \n",
            "        >>> res = wilcoxon(d, alternative='greater')\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (96.0, 0.0206298828125)\n",
            "        \n",
            "        This shows that the null hypothesis that the median is negative can be\n",
            "        rejected at a confidence level of 5% in favor of the alternative that\n",
            "        the median is greater than zero. The p-values above are exact. Using the\n",
            "        normal approximation gives very similar values:\n",
            "        \n",
            "        >>> res = wilcoxon(d, method='approx')\n",
            "        >>> res.statistic, res.pvalue\n",
            "        (24.0, 0.04088813291185591)\n",
            "        \n",
            "        Note that the statistic changed to 96 in the one-sided case (the sum\n",
            "        of ranks of positive differences) whereas it is 24 in the two-sided\n",
            "        case (the minimum of sum of ranks above and below zero).\n",
            "        \n",
            "        In the example above, the differences in height between paired plants are\n",
            "        provided to `wilcoxon` directly. Alternatively, `wilcoxon` accepts two\n",
            "        samples of equal length, calculates the differences between paired\n",
            "        elements, then performs the test. Consider the samples ``x`` and ``y``:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> x = np.array([0.5, 0.825, 0.375, 0.5])\n",
            "        >>> y = np.array([0.525, 0.775, 0.325, 0.55])\n",
            "        >>> res = wilcoxon(x, y, alternative='greater')\n",
            "        >>> res\n",
            "        WilcoxonResult(statistic=5.0, pvalue=0.5625)\n",
            "        \n",
            "        Note that had we calculated the differences by hand, the test would have\n",
            "        produced different results:\n",
            "        \n",
            "        >>> d = [-0.025, 0.05, 0.05, -0.05]\n",
            "        >>> ref = wilcoxon(d, alternative='greater')\n",
            "        >>> ref\n",
            "        WilcoxonResult(statistic=6.0, pvalue=0.4375)\n",
            "        \n",
            "        The substantial difference is due to roundoff error in the results of\n",
            "        ``x-y``:\n",
            "        \n",
            "        >>> d - (x-y)\n",
            "        array([2.08166817e-17, 6.93889390e-17, 1.38777878e-17, 4.16333634e-17])\n",
            "        \n",
            "        Even though we expected all the elements of ``(x-y)[1:]`` to have the same\n",
            "        magnitude ``0.05``, they have slightly different magnitudes in practice,\n",
            "        and therefore are assigned different ranks in the test. Before performing\n",
            "        the test, consider calculating ``d`` and adjusting it as necessary to\n",
            "        ensure that theoretically identically values are not numerically distinct.\n",
            "        For example:\n",
            "        \n",
            "        >>> d2 = np.around(x - y, decimals=3)\n",
            "        >>> wilcoxon(d2, alternative='greater')\n",
            "        WilcoxonResult(statistic=6.0, pvalue=0.4375)\n",
            "    \n",
            "    yeojohnson(x, lmbda=None)\n",
            "        Return a dataset transformed by a Yeo-Johnson power transformation.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : ndarray\n",
            "            Input array.  Should be 1-dimensional.\n",
            "        lmbda : float, optional\n",
            "            If ``lmbda`` is ``None``, find the lambda that maximizes the\n",
            "            log-likelihood function and return it as the second output argument.\n",
            "            Otherwise the transformation is done for the given value.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        yeojohnson: ndarray\n",
            "            Yeo-Johnson power transformed array.\n",
            "        maxlog : float, optional\n",
            "            If the `lmbda` parameter is None, the second returned argument is\n",
            "            the lambda that maximizes the log-likelihood function.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Yeo-Johnson transform is given by::\n",
            "        \n",
            "            y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0\n",
            "                log(x + 1),                                  for x >= 0, lmbda = 0\n",
            "                -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2\n",
            "                -log(-x + 1),                                for x < 0, lmbda = 2\n",
            "        \n",
            "        Unlike `boxcox`, `yeojohnson` does not require the input data to be\n",
            "        positive.\n",
            "        \n",
            "        .. versionadded:: 1.2.0\n",
            "        \n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        I. Yeo and R.A. Johnson, \"A New Family of Power Transformations to\n",
            "        Improve Normality or Symmetry\", Biometrika 87.4 (2000):\n",
            "        \n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        We generate some random variates from a non-normal distribution and make a\n",
            "        probability plot for it, to show it is non-normal in the tails:\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax1 = fig.add_subplot(211)\n",
            "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
            "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
            "        >>> ax1.set_xlabel('')\n",
            "        >>> ax1.set_title('Probplot against normal distribution')\n",
            "        \n",
            "        We now use `yeojohnson` to transform the data so it's closest to normal:\n",
            "        \n",
            "        >>> ax2 = fig.add_subplot(212)\n",
            "        >>> xt, lmbda = stats.yeojohnson(x)\n",
            "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
            "        >>> ax2.set_title('Probplot after Yeo-Johnson transformation')\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    yeojohnson_llf(lmb, data)\n",
            "        The yeojohnson log-likelihood function.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        lmb : scalar\n",
            "            Parameter for Yeo-Johnson transformation. See `yeojohnson` for\n",
            "            details.\n",
            "        data : array_like\n",
            "            Data to calculate Yeo-Johnson log-likelihood for. If `data` is\n",
            "            multi-dimensional, the log-likelihood is calculated along the first\n",
            "            axis.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        llf : float\n",
            "            Yeo-Johnson log-likelihood of `data` given `lmb`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Yeo-Johnson log-likelihood function is defined here as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            llf = -N/2 \\log(\\hat{\\sigma}^2) + (\\lambda - 1)\n",
            "                  \\sum_i \\text{ sign }(x_i)\\log(|x_i| + 1)\n",
            "        \n",
            "        where :math:`\\hat{\\sigma}^2` is estimated variance of the Yeo-Johnson\n",
            "        transformed input data ``x``.\n",
            "        \n",
            "        .. versionadded:: 1.2.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
            "        \n",
            "        Generate some random variates and calculate Yeo-Johnson log-likelihood\n",
            "        values for them for a range of ``lmbda`` values:\n",
            "        \n",
            "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n",
            "        >>> lmbdas = np.linspace(-2, 10)\n",
            "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
            "        >>> for ii, lmbda in enumerate(lmbdas):\n",
            "        ...     llf[ii] = stats.yeojohnson_llf(lmbda, x)\n",
            "        \n",
            "        Also find the optimal lmbda value with `yeojohnson`:\n",
            "        \n",
            "        >>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)\n",
            "        \n",
            "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
            "        horizontal line to check that that's really the optimum:\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
            "        >>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')\n",
            "        >>> ax.set_xlabel('lmbda parameter')\n",
            "        >>> ax.set_ylabel('Yeo-Johnson log-likelihood')\n",
            "        \n",
            "        Now add some probability plots to show that where the log-likelihood is\n",
            "        maximized the data transformed with `yeojohnson` looks closest to normal:\n",
            "        \n",
            "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
            "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
            "        ...     xt = stats.yeojohnson(x, lmbda=lmbda)\n",
            "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
            "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
            "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
            "        ...     ax_inset.set_xticklabels([])\n",
            "        ...     ax_inset.set_yticklabels([])\n",
            "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    yeojohnson_normmax(x, brack=None)\n",
            "        Compute optimal Yeo-Johnson transform parameter.\n",
            "        \n",
            "        Compute optimal Yeo-Johnson transform parameter for input data, using\n",
            "        maximum likelihood estimation.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array.\n",
            "        brack : 2-tuple, optional\n",
            "            The starting interval for a downhill bracket search with\n",
            "            `optimize.brent`. Note that this is in most cases not critical; the\n",
            "            final result is allowed to be outside this bracket. If None,\n",
            "            `optimize.fminbound` is used with bounds that avoid overflow.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        maxlog : float\n",
            "            The optimal transform parameter found.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        yeojohnson, yeojohnson_llf, yeojohnson_normplot\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        .. versionadded:: 1.2.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        Generate some data and determine optimal ``lmbda``\n",
            "        \n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n",
            "        >>> lmax = stats.yeojohnson_normmax(x)\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n",
            "        >>> ax.axvline(lmax, color='r')\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    yeojohnson_normplot(x, la, lb, plot=None, N=80)\n",
            "        Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n",
            "        \n",
            "        A Yeo-Johnson normality plot shows graphically what the best\n",
            "        transformation parameter is to use in `yeojohnson` to obtain a\n",
            "        distribution that is close to normal.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        x : array_like\n",
            "            Input array.\n",
            "        la, lb : scalar\n",
            "            The lower and upper bounds for the ``lmbda`` values to pass to\n",
            "            `yeojohnson` for Yeo-Johnson transformations. These are also the\n",
            "            limits of the horizontal axis of the plot if that is generated.\n",
            "        plot : object, optional\n",
            "            If given, plots the quantiles and least squares fit.\n",
            "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
            "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
            "            or a custom object with the same methods.\n",
            "            Default is None, which means that no plot is created.\n",
            "        N : int, optional\n",
            "            Number of points on the horizontal axis (equally distributed from\n",
            "            `la` to `lb`).\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        lmbdas : ndarray\n",
            "            The ``lmbda`` values for which a Yeo-Johnson transform was done.\n",
            "        ppcc : ndarray\n",
            "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
            "            when fitting the Box-Cox transformed input `x` against a normal\n",
            "            distribution.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Even if `plot` is given, the figure is not shown or saved by\n",
            "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
            "        should be used after calling `probplot`.\n",
            "        \n",
            "        .. versionadded:: 1.2.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy import stats\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n",
            "        \n",
            "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n",
            "        \n",
            "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
            "        the same plot:\n",
            "        \n",
            "        >>> _, maxlog = stats.yeojohnson(x)\n",
            "        >>> ax.axvline(maxlog, color='r')\n",
            "        \n",
            "        >>> plt.show()\n",
            "    \n",
            "    zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate')\n",
            "        Calculate the relative z-scores.\n",
            "        \n",
            "        Return an array of z-scores, i.e., scores that are standardized to\n",
            "        zero mean and unit variance, where mean and variance are calculated\n",
            "        from the comparison array.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        scores : array_like\n",
            "            The input for which z-scores are calculated.\n",
            "        compare : array_like\n",
            "            The input from which the mean and standard deviation of the\n",
            "            normalization are taken; assumed to have the same dimension as\n",
            "            `scores`.\n",
            "        axis : int or None, optional\n",
            "            Axis over which mean and variance of `compare` are calculated.\n",
            "            Default is 0. If None, compute over the whole array `scores`.\n",
            "        ddof : int, optional\n",
            "            Degrees of freedom correction in the calculation of the\n",
            "            standard deviation. Default is 0.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle the occurrence of nans in `compare`.\n",
            "            'propagate' returns nan, 'raise' raises an exception, 'omit'\n",
            "            performs the calculations ignoring nan values. Default is\n",
            "            'propagate'. Note that when the value is 'omit', nans in `scores`\n",
            "            also propagate to the output, but they do not affect the z-scores\n",
            "            computed for the non-nan values.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        zscore : array_like\n",
            "            Z-scores, in the same shape as `scores`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function preserves ndarray subclasses, and works also with\n",
            "        matrices and masked arrays (it uses `asanyarray` instead of\n",
            "        `asarray` for parameters).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import zmap\n",
            "        >>> a = [0.5, 2.0, 2.5, 3]\n",
            "        >>> b = [0, 1, 2, 3, 4]\n",
            "        >>> zmap(a, b)\n",
            "        array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n",
            "    \n",
            "    zscore(a, axis=0, ddof=0, nan_policy='propagate')\n",
            "        Compute the z score.\n",
            "        \n",
            "        Compute the z score of each value in the sample, relative to the\n",
            "        sample mean and standard deviation.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        a : array_like\n",
            "            An array like object containing the sample data.\n",
            "        axis : int or None, optional\n",
            "            Axis along which to operate. Default is 0. If None, compute over\n",
            "            the whole array `a`.\n",
            "        ddof : int, optional\n",
            "            Degrees of freedom correction in the calculation of the\n",
            "            standard deviation. Default is 0.\n",
            "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
            "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
            "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
            "            values. Default is 'propagate'.  Note that when the value is 'omit',\n",
            "            nans in the input also propagate to the output, but they do not affect\n",
            "            the z-scores computed for the non-nan values.\n",
            "        \n",
            "        Returns\n",
            "        -------\n",
            "        zscore : array_like\n",
            "            The z-scores, standardized by mean and standard deviation of\n",
            "            input array `a`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        numpy.mean : Arithmetic average\n",
            "        numpy.std : Arithmetic standard deviation\n",
            "        scipy.stats.gzscore : Geometric standard score\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This function preserves ndarray subclasses, and works also with\n",
            "        matrices and masked arrays (it uses `asanyarray` instead of\n",
            "        `asarray` for parameters).\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Standard score\", *Wikipedia*,\n",
            "               https://en.wikipedia.org/wiki/Standard_score.\n",
            "        .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n",
            "               about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n",
            "        ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n",
            "        >>> from scipy import stats\n",
            "        >>> stats.zscore(a)\n",
            "        array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n",
            "                0.6748, -1.1488, -1.3324])\n",
            "        \n",
            "        Computing along a specified axis, using n-1 degrees of freedom\n",
            "        (``ddof=1``) to calculate the standard deviation:\n",
            "        \n",
            "        >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n",
            "        ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n",
            "        ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n",
            "        ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n",
            "        ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n",
            "        >>> stats.zscore(b, axis=1, ddof=1)\n",
            "        array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n",
            "               [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n",
            "               [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n",
            "               [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n",
            "               [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n",
            "        \n",
            "        An example with `nan_policy='omit'`:\n",
            "        \n",
            "        >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n",
            "        ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n",
            "        >>> stats.zscore(x, axis=1, nan_policy='omit')\n",
            "        array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n",
            "               [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n",
            "\n",
            "DATA\n",
            "    __all__ = ['BootstrapMethod', 'CensoredData', 'ConstantInputWarning', ...\n",
            "    alpha = <scipy.stats._continuous_distns.alpha_gen object>\n",
            "        An alpha continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `alpha` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `alpha` ([1]_, [2]_) is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = \\frac{1}{x^2 \\Phi(a) \\sqrt{2\\pi}} *\n",
            "                      \\exp(-\\frac{1}{2} (a-1/x)^2)\n",
            "        \n",
            "        where :math:`\\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.\n",
            "        \n",
            "        `alpha` takes ``a`` as a shape parameter.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``alpha.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``alpha.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Johnson, Kotz, and Balakrishnan, \"Continuous Univariate\n",
            "               Distributions, Volume 1\", Second Edition, John Wiley and Sons,\n",
            "               p. 173 (1994).\n",
            "        .. [2] Anthony A. Salvia, \"Reliability applications of the Alpha\n",
            "               Distribution\", IEEE Transactions on Reliability, Vol. R-34,\n",
            "               No. 3, pp. 251-252 (1985).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import alpha\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 3.57\n",
            "        >>> mean, var, skew, kurt = alpha.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(alpha.ppf(0.01, a),\n",
            "        ...                 alpha.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, alpha.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='alpha pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = alpha(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = alpha.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], alpha.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = alpha.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    anglit = <scipy.stats._continuous_distns.anglit_gen object>\n",
            "        An anglit continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `anglit` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `anglit` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\sin(2x + \\pi/2) = \\cos(2x)\n",
            "        \n",
            "        for :math:`-\\pi/4 \\le x \\le \\pi/4`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``anglit.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``anglit.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import anglit\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = anglit.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(anglit.ppf(0.01),\n",
            "        ...                 anglit.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, anglit.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='anglit pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = anglit()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = anglit.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], anglit.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = anglit.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    arcsine = <scipy.stats._continuous_distns.arcsine_gen object>\n",
            "        An arcsine continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `arcsine` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `arcsine` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\pi \\sqrt{x (1-x)}}\n",
            "        \n",
            "        for :math:`0 < x < 1`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``arcsine.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``arcsine.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import arcsine\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = arcsine.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(arcsine.ppf(0.01),\n",
            "        ...                 arcsine.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, arcsine.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='arcsine pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = arcsine()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = arcsine.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], arcsine.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = arcsine.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    argus = <scipy.stats._continuous_distns.argus_gen object>\n",
            "        Argus distribution\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `argus` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(chi, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, chi, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, chi, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, chi, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, chi, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, chi, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, chi, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, chi, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, chi, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, chi, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(chi, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(chi, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(chi,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(chi, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(chi, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(chi, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(chi, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, chi, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `argus` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\chi) = \\frac{\\chi^3}{\\sqrt{2\\pi} \\Psi(\\chi)} x \\sqrt{1-x^2}\n",
            "                         \\exp(-\\chi^2 (1 - x^2)/2)\n",
            "        \n",
            "        for :math:`0 < x < 1` and :math:`\\chi > 0`, where\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\Psi(\\chi) = \\Phi(\\chi) - \\chi \\phi(\\chi) - 1/2\n",
            "        \n",
            "        with :math:`\\Phi` and :math:`\\phi` being the CDF and PDF of a standard\n",
            "        normal distribution, respectively.\n",
            "        \n",
            "        `argus` takes :math:`\\chi` as shape a parameter. Details about sampling\n",
            "        from the ARGUS distribution can be found in [2]_.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``argus.pdf(x, chi, loc, scale)`` is identically\n",
            "        equivalent to ``argus.pdf(y, chi) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"ARGUS distribution\",\n",
            "               https://en.wikipedia.org/wiki/ARGUS_distribution\n",
            "        .. [2] Christoph Baumgarten \"Random variate generation by fast numerical\n",
            "               inversion in the varying parameter case.\" Research in Statistics,\n",
            "               vol. 1, 2023, doi:10.1080/27684520.2023.2279060.\n",
            "        \n",
            "        .. versionadded:: 0.19.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import argus\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> chi = 1\n",
            "        >>> mean, var, skew, kurt = argus.stats(chi, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(argus.ppf(0.01, chi),\n",
            "        ...                 argus.ppf(0.99, chi), 100)\n",
            "        >>> ax.plot(x, argus.pdf(x, chi),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='argus pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = argus(chi)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = argus.ppf([0.001, 0.5, 0.999], chi)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], argus.cdf(vals, chi))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = argus.rvs(chi, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    bernoulli = <scipy.stats._discrete_distns.bernoulli_gen object>\n",
            "        A Bernoulli discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `bernoulli` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(p, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, p, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, p, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, p, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, p, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, p, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, p, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, p, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, p, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(p, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(p, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(p, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(p, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(p, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(p, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, p, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `bernoulli` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(k) = \\begin{cases}1-p  &\\text{if } k = 0\\\\\n",
            "                               p    &\\text{if } k = 1\\end{cases}\n",
            "        \n",
            "        for :math:`k` in :math:`\\{0, 1\\}`, :math:`0 \\leq p \\leq 1`\n",
            "        \n",
            "        `bernoulli` takes :math:`p` as shape parameter,\n",
            "        where :math:`p` is the probability of a single success\n",
            "        and :math:`1-p` is the probability of a single failure.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``bernoulli.pmf(k, p, loc)`` is identically\n",
            "        equivalent to ``bernoulli.pmf(k - loc, p)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import bernoulli\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> p = 0.3\n",
            "        >>> mean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(bernoulli.ppf(0.01, p),\n",
            "        ...               bernoulli.ppf(0.99, p))\n",
            "        >>> ax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\n",
            "        >>> ax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = bernoulli(p)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = bernoulli.cdf(x, p)\n",
            "        >>> np.allclose(x, bernoulli.ppf(prob, p))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = bernoulli.rvs(p, size=1000)\n",
            "    \n",
            "    beta = <scipy.stats._continuous_distns.beta_gen object>\n",
            "        A beta continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `beta` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `beta` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{\\Gamma(a+b) x^{a-1} (1-x)^{b-1}}\n",
            "                              {\\Gamma(a) \\Gamma(b)}\n",
            "        \n",
            "        for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where\n",
            "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
            "        \n",
            "        `beta` takes :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``beta.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import beta\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 2.31, 0.627\n",
            "        >>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(beta.ppf(0.01, a, b),\n",
            "        ...                 beta.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, beta.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='beta pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = beta(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = beta.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    betabinom = <scipy.stats._discrete_distns.betabinom_gen object>\n",
            "        A beta-binomial discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `betabinom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, a, b, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, n, a, b, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, n, a, b, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, n, a, b, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, n, a, b, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, n, a, b, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, n, a, b, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, a, b, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, a, b, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(n, a, b, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, a, b, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, a, b, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(n, a, b, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(n, a, b, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(n, a, b, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, a, b, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The beta-binomial distribution is a binomial distribution with a\n",
            "        probability of success `p` that follows a beta distribution.\n",
            "        \n",
            "        The probability mass function for `betabinom` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(k) = \\binom{n}{k} \\frac{B(k + a, n - k + b)}{B(a, b)}\n",
            "        \n",
            "        for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`n \\geq 0`, :math:`a > 0`,\n",
            "        :math:`b > 0`, where :math:`B(a, b)` is the beta function.\n",
            "        \n",
            "        `betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``betabinom.pmf(k, n, a, b, loc)`` is identically\n",
            "        equivalent to ``betabinom.pmf(k - loc, n, a, b)``.\n",
            "        \n",
            "        .. versionadded:: 1.4.0\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        beta, binom\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import betabinom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n, a, b = 5, 2.3, 0.63\n",
            "        >>> mean, var, skew, kurt = betabinom.stats(n, a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(betabinom.ppf(0.01, n, a, b),\n",
            "        ...               betabinom.ppf(0.99, n, a, b))\n",
            "        >>> ax.plot(x, betabinom.pmf(x, n, a, b), 'bo', ms=8, label='betabinom pmf')\n",
            "        >>> ax.vlines(x, 0, betabinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = betabinom(n, a, b)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = betabinom.cdf(x, n, a, b)\n",
            "        >>> np.allclose(x, betabinom.ppf(prob, n, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = betabinom.rvs(n, a, b, size=1000)\n",
            "    \n",
            "    betanbinom = <scipy.stats._discrete_distns.betanbinom_gen object>\n",
            "        A beta-negative-binomial discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `betanbinom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, a, b, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, n, a, b, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, n, a, b, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, n, a, b, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, n, a, b, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, n, a, b, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, n, a, b, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, a, b, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, a, b, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(n, a, b, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, a, b, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, a, b, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(n, a, b, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(n, a, b, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(n, a, b, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, a, b, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The beta-negative-binomial distribution is a negative binomial\n",
            "        distribution with a probability of success `p` that follows a\n",
            "        beta distribution.\n",
            "        \n",
            "        The probability mass function for `betanbinom` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(k) = \\binom{n + k - 1}{k} \\frac{B(a + n, b + k)}{B(a, b)}\n",
            "        \n",
            "        for :math:`k \\ge 0`, :math:`n \\geq 0`, :math:`a > 0`,\n",
            "        :math:`b > 0`, where :math:`B(a, b)` is the beta function.\n",
            "        \n",
            "        `betanbinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``betanbinom.pmf(k, n, a, b, loc)`` is identically\n",
            "        equivalent to ``betanbinom.pmf(k - loc, n, a, b)``.\n",
            "        \n",
            "        .. versionadded:: 1.12.0\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        betabinom : Beta binomial distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import betanbinom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n, a, b = 5, 9.3, 1\n",
            "        >>> mean, var, skew, kurt = betanbinom.stats(n, a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(betanbinom.ppf(0.01, n, a, b),\n",
            "        ...               betanbinom.ppf(0.99, n, a, b))\n",
            "        >>> ax.plot(x, betanbinom.pmf(x, n, a, b), 'bo', ms=8, label='betanbinom pmf')\n",
            "        >>> ax.vlines(x, 0, betanbinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = betanbinom(n, a, b)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = betanbinom.cdf(x, n, a, b)\n",
            "        >>> np.allclose(x, betanbinom.ppf(prob, n, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = betanbinom.rvs(n, a, b, size=1000)\n",
            "    \n",
            "    betaprime = <scipy.stats._continuous_distns.betaprime_gen object>\n",
            "        A beta prime continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `betaprime` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `betaprime` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{x^{a-1} (1+x)^{-a-b}}{\\beta(a, b)}\n",
            "        \n",
            "        for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where\n",
            "        :math:`\\beta(a, b)` is the beta function (see `scipy.special.beta`).\n",
            "        \n",
            "        `betaprime` takes ``a`` and ``b`` as shape parameters.\n",
            "        \n",
            "        The distribution is related to the `beta` distribution as follows:\n",
            "        If :math:`X` follows a beta distribution with parameters :math:`a, b`,\n",
            "        then :math:`Y = X/(1-X)` has a beta prime distribution with\n",
            "        parameters :math:`a, b` ([1]_).\n",
            "        \n",
            "        The beta prime distribution is a reparametrized version of the\n",
            "        F distribution.  The beta prime distribution with shape parameters\n",
            "        ``a`` and ``b`` and ``scale = s`` is equivalent to the F distribution\n",
            "        with parameters ``d1 = 2*a``, ``d2 = 2*b`` and ``scale = (a/b)*s``.\n",
            "        For example,\n",
            "        \n",
            "        >>> from scipy.stats import betaprime, f\n",
            "        >>> x = [1, 2, 5, 10]\n",
            "        >>> a = 12\n",
            "        >>> b = 5\n",
            "        >>> betaprime.pdf(x, a, b, scale=2)\n",
            "        array([0.00541179, 0.08331299, 0.14669185, 0.03150079])\n",
            "        >>> f.pdf(x, 2*a, 2*b, scale=(a/b)*2)\n",
            "        array([0.00541179, 0.08331299, 0.14669185, 0.03150079])\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``betaprime.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``betaprime.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Beta prime distribution, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Beta_prime_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import betaprime\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 5, 6\n",
            "        >>> mean, var, skew, kurt = betaprime.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(betaprime.ppf(0.01, a, b),\n",
            "        ...                 betaprime.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, betaprime.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='betaprime pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = betaprime(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = betaprime.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], betaprime.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = betaprime.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    binom = <scipy.stats._discrete_distns.binom_gen object>\n",
            "        A binomial discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `binom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, p, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, n, p, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, n, p, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, n, p, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, n, p, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, n, p, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, n, p, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, p, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, p, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(n, p, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, p, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, p, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(n, p, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(n, p, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(n, p, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, p, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `binom` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
            "        \n",
            "        for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`0 \\leq p \\leq 1`\n",
            "        \n",
            "        `binom` takes :math:`n` and :math:`p` as shape parameters,\n",
            "        where :math:`p` is the probability of a single success\n",
            "        and :math:`1-p` is the probability of a single failure.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``binom.pmf(k, n, p, loc)`` is identically\n",
            "        equivalent to ``binom.pmf(k - loc, n, p)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import binom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n, p = 5, 0.4\n",
            "        >>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(binom.ppf(0.01, n, p),\n",
            "        ...               binom.ppf(0.99, n, p))\n",
            "        >>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n",
            "        >>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = binom(n, p)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = binom.cdf(x, n, p)\n",
            "        >>> np.allclose(x, binom.ppf(prob, n, p))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = binom.rvs(n, p, size=1000)\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        hypergeom, nbinom, nhypergeom\n",
            "    \n",
            "    boltzmann = <scipy.stats._discrete_distns.boltzmann_gen object>\n",
            "        A Boltzmann (Truncated Discrete Exponential) random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `boltzmann` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(lambda_, N, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, lambda_, N, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, lambda_, N, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, lambda_, N, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, lambda_, N, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, lambda_, N, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, lambda_, N, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, lambda_, N, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, lambda_, N, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(lambda_, N, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(lambda_, N, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(lambda_, N), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(lambda_, N, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(lambda_, N, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(lambda_, N, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(lambda_, N, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, lambda_, N, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `boltzmann` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k) / (1-\\exp(-\\lambda N))\n",
            "        \n",
            "        for :math:`k = 0,..., N-1`.\n",
            "        \n",
            "        `boltzmann` takes :math:`\\lambda > 0` and :math:`N > 0` as shape parameters.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``boltzmann.pmf(k, lambda_, N, loc)`` is identically\n",
            "        equivalent to ``boltzmann.pmf(k - loc, lambda_, N)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import boltzmann\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> lambda_, N = 1.4, 19\n",
            "        >>> mean, var, skew, kurt = boltzmann.stats(lambda_, N, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(boltzmann.ppf(0.01, lambda_, N),\n",
            "        ...               boltzmann.ppf(0.99, lambda_, N))\n",
            "        >>> ax.plot(x, boltzmann.pmf(x, lambda_, N), 'bo', ms=8, label='boltzmann pmf')\n",
            "        >>> ax.vlines(x, 0, boltzmann.pmf(x, lambda_, N), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = boltzmann(lambda_, N)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = boltzmann.cdf(x, lambda_, N)\n",
            "        >>> np.allclose(x, boltzmann.ppf(prob, lambda_, N))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = boltzmann.rvs(lambda_, N, size=1000)\n",
            "    \n",
            "    bradford = <scipy.stats._continuous_distns.bradford_gen object>\n",
            "        A Bradford continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `bradford` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `bradford` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{c}{\\log(1+c) (1+cx)}\n",
            "        \n",
            "        for :math:`0 <= x <= 1` and :math:`c > 0`.\n",
            "        \n",
            "        `bradford` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``bradford.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``bradford.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import bradford\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.299\n",
            "        >>> mean, var, skew, kurt = bradford.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(bradford.ppf(0.01, c),\n",
            "        ...                 bradford.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, bradford.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='bradford pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = bradford(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = bradford.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], bradford.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = bradford.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    burr = <scipy.stats._continuous_distns.burr_gen object>\n",
            "        A Burr (Type III) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `burr` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, d, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, d, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, d, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, d, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, d, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, d, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, d, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, d, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, d, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, d, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, d, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, d, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, d, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, d, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        fisk : a special case of either `burr` or `burr12` with ``d=1``\n",
            "        burr12 : Burr Type XII distribution\n",
            "        mielke : Mielke Beta-Kappa / Dagum distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `burr` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x; c, d) = c d \\frac{x^{-c - 1}}\n",
            "                                  {{(1 + x^{-c})}^{d + 1}}\n",
            "        \n",
            "        for :math:`x >= 0` and :math:`c, d > 0`.\n",
            "        \n",
            "        `burr` takes ``c`` and ``d`` as shape parameters for :math:`c` and\n",
            "        :math:`d`.\n",
            "        \n",
            "        This is the PDF corresponding to the third CDF given in Burr's list;\n",
            "        specifically, it is equation (11) in Burr's paper [1]_. The distribution\n",
            "        is also commonly referred to as the Dagum distribution [2]_. If the\n",
            "        parameter :math:`c < 1` then the mean of the distribution does not\n",
            "        exist and if :math:`c < 2` the variance does not exist [2]_.\n",
            "        The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``burr.pdf(x, c, d, loc, scale)`` is identically\n",
            "        equivalent to ``burr.pdf(y, c, d) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
            "           Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
            "        .. [2] https://en.wikipedia.org/wiki/Dagum_distribution\n",
            "        .. [3] Kleiber, Christian. \"A guide to the Dagum distributions.\"\n",
            "           Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import burr\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c, d = 10.5, 4.3\n",
            "        >>> mean, var, skew, kurt = burr.stats(c, d, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(burr.ppf(0.01, c, d),\n",
            "        ...                 burr.ppf(0.99, c, d), 100)\n",
            "        >>> ax.plot(x, burr.pdf(x, c, d),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='burr pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = burr(c, d)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = burr.ppf([0.001, 0.5, 0.999], c, d)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], burr.cdf(vals, c, d))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = burr.rvs(c, d, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    burr12 = <scipy.stats._continuous_distns.burr12_gen object>\n",
            "        A Burr (Type XII) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `burr12` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, d, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, d, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, d, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, d, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, d, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, d, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, d, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, d, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, d, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, d, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, d, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, d, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, d, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, d, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        fisk : a special case of either `burr` or `burr12` with ``d=1``\n",
            "        burr : Burr Type III distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `burr12` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x; c, d) = c d \\frac{x^{c-1}}\n",
            "                                  {(1 + x^c)^{d + 1}}\n",
            "        \n",
            "        for :math:`x >= 0` and :math:`c, d > 0`.\n",
            "        \n",
            "        `burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`\n",
            "        and :math:`d`.\n",
            "        \n",
            "        This is the PDF corresponding to the twelfth CDF given in Burr's list;\n",
            "        specifically, it is equation (20) in Burr's paper [1]_.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``burr12.pdf(x, c, d, loc, scale)`` is identically\n",
            "        equivalent to ``burr12.pdf(y, c, d) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The Burr type 12 distribution is also sometimes referred to as\n",
            "        the Singh-Maddala distribution from NIST [2]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
            "           Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
            "        \n",
            "        .. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm\n",
            "        \n",
            "        .. [3] \"Burr distribution\",\n",
            "           https://en.wikipedia.org/wiki/Burr_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import burr12\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c, d = 10, 4\n",
            "        >>> mean, var, skew, kurt = burr12.stats(c, d, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(burr12.ppf(0.01, c, d),\n",
            "        ...                 burr12.ppf(0.99, c, d), 100)\n",
            "        >>> ax.plot(x, burr12.pdf(x, c, d),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='burr12 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = burr12(c, d)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = burr12.ppf([0.001, 0.5, 0.999], c, d)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], burr12.cdf(vals, c, d))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = burr12.rvs(c, d, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    cauchy = <scipy.stats._continuous_distns.cauchy_gen object>\n",
            "        A Cauchy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `cauchy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `cauchy` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\pi (1 + x^2)}\n",
            "        \n",
            "        for a real number :math:`x`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``cauchy.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``cauchy.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import cauchy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = cauchy.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(cauchy.ppf(0.01),\n",
            "        ...                 cauchy.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, cauchy.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='cauchy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = cauchy()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = cauchy.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], cauchy.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = cauchy.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    chi = <scipy.stats._continuous_distns.chi_gen object>\n",
            "        A chi continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `chi` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, df, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, df, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, df, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, df, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, df, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, df, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, df, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, df, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(df, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(df, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(df, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(df, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(df, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(df, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, df, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `chi` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, k) = \\frac{1}{2^{k/2-1} \\Gamma \\left( k/2 \\right)}\n",
            "                       x^{k-1} \\exp \\left( -x^2/2 \\right)\n",
            "        \n",
            "        for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``\n",
            "        in the implementation). :math:`\\Gamma` is the gamma function\n",
            "        (`scipy.special.gamma`).\n",
            "        \n",
            "        Special cases of `chi` are:\n",
            "        \n",
            "            - ``chi(1, loc, scale)`` is equivalent to `halfnorm`\n",
            "            - ``chi(2, 0, scale)`` is equivalent to `rayleigh`\n",
            "            - ``chi(3, 0, scale)`` is equivalent to `maxwell`\n",
            "        \n",
            "        `chi` takes ``df`` as a shape parameter.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``chi.pdf(x, df, loc, scale)`` is identically\n",
            "        equivalent to ``chi.pdf(y, df) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import chi\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> df = 78\n",
            "        >>> mean, var, skew, kurt = chi.stats(df, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(chi.ppf(0.01, df),\n",
            "        ...                 chi.ppf(0.99, df), 100)\n",
            "        >>> ax.plot(x, chi.pdf(x, df),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='chi pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = chi(df)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = chi.ppf([0.001, 0.5, 0.999], df)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], chi.cdf(vals, df))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = chi.rvs(df, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    chi2 = <scipy.stats._continuous_distns.chi2_gen object>\n",
            "        A chi-squared continuous random variable.\n",
            "        \n",
            "        For the noncentral chi-square distribution, see `ncx2`.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `chi2` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, df, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, df, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, df, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, df, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, df, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, df, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, df, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, df, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(df, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(df, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(df, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(df, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(df, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(df, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, df, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ncx2\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `chi2` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, k) = \\frac{1}{2^{k/2} \\Gamma \\left( k/2 \\right)}\n",
            "                       x^{k/2-1} \\exp \\left( -x/2 \\right)\n",
            "        \n",
            "        for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``\n",
            "        in the implementation).\n",
            "        \n",
            "        `chi2` takes ``df`` as a shape parameter.\n",
            "        \n",
            "        The chi-squared distribution is a special case of the gamma\n",
            "        distribution, with gamma parameters ``a = df/2``, ``loc = 0`` and\n",
            "        ``scale = 2``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``chi2.pdf(x, df, loc, scale)`` is identically\n",
            "        equivalent to ``chi2.pdf(y, df) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import chi2\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> df = 55\n",
            "        >>> mean, var, skew, kurt = chi2.stats(df, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(chi2.ppf(0.01, df),\n",
            "        ...                 chi2.ppf(0.99, df), 100)\n",
            "        >>> ax.plot(x, chi2.pdf(x, df),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='chi2 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = chi2(df)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = chi2.ppf([0.001, 0.5, 0.999], df)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = chi2.rvs(df, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    cosine = <scipy.stats._continuous_distns.cosine_gen object>\n",
            "        A cosine continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `cosine` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The cosine distribution is an approximation to the normal distribution.\n",
            "        The probability density function for `cosine` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{2\\pi} (1+\\cos(x))\n",
            "        \n",
            "        for :math:`-\\pi \\le x \\le \\pi`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``cosine.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``cosine.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import cosine\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = cosine.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(cosine.ppf(0.01),\n",
            "        ...                 cosine.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, cosine.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='cosine pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = cosine()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = cosine.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], cosine.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = cosine.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    crystalball = <scipy.stats._continuous_distns.crystalball_gen object>\n",
            "        Crystalball distribution\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `crystalball` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(beta, m, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, beta, m, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, beta, m, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, beta, m, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, beta, m, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, beta, m, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, beta, m, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, beta, m, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, beta, m, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, beta, m, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(beta, m, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(beta, m, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(beta, m), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(beta, m, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(beta, m, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(beta, m, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(beta, m, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, beta, m, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `crystalball` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\beta, m) =  \\begin{cases}\n",
            "                                N \\exp(-x^2 / 2),  &\\text{for } x > -\\beta\\\\\n",
            "                                N A (B - x)^{-m}  &\\text{for } x \\le -\\beta\n",
            "                              \\end{cases}\n",
            "        \n",
            "        where :math:`A = (m / |\\beta|)^m  \\exp(-\\beta^2 / 2)`,\n",
            "        :math:`B = m/|\\beta| - |\\beta|` and :math:`N` is a normalisation constant.\n",
            "        \n",
            "        `crystalball` takes :math:`\\beta > 0` and :math:`m > 1` as shape\n",
            "        parameters.  :math:`\\beta` defines the point where the pdf changes\n",
            "        from a power-law to a Gaussian distribution.  :math:`m` is the power\n",
            "        of the power-law tail.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``crystalball.pdf(x, beta, m, loc, scale)`` is identically\n",
            "        equivalent to ``crystalball.pdf(y, beta, m) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        .. versionadded:: 0.19.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Crystal Ball Function\",\n",
            "               https://en.wikipedia.org/wiki/Crystal_Ball_function\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import crystalball\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> beta, m = 2, 3\n",
            "        >>> mean, var, skew, kurt = crystalball.stats(beta, m, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(crystalball.ppf(0.01, beta, m),\n",
            "        ...                 crystalball.ppf(0.99, beta, m), 100)\n",
            "        >>> ax.plot(x, crystalball.pdf(x, beta, m),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='crystalball pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = crystalball(beta, m)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = crystalball.ppf([0.001, 0.5, 0.999], beta, m)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], crystalball.cdf(vals, beta, m))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = crystalball.rvs(beta, m, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    dgamma = <scipy.stats._continuous_distns.dgamma_gen object>\n",
            "        A double gamma continuous random variable.\n",
            "        \n",
            "        The double gamma distribution is also known as the reflected gamma\n",
            "        distribution [1]_.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `dgamma` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `dgamma` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = \\frac{1}{2\\Gamma(a)} |x|^{a-1} \\exp(-|x|)\n",
            "        \n",
            "        for a real number :math:`x` and :math:`a > 0`. :math:`\\Gamma` is the\n",
            "        gamma function (`scipy.special.gamma`).\n",
            "        \n",
            "        `dgamma` takes ``a`` as a shape parameter for :math:`a`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``dgamma.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``dgamma.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Johnson, Kotz, and Balakrishnan, \"Continuous Univariate\n",
            "               Distributions, Volume 1\", Second Edition, John Wiley and Sons\n",
            "               (1994).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import dgamma\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 1.1\n",
            "        >>> mean, var, skew, kurt = dgamma.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(dgamma.ppf(0.01, a),\n",
            "        ...                 dgamma.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, dgamma.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='dgamma pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = dgamma(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = dgamma.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], dgamma.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = dgamma.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    dirichlet = <scipy.stats._multivariate.dirichlet_gen object>\n",
            "        A Dirichlet random variable.\n",
            "        \n",
            "        The ``alpha`` keyword specifies the concentration parameters of the\n",
            "        distribution.\n",
            "        \n",
            "        .. versionadded:: 0.15.0\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(x, alpha)\n",
            "            Probability density function.\n",
            "        logpdf(x, alpha)\n",
            "            Log of the probability density function.\n",
            "        rvs(alpha, size=1, random_state=None)\n",
            "            Draw random samples from a Dirichlet distribution.\n",
            "        mean(alpha)\n",
            "            The mean of the Dirichlet distribution\n",
            "        var(alpha)\n",
            "            The variance of the Dirichlet distribution\n",
            "        cov(alpha)\n",
            "            The covariance of the Dirichlet distribution\n",
            "        entropy(alpha)\n",
            "            Compute the differential entropy of the Dirichlet distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        alpha : array_like\n",
            "            The concentration parameters. The number of entries determines the\n",
            "            dimensionality of the distribution.\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Each :math:`\\alpha` entry must be positive. The distribution has only\n",
            "        support on the simplex defined by\n",
            "        \n",
            "        .. math::\n",
            "            \\sum_{i=1}^{K} x_i = 1\n",
            "        \n",
            "        where :math:`0 < x_i < 1`.\n",
            "        \n",
            "        If the quantiles don't lie within the simplex, a ValueError is raised.\n",
            "        \n",
            "        The probability density function for `dirichlet` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\mathrm{B}(\\boldsymbol\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n",
            "        \n",
            "        where\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\mathrm{B}(\\boldsymbol\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\n",
            "                                         {\\Gamma\\bigl(\\sum_{i=1}^K \\alpha_i\\bigr)}\n",
            "        \n",
            "        and :math:`\\boldsymbol\\alpha=(\\alpha_1,\\ldots,\\alpha_K)`, the\n",
            "        concentration parameters and :math:`K` is the dimension of the space\n",
            "        where :math:`x` takes values.\n",
            "        \n",
            "        Note that the `dirichlet` interface is somewhat inconsistent.\n",
            "        The array returned by the rvs function is transposed\n",
            "        with respect to the format expected by the pdf and logpdf.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import dirichlet\n",
            "        \n",
            "        Generate a dirichlet random variable\n",
            "        \n",
            "        >>> quantiles = np.array([0.2, 0.2, 0.6])  # specify quantiles\n",
            "        >>> alpha = np.array([0.4, 5, 15])  # specify concentration parameters\n",
            "        >>> dirichlet.pdf(quantiles, alpha)\n",
            "        0.2843831684937255\n",
            "        \n",
            "        The same PDF but following a log scale\n",
            "        \n",
            "        >>> dirichlet.logpdf(quantiles, alpha)\n",
            "        -1.2574327653159187\n",
            "        \n",
            "        Once we specify the dirichlet distribution\n",
            "        we can then calculate quantities of interest\n",
            "        \n",
            "        >>> dirichlet.mean(alpha)  # get the mean of the distribution\n",
            "        array([0.01960784, 0.24509804, 0.73529412])\n",
            "        >>> dirichlet.var(alpha) # get variance\n",
            "        array([0.00089829, 0.00864603, 0.00909517])\n",
            "        >>> dirichlet.entropy(alpha)  # calculate the differential entropy\n",
            "        -4.3280162474082715\n",
            "        \n",
            "        We can also return random samples from the distribution\n",
            "        \n",
            "        >>> dirichlet.rvs(alpha, size=1, random_state=1)\n",
            "        array([[0.00766178, 0.24670518, 0.74563305]])\n",
            "        >>> dirichlet.rvs(alpha, size=2, random_state=2)\n",
            "        array([[0.01639427, 0.1292273 , 0.85437844],\n",
            "               [0.00156917, 0.19033695, 0.80809388]])\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix\n",
            "        concentration parameters, returning a \"frozen\" Dirichlet\n",
            "        random variable:\n",
            "        \n",
            "        >>> rv = dirichlet(alpha)\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # concentration parameters fixed.\n",
            "    \n",
            "    dirichlet_multinomial = <scipy.stats._multivariate.dirichlet_multinomi...\n",
            "        A Dirichlet multinomial random variable.\n",
            "        \n",
            "        The Dirichlet multinomial distribution is a compound probability\n",
            "        distribution: it is the multinomial distribution with number of trials\n",
            "        `n` and class probabilities ``p`` randomly sampled from a Dirichlet\n",
            "        distribution with concentration parameters ``alpha``.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        logpmf(x, alpha, n):\n",
            "            Log of the probability mass function.\n",
            "        pmf(x, alpha, n):\n",
            "            Probability mass function.\n",
            "        mean(alpha, n):\n",
            "            Mean of the Dirichlet multinomial distribution.\n",
            "        var(alpha, n):\n",
            "            Variance of the Dirichlet multinomial distribution.\n",
            "        cov(alpha, n):\n",
            "            The covariance of the Dirichlet multinomial distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        alpha : array_like\n",
            "            The concentration parameters. The number of entries along the last axis\n",
            "            determines the dimensionality of the distribution. Each entry must be\n",
            "            strictly positive.\n",
            "        n : int or array_like\n",
            "            The number of trials. Each element must be a strictly positive integer.\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.dirichlet : The dirichlet distribution.\n",
            "        scipy.stats.multinomial : The multinomial distribution.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Dirichlet-multinomial distribution, Wikipedia,\n",
            "               https://www.wikipedia.org/wiki/Dirichlet-multinomial_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> from scipy.stats import dirichlet_multinomial\n",
            "        \n",
            "        Get the PMF\n",
            "        \n",
            "        >>> n = 6  # number of trials\n",
            "        >>> alpha = [3, 4, 5]  # concentration parameters\n",
            "        >>> x = [1, 2, 3]  # counts\n",
            "        >>> dirichlet_multinomial.pmf(x, alpha, n)\n",
            "        0.08484162895927604\n",
            "        \n",
            "        If the sum of category counts does not equal the number of trials,\n",
            "        the probability mass is zero.\n",
            "        \n",
            "        >>> dirichlet_multinomial.pmf(x, alpha, n=7)\n",
            "        0.0\n",
            "        \n",
            "        Get the log of the PMF\n",
            "        \n",
            "        >>> dirichlet_multinomial.logpmf(x, alpha, n)\n",
            "        -2.4669689491013327\n",
            "        \n",
            "        Get the mean\n",
            "        \n",
            "        >>> dirichlet_multinomial.mean(alpha, n)\n",
            "        array([1.5, 2. , 2.5])\n",
            "        \n",
            "        Get the variance\n",
            "        \n",
            "        >>> dirichlet_multinomial.var(alpha, n)\n",
            "        array([1.55769231, 1.84615385, 2.01923077])\n",
            "        \n",
            "        Get the covariance\n",
            "        \n",
            "        >>> dirichlet_multinomial.cov(alpha, n)\n",
            "        array([[ 1.55769231, -0.69230769, -0.86538462],\n",
            "               [-0.69230769,  1.84615385, -1.15384615],\n",
            "               [-0.86538462, -1.15384615,  2.01923077]])\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the\n",
            "        `alpha` and `n` parameters, returning a \"frozen\" Dirichlet multinomial\n",
            "        random variable.\n",
            "        \n",
            "        >>> dm = dirichlet_multinomial(alpha, n)\n",
            "        >>> dm.pmf(x)\n",
            "        0.08484162895927579\n",
            "        \n",
            "        All methods are fully vectorized. Each element of `x` and `alpha` is\n",
            "        a vector (along the last axis), each element of `n` is an\n",
            "        integer (scalar), and the result is computed element-wise.\n",
            "        \n",
            "        >>> x = [[1, 2, 3], [4, 5, 6]]\n",
            "        >>> alpha = [[1, 2, 3], [4, 5, 6]]\n",
            "        >>> n = [6, 15]\n",
            "        >>> dirichlet_multinomial.pmf(x, alpha, n)\n",
            "        array([0.06493506, 0.02626937])\n",
            "        \n",
            "        >>> dirichlet_multinomial.cov(alpha, n).shape  # both covariance matrices\n",
            "        (2, 3, 3)\n",
            "        \n",
            "        Broadcasting according to standard NumPy conventions is supported. Here,\n",
            "        we have four sets of concentration parameters (each a two element vector)\n",
            "        for each of three numbers of trials (each a scalar).\n",
            "        \n",
            "        >>> alpha = [[3, 4], [4, 5], [5, 6], [6, 7]]\n",
            "        >>> n = [[6], [7], [8]]\n",
            "        >>> dirichlet_multinomial.mean(alpha, n).shape\n",
            "        (3, 4, 2)\n",
            "    \n",
            "    dlaplace = <scipy.stats._discrete_distns.dlaplace_gen object>\n",
            "        A  Laplacian discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `dlaplace` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, a, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, a, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, a, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, a, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, a, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, a, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(a, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `dlaplace` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = \\tanh(a/2) \\exp(-a |k|)\n",
            "        \n",
            "        for integers :math:`k` and :math:`a > 0`.\n",
            "        \n",
            "        `dlaplace` takes :math:`a` as shape parameter.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``dlaplace.pmf(k, a, loc)`` is identically\n",
            "        equivalent to ``dlaplace.pmf(k - loc, a)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import dlaplace\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 0.8\n",
            "        >>> mean, var, skew, kurt = dlaplace.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(dlaplace.ppf(0.01, a),\n",
            "        ...               dlaplace.ppf(0.99, a))\n",
            "        >>> ax.plot(x, dlaplace.pmf(x, a), 'bo', ms=8, label='dlaplace pmf')\n",
            "        >>> ax.vlines(x, 0, dlaplace.pmf(x, a), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = dlaplace(a)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = dlaplace.cdf(x, a)\n",
            "        >>> np.allclose(x, dlaplace.ppf(prob, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = dlaplace.rvs(a, size=1000)\n",
            "    \n",
            "    dweibull = <scipy.stats._continuous_distns.dweibull_gen object>\n",
            "        A double Weibull continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `dweibull` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `dweibull` is given by\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c / 2 |x|^{c-1} \\exp(-|x|^c)\n",
            "        \n",
            "        for a real number :math:`x` and :math:`c > 0`.\n",
            "        \n",
            "        `dweibull` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``dweibull.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``dweibull.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import dweibull\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 2.07\n",
            "        >>> mean, var, skew, kurt = dweibull.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(dweibull.ppf(0.01, c),\n",
            "        ...                 dweibull.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, dweibull.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='dweibull pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = dweibull(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = dweibull.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], dweibull.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = dweibull.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    erlang = <scipy.stats._continuous_distns.erlang_gen object>\n",
            "        An Erlang continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `erlang` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gamma\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The Erlang distribution is a special case of the Gamma distribution, with\n",
            "        the shape parameter `a` an integer.  Note that this restriction is not\n",
            "        enforced by `erlang`. It will, however, generate a warning the first time\n",
            "        a non-integer value is used for the shape parameter.\n",
            "        \n",
            "        Refer to `gamma` for examples.\n",
            "    \n",
            "    expon = <scipy.stats._continuous_distns.expon_gen object>\n",
            "        An exponential continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `expon` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `expon` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\exp(-x)\n",
            "        \n",
            "        for :math:`x \\ge 0`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``expon.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``expon.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        A common parameterization for `expon` is in terms of the rate parameter\n",
            "        ``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This\n",
            "        parameterization corresponds to using ``scale = 1 / lambda``.\n",
            "        \n",
            "        The exponential distribution is a special case of the gamma\n",
            "        distributions, with gamma shape parameter ``a = 1``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import expon\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = expon.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(expon.ppf(0.01),\n",
            "        ...                 expon.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, expon.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='expon pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = expon()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = expon.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], expon.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = expon.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    exponnorm = <scipy.stats._continuous_distns.exponnorm_gen object>\n",
            "        An exponentially modified Normal continuous random variable.\n",
            "        \n",
            "        Also known as the exponentially modified Gaussian distribution [1]_.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `exponnorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(K, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, K, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, K, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, K, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, K, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, K, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, K, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, K, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, K, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, K, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(K, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(K, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(K,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(K, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(K, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(K, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(K, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, K, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `exponnorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, K) = \\frac{1}{2K} \\exp\\left(\\frac{1}{2 K^2} - x / K \\right)\n",
            "                      \\text{erfc}\\left(-\\frac{x - 1/K}{\\sqrt{2}}\\right)\n",
            "        \n",
            "        where :math:`x` is a real number and :math:`K > 0`.\n",
            "        \n",
            "        It can be thought of as the sum of a standard normal random variable\n",
            "        and an independent exponentially distributed random variable with rate\n",
            "        ``1/K``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``exponnorm.pdf(x, K, loc, scale)`` is identically\n",
            "        equivalent to ``exponnorm.pdf(y, K) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        An alternative parameterization of this distribution (for example, in\n",
            "        the Wikipedia article [1]_) involves three parameters, :math:`\\mu`,\n",
            "        :math:`\\lambda` and :math:`\\sigma`.\n",
            "        \n",
            "        In the present parameterization this corresponds to having ``loc`` and\n",
            "        ``scale`` equal to :math:`\\mu` and :math:`\\sigma`, respectively, and\n",
            "        shape parameter :math:`K = 1/(\\sigma\\lambda)`.\n",
            "        \n",
            "        .. versionadded:: 0.16.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Exponentially modified Gaussian distribution, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import exponnorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> K = 1.5\n",
            "        >>> mean, var, skew, kurt = exponnorm.stats(K, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(exponnorm.ppf(0.01, K),\n",
            "        ...                 exponnorm.ppf(0.99, K), 100)\n",
            "        >>> ax.plot(x, exponnorm.pdf(x, K),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='exponnorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = exponnorm(K)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = exponnorm.ppf([0.001, 0.5, 0.999], K)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], exponnorm.cdf(vals, K))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = exponnorm.rvs(K, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    exponpow = <scipy.stats._continuous_distns.exponpow_gen object>\n",
            "        An exponential power continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `exponpow` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `exponpow` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, b) = b x^{b-1} \\exp(1 + x^b - \\exp(x^b))\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`b > 0`.  Note that this is a different\n",
            "        distribution from the exponential power distribution that is also known\n",
            "        under the names \"generalized normal\" or \"generalized Gaussian\".\n",
            "        \n",
            "        `exponpow` takes ``b`` as a shape parameter for :math:`b`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``exponpow.pdf(x, b, loc, scale)`` is identically\n",
            "        equivalent to ``exponpow.pdf(y, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import exponpow\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> b = 2.7\n",
            "        >>> mean, var, skew, kurt = exponpow.stats(b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(exponpow.ppf(0.01, b),\n",
            "        ...                 exponpow.ppf(0.99, b), 100)\n",
            "        >>> ax.plot(x, exponpow.pdf(x, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='exponpow pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = exponpow(b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = exponpow.ppf([0.001, 0.5, 0.999], b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], exponpow.cdf(vals, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = exponpow.rvs(b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    exponweib = <scipy.stats._continuous_distns.exponweib_gen object>\n",
            "        An exponentiated Weibull continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `exponweib` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        weibull_min, numpy.random.Generator.weibull\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `exponweib` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, c) = a c [1-\\exp(-x^c)]^{a-1} \\exp(-x^c) x^{c-1}\n",
            "        \n",
            "        and its cumulative distribution function is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            F(x, a, c) = [1-\\exp(-x^c)]^a\n",
            "        \n",
            "        for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.\n",
            "        \n",
            "        `exponweib` takes :math:`a` and :math:`c` as shape parameters:\n",
            "        \n",
            "        * :math:`a` is the exponentiation parameter,\n",
            "          with the special case :math:`a=1` corresponding to the\n",
            "          (non-exponentiated) Weibull distribution `weibull_min`.\n",
            "        * :math:`c` is the shape parameter of the non-exponentiated Weibull law.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``exponweib.pdf(x, a, c, loc, scale)`` is identically\n",
            "        equivalent to ``exponweib.pdf(y, a, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import exponweib\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, c = 2.89, 1.95\n",
            "        >>> mean, var, skew, kurt = exponweib.stats(a, c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(exponweib.ppf(0.01, a, c),\n",
            "        ...                 exponweib.ppf(0.99, a, c), 100)\n",
            "        >>> ax.plot(x, exponweib.pdf(x, a, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='exponweib pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = exponweib(a, c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = exponweib.ppf([0.001, 0.5, 0.999], a, c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], exponweib.cdf(vals, a, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = exponweib.rvs(a, c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    f = <scipy.stats._continuous_distns.f_gen object>\n",
            "        An F continuous random variable.\n",
            "        \n",
            "        For the noncentral F distribution, see `ncf`.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `f` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(dfn, dfd, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, dfn, dfd, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, dfn, dfd, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, dfn, dfd, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, dfn, dfd, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(dfn, dfd, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(dfn, dfd, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(dfn, dfd), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(dfn, dfd, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(dfn, dfd, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(dfn, dfd, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(dfn, dfd, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, dfn, dfd, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ncf\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The F distribution with :math:`df_1 > 0` and :math:`df_2 > 0` degrees of freedom is\n",
            "        the distribution of the ratio of two independent chi-squared distributions with\n",
            "        :math:`df_1` and :math:`df_2` degrees of freedom, after rescaling by\n",
            "        :math:`df_2 / df_1`.\n",
            "        \n",
            "        The probability density function for `f` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, df_1, df_2) = \\frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}\n",
            "                                    {(df_2+df_1 x)^{(df_1+df_2)/2}\n",
            "                                     B(df_1/2, df_2/2)}\n",
            "        \n",
            "        for :math:`x > 0`.\n",
            "        \n",
            "        `f` accepts shape parameters ``dfn`` and ``dfd`` for :math:`df_1`, the degrees of\n",
            "        freedom of the chi-squared distribution in the numerator, and :math:`df_2`, the\n",
            "        degrees of freedom of the chi-squared distribution in the denominator, respectively.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``f.pdf(x, dfn, dfd, loc, scale)`` is identically\n",
            "        equivalent to ``f.pdf(y, dfn, dfd) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import f\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> dfn, dfd = 29, 18\n",
            "        >>> mean, var, skew, kurt = f.stats(dfn, dfd, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(f.ppf(0.01, dfn, dfd),\n",
            "        ...                 f.ppf(0.99, dfn, dfd), 100)\n",
            "        >>> ax.plot(x, f.pdf(x, dfn, dfd),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='f pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = f(dfn, dfd)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = f.rvs(dfn, dfd, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    fatiguelife = <scipy.stats._continuous_distns.fatiguelife_gen object>\n",
            "        A fatigue-life (Birnbaum-Saunders) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `fatiguelife` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `fatiguelife` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{x+1}{2c\\sqrt{2\\pi x^3}} \\exp(-\\frac{(x-1)^2}{2x c^2})\n",
            "        \n",
            "        for :math:`x >= 0` and :math:`c > 0`.\n",
            "        \n",
            "        `fatiguelife` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``fatiguelife.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``fatiguelife.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Birnbaum-Saunders distribution\",\n",
            "               https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import fatiguelife\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 29\n",
            "        >>> mean, var, skew, kurt = fatiguelife.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(fatiguelife.ppf(0.01, c),\n",
            "        ...                 fatiguelife.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, fatiguelife.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='fatiguelife pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = fatiguelife(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = fatiguelife.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], fatiguelife.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = fatiguelife.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    fisk = <scipy.stats._continuous_distns.fisk_gen object>\n",
            "        A Fisk continuous random variable.\n",
            "        \n",
            "        The Fisk distribution is also known as the log-logistic distribution.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `fisk` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        burr\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `fisk` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{c x^{c-1}}\n",
            "                           {(1 + x^c)^2}\n",
            "        \n",
            "        for :math:`x >= 0` and :math:`c > 0`.\n",
            "        \n",
            "        Please note that the above expression can be transformed into the following\n",
            "        one, which is also commonly used:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{c x^{-c-1}}\n",
            "                           {(1 + x^{-c})^2}\n",
            "        \n",
            "        `fisk` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        `fisk` is a special case of `burr` or `burr12` with ``d=1``.\n",
            "        \n",
            "        Suppose ``X`` is a logistic random variable with location ``l``\n",
            "        and scale ``s``. Then ``Y = exp(X)`` is a Fisk (log-logistic)\n",
            "        random variable with ``scale = exp(l)`` and shape ``c = 1/s``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``fisk.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``fisk.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import fisk\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 3.09\n",
            "        >>> mean, var, skew, kurt = fisk.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(fisk.ppf(0.01, c),\n",
            "        ...                 fisk.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, fisk.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='fisk pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = fisk(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = fisk.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], fisk.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = fisk.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    foldcauchy = <scipy.stats._continuous_distns.foldcauchy_gen object>\n",
            "        A folded Cauchy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `foldcauchy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `foldcauchy` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{1}{\\pi (1+(x-c)^2)} + \\frac{1}{\\pi (1+(x+c)^2)}\n",
            "        \n",
            "        for :math:`x \\ge 0` and :math:`c \\ge 0`.\n",
            "        \n",
            "        `foldcauchy` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import foldcauchy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 4.72\n",
            "        >>> mean, var, skew, kurt = foldcauchy.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(foldcauchy.ppf(0.01, c),\n",
            "        ...                 foldcauchy.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, foldcauchy.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='foldcauchy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = foldcauchy(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = foldcauchy.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], foldcauchy.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = foldcauchy.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    foldnorm = <scipy.stats._continuous_distns.foldnorm_gen object>\n",
            "        A folded normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `foldnorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `foldnorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\sqrt{2/\\pi} cosh(c x) \\exp(-\\frac{x^2+c^2}{2})\n",
            "        \n",
            "        for :math:`x \\ge 0` and :math:`c \\ge 0`.\n",
            "        \n",
            "        `foldnorm` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``foldnorm.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``foldnorm.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import foldnorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 1.95\n",
            "        >>> mean, var, skew, kurt = foldnorm.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(foldnorm.ppf(0.01, c),\n",
            "        ...                 foldnorm.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, foldnorm.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='foldnorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = foldnorm(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = foldnorm.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], foldnorm.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = foldnorm.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gamma = <scipy.stats._continuous_distns.gamma_gen object>\n",
            "        A gamma continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gamma` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        erlang, expon\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gamma` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = \\frac{x^{a-1} e^{-x}}{\\Gamma(a)}\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`a > 0`. Here :math:`\\Gamma(a)` refers to the\n",
            "        gamma function.\n",
            "        \n",
            "        `gamma` takes ``a`` as a shape parameter for :math:`a`.\n",
            "        \n",
            "        When :math:`a` is an integer, `gamma` reduces to the Erlang\n",
            "        distribution, and when :math:`a=1` to the exponential distribution.\n",
            "        \n",
            "        Gamma distributions are sometimes parameterized with two variables,\n",
            "        with a probability density function of:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\alpha, \\beta) =\n",
            "            \\frac{\\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x }}{\\Gamma(\\alpha)}\n",
            "        \n",
            "        Note that this parameterization is equivalent to the above, with\n",
            "        ``scale = 1 / beta``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gamma.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``gamma.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gamma\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 1.99\n",
            "        >>> mean, var, skew, kurt = gamma.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gamma.ppf(0.01, a),\n",
            "        ...                 gamma.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, gamma.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gamma pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gamma(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gamma.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gamma.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gausshyper = <scipy.stats._continuous_distns.gausshyper_gen object>\n",
            "        A Gauss hypergeometric continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gausshyper` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, c, z, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, c, z, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, c, z, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, c, z, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, c, z, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, c, z, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, c, z, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b, c, z), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, c, z, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, c, z, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, c, z, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, c, z, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, c, z, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gausshyper` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}\n",
            "        \n",
            "        for :math:`0 \\le x \\le 1`, :math:`a,b > 0`, :math:`c` a real number,\n",
            "        :math:`z > -1`, and :math:`C = \\frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.\n",
            "        :math:`F[2, 1]` is the Gauss hypergeometric function\n",
            "        `scipy.special.hyp2f1`.\n",
            "        \n",
            "        `gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape\n",
            "        parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gausshyper.pdf(x, a, b, c, z, loc, scale)`` is identically\n",
            "        equivalent to ``gausshyper.pdf(y, a, b, c, z) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Armero, C., and M. J. Bayarri. \"Prior Assessments for Prediction in\n",
            "               Queues.\" *Journal of the Royal Statistical Society*. Series D (The\n",
            "               Statistician) 43, no. 1 (1994): 139-53. doi:10.2307/2348939\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gausshyper\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b, c, z = 13.8, 3.12, 2.51, 5.18\n",
            "        >>> mean, var, skew, kurt = gausshyper.stats(a, b, c, z, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gausshyper.ppf(0.01, a, b, c, z),\n",
            "        ...                 gausshyper.ppf(0.99, a, b, c, z), 100)\n",
            "        >>> ax.plot(x, gausshyper.pdf(x, a, b, c, z),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gausshyper pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gausshyper(a, b, c, z)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gausshyper.ppf([0.001, 0.5, 0.999], a, b, c, z)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gausshyper.cdf(vals, a, b, c, z))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gausshyper.rvs(a, b, c, z, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genexpon = <scipy.stats._continuous_distns.genexpon_gen object>\n",
            "        A generalized exponential continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genexpon` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `genexpon` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b, c) = (a + b (1 - \\exp(-c x)))\n",
            "                            \\exp(-a x - b x + \\frac{b}{c}  (1-\\exp(-c x)))\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`a, b, c > 0`.\n",
            "        \n",
            "        `genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genexpon.pdf(x, a, b, c, loc, scale)`` is identically\n",
            "        equivalent to ``genexpon.pdf(y, a, b, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        H.K. Ryu, \"An Extension of Marshall and Olkin's Bivariate Exponential\n",
            "        Distribution\", Journal of the American Statistical Association, 1993.\n",
            "        \n",
            "        N. Balakrishnan, Asit P. Basu (editors), *The Exponential Distribution:\n",
            "        Theory, Methods and Applications*, Gordon and Breach, 1995.\n",
            "        ISBN 10: 2884491929\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genexpon\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b, c = 9.13, 16.2, 3.28\n",
            "        >>> mean, var, skew, kurt = genexpon.stats(a, b, c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genexpon.ppf(0.01, a, b, c),\n",
            "        ...                 genexpon.ppf(0.99, a, b, c), 100)\n",
            "        >>> ax.plot(x, genexpon.pdf(x, a, b, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genexpon pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genexpon(a, b, c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genexpon.ppf([0.001, 0.5, 0.999], a, b, c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genexpon.cdf(vals, a, b, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genexpon.rvs(a, b, c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genextreme = <scipy.stats._continuous_distns.genextreme_gen object>\n",
            "        A generalized extreme value continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genextreme` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gumbel_r\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        For :math:`c=0`, `genextreme` is equal to `gumbel_r` with\n",
            "        probability density function\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\exp(-\\exp(-x)) \\exp(-x),\n",
            "        \n",
            "        where :math:`-\\infty < x < \\infty`.\n",
            "        \n",
            "        For :math:`c \\ne 0`, the probability density function for `genextreme` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1},\n",
            "        \n",
            "        where :math:`-\\infty < x \\le 1/c` if :math:`c > 0` and\n",
            "        :math:`1/c \\le x < \\infty` if :math:`c < 0`.\n",
            "        \n",
            "        Note that several sources and software packages use the opposite\n",
            "        convention for the sign of the shape parameter :math:`c`.\n",
            "        \n",
            "        `genextreme` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genextreme.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``genextreme.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genextreme\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = -0.1\n",
            "        >>> mean, var, skew, kurt = genextreme.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genextreme.ppf(0.01, c),\n",
            "        ...                 genextreme.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, genextreme.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genextreme pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genextreme(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genextreme.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genextreme.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gengamma = <scipy.stats._continuous_distns.gengamma_gen object>\n",
            "        A generalized gamma continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gengamma` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gamma, invgamma, weibull_min\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gengamma` is ([1]_):\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, c) = \\frac{|c| x^{c a-1} \\exp(-x^c)}{\\Gamma(a)}\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`a > 0`, and :math:`c \\ne 0`.\n",
            "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
            "        \n",
            "        `gengamma` takes :math:`a` and :math:`c` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gengamma.pdf(x, a, c, loc, scale)`` is identically\n",
            "        equivalent to ``gengamma.pdf(y, a, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] E.W. Stacy, \"A Generalization of the Gamma Distribution\",\n",
            "           Annals of Mathematical Statistics, Vol 33(3), pp. 1187--1192.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gengamma\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, c = 4.42, -3.12\n",
            "        >>> mean, var, skew, kurt = gengamma.stats(a, c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gengamma.ppf(0.01, a, c),\n",
            "        ...                 gengamma.ppf(0.99, a, c), 100)\n",
            "        >>> ax.plot(x, gengamma.pdf(x, a, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gengamma pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gengamma(a, c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gengamma.ppf([0.001, 0.5, 0.999], a, c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gengamma.cdf(vals, a, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gengamma.rvs(a, c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genhalflogistic = <scipy.stats._continuous_distns.genhalflogistic_gen ...\n",
            "        A generalized half-logistic continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genhalflogistic` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `genhalflogistic` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}\n",
            "        \n",
            "        for :math:`0 \\le x \\le 1/c`, and :math:`c > 0`.\n",
            "        \n",
            "        `genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genhalflogistic.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``genhalflogistic.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genhalflogistic\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.773\n",
            "        >>> mean, var, skew, kurt = genhalflogistic.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genhalflogistic.ppf(0.01, c),\n",
            "        ...                 genhalflogistic.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, genhalflogistic.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genhalflogistic pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genhalflogistic(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genhalflogistic.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genhalflogistic.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genhalflogistic.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genhyperbolic = <scipy.stats._continuous_distns.genhyperbolic_gen obje...\n",
            "        A generalized hyperbolic continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genhyperbolic` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(p, a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, p, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, p, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, p, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, p, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, p, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, p, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, p, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, p, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, p, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(p, a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(p, a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(p, a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(p, a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(p, a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(p, a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(p, a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, p, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        t, norminvgauss, geninvgauss, laplace, cauchy\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `genhyperbolic` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, p, a, b) =\n",
            "                \\frac{(a^2 - b^2)^{p/2}}\n",
            "                {\\sqrt{2\\pi}a^{p-1/2}\n",
            "                K_p\\Big(\\sqrt{a^2 - b^2}\\Big)}\n",
            "                e^{bx} \\times \\frac{K_{p - 1/2}\n",
            "                (a \\sqrt{1 + x^2})}\n",
            "                {(\\sqrt{1 + x^2})^{1/2 - p}}\n",
            "        \n",
            "        for :math:`x, p \\in ( - \\infty; \\infty)`,\n",
            "        :math:`|b| < a` if :math:`p \\ge 0`,\n",
            "        :math:`|b| \\le a` if :math:`p < 0`.\n",
            "        :math:`K_{p}(.)` denotes the modified Bessel function of the second\n",
            "        kind and order :math:`p` (`scipy.special.kv`)\n",
            "        \n",
            "        `genhyperbolic` takes ``p`` as a tail parameter,\n",
            "        ``a`` as a shape parameter,\n",
            "        ``b`` as a skewness parameter.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genhyperbolic.pdf(x, p, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``genhyperbolic.pdf(y, p, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The original parameterization of the Generalized Hyperbolic Distribution\n",
            "        is found in [1]_ as follows\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\lambda, \\alpha, \\beta, \\delta, \\mu) =\n",
            "               \\frac{(\\gamma/\\delta)^\\lambda}{\\sqrt{2\\pi}K_\\lambda(\\delta \\gamma)}\n",
            "               e^{\\beta (x - \\mu)} \\times \\frac{K_{\\lambda - 1/2}\n",
            "               (\\alpha \\sqrt{\\delta^2 + (x - \\mu)^2})}\n",
            "               {(\\sqrt{\\delta^2 + (x - \\mu)^2} / \\alpha)^{1/2 - \\lambda}}\n",
            "        \n",
            "        for :math:`x \\in ( - \\infty; \\infty)`,\n",
            "        :math:`\\gamma := \\sqrt{\\alpha^2 - \\beta^2}`,\n",
            "        :math:`\\lambda, \\mu \\in ( - \\infty; \\infty)`,\n",
            "        :math:`\\delta \\ge 0, |\\beta| < \\alpha` if :math:`\\lambda \\ge 0`,\n",
            "        :math:`\\delta > 0, |\\beta| \\le \\alpha` if :math:`\\lambda < 0`.\n",
            "        \n",
            "        The location-scale-based parameterization implemented in\n",
            "        SciPy is based on [2]_, where :math:`a = \\alpha\\delta`,\n",
            "        :math:`b = \\beta\\delta`, :math:`p = \\lambda`,\n",
            "        :math:`scale=\\delta` and :math:`loc=\\mu`\n",
            "        \n",
            "        Moments are implemented based on [3]_ and [4]_.\n",
            "        \n",
            "        For the distributions that are a special case such as Student's t,\n",
            "        it is not recommended to rely on the implementation of genhyperbolic.\n",
            "        To avoid potential numerical problems and for performance reasons,\n",
            "        the methods of the specific distributions should be used.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions\n",
            "           on Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n",
            "           pp. 151-157, 1978. https://www.jstor.org/stable/4615705\n",
            "        \n",
            "        .. [2] Eberlein E., Prause K. (2002) The Generalized Hyperbolic Model:\n",
            "            Financial Derivatives and Risk Measures. In: Geman H., Madan D.,\n",
            "            Pliska S.R., Vorst T. (eds) Mathematical Finance - Bachelier\n",
            "            Congress 2000. Springer Finance. Springer, Berlin, Heidelberg.\n",
            "            :doi:`10.1007/978-3-662-12429-1_12`\n",
            "        \n",
            "        .. [3] Scott, David J, Wrtz, Diethelm, Dong, Christine and Tran,\n",
            "           Thanh Tam, (2009), Moments of the generalized hyperbolic\n",
            "           distribution, MPRA Paper, University Library of Munich, Germany,\n",
            "           https://EconPapers.repec.org/RePEc:pra:mprapa:19081.\n",
            "        \n",
            "        .. [4] E. Eberlein and E. A. von Hammerstein. Generalized hyperbolic\n",
            "           and inverse Gaussian distributions: Limiting cases and approximation\n",
            "           of processes. FDM Preprint 80, April 2003. University of Freiburg.\n",
            "           https://freidok.uni-freiburg.de/fedora/objects/freidok:7974/datastreams/FILE1/content\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genhyperbolic\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> p, a, b = 0.5, 1.5, -0.5\n",
            "        >>> mean, var, skew, kurt = genhyperbolic.stats(p, a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genhyperbolic.ppf(0.01, p, a, b),\n",
            "        ...                 genhyperbolic.ppf(0.99, p, a, b), 100)\n",
            "        >>> ax.plot(x, genhyperbolic.pdf(x, p, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genhyperbolic pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genhyperbolic(p, a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genhyperbolic.ppf([0.001, 0.5, 0.999], p, a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genhyperbolic.cdf(vals, p, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genhyperbolic.rvs(p, a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    geninvgauss = <scipy.stats._continuous_distns.geninvgauss_gen object>\n",
            "        A Generalized Inverse Gaussian continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `geninvgauss` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(p, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, p, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, p, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, p, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, p, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, p, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, p, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, p, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, p, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, p, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(p, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(p, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(p, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(p, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(p, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(p, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(p, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, p, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `geninvgauss` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, p, b) = x^{p-1} \\exp(-b (x + 1/x) / 2) / (2 K_p(b))\n",
            "        \n",
            "        where `x > 0`, `p` is a real number and `b > 0`\\([1]_).\n",
            "        :math:`K_p` is the modified Bessel function of second kind of order `p`\n",
            "        (`scipy.special.kv`).\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``geninvgauss.pdf(x, p, b, loc, scale)`` is identically\n",
            "        equivalent to ``geninvgauss.pdf(y, p, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of\n",
            "        `geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.\n",
            "        \n",
            "        Generating random variates is challenging for this distribution. The\n",
            "        implementation is based on [2]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, \"First hitting time\n",
            "           models for the generalized inverse gaussian distribution\",\n",
            "           Stochastic Processes and their Applications 7, pp. 49--54, 1978.\n",
            "        \n",
            "        .. [2] W. Hoermann and J. Leydold, \"Generating generalized inverse Gaussian\n",
            "           random variates\", Statistics and Computing, 24(4), p. 547--557, 2014.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import geninvgauss\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> p, b = 2.3, 1.5\n",
            "        >>> mean, var, skew, kurt = geninvgauss.stats(p, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(geninvgauss.ppf(0.01, p, b),\n",
            "        ...                 geninvgauss.ppf(0.99, p, b), 100)\n",
            "        >>> ax.plot(x, geninvgauss.pdf(x, p, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='geninvgauss pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = geninvgauss(p, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = geninvgauss.ppf([0.001, 0.5, 0.999], p, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], geninvgauss.cdf(vals, p, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = geninvgauss.rvs(p, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genlogistic = <scipy.stats._continuous_distns.genlogistic_gen object>\n",
            "        A generalized logistic continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genlogistic` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `genlogistic` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c \\frac{\\exp(-x)}\n",
            "                             {(1 + \\exp(-x))^{c+1}}\n",
            "        \n",
            "        for real :math:`x` and :math:`c > 0`. In literature, different\n",
            "        generalizations of the logistic distribution can be found. This is the type 1\n",
            "        generalized logistic distribution according to [1]_. It is also referred to\n",
            "        as the skew-logistic distribution [2]_.\n",
            "        \n",
            "        `genlogistic` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genlogistic.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``genlogistic.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Johnson et al. \"Continuous Univariate Distributions\", Volume 2,\n",
            "               Wiley. 1995.\n",
            "        .. [2] \"Generalized Logistic Distribution\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Generalized_logistic_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genlogistic\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.412\n",
            "        >>> mean, var, skew, kurt = genlogistic.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genlogistic.ppf(0.01, c),\n",
            "        ...                 genlogistic.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, genlogistic.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genlogistic pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genlogistic(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genlogistic.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genlogistic.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genlogistic.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gennorm = <scipy.stats._continuous_distns.gennorm_gen object>\n",
            "        A generalized normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gennorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(beta, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, beta, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, beta, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, beta, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, beta, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, beta, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, beta, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, beta, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, beta, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, beta, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(beta, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(beta, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(beta, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(beta, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(beta, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(beta, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, beta, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        laplace : Laplace distribution\n",
            "        norm : normal distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gennorm` is [1]_:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\beta) = \\frac{\\beta}{2 \\Gamma(1/\\beta)} \\exp(-|x|^\\beta),\n",
            "        \n",
            "        where :math:`x` is a real number, :math:`\\beta > 0` and\n",
            "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
            "        \n",
            "        `gennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n",
            "        For :math:`\\beta = 1`, it is identical to a Laplace distribution.\n",
            "        For :math:`\\beta = 2`, it is identical to a normal distribution\n",
            "        (with ``scale=1/sqrt(2)``).\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] \"Generalized normal distribution, Version 1\",\n",
            "               https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n",
            "        \n",
            "        .. [2] Nardon, Martina, and Paolo Pianca. \"Simulation techniques for\n",
            "               generalized Gaussian densities.\" Journal of Statistical\n",
            "               Computation and Simulation 79.11 (2009): 1317-1329\n",
            "        \n",
            "        .. [3] Wicklin, Rick. \"Simulate data from a generalized Gaussian\n",
            "               distribution\" in The DO Loop blog, September 21, 2016,\n",
            "               https://blogs.sas.com/content/iml/2016/09/21/simulate-generalized-gaussian-sas.html\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gennorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> beta = 1.3\n",
            "        >>> mean, var, skew, kurt = gennorm.stats(beta, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gennorm.ppf(0.01, beta),\n",
            "        ...                 gennorm.ppf(0.99, beta), 100)\n",
            "        >>> ax.plot(x, gennorm.pdf(x, beta),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gennorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gennorm(beta)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gennorm.ppf([0.001, 0.5, 0.999], beta)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gennorm.cdf(vals, beta))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gennorm.rvs(beta, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    genpareto = <scipy.stats._continuous_distns.genpareto_gen object>\n",
            "        A generalized Pareto continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `genpareto` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `genpareto` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = (1 + c x)^{-1 - 1/c}\n",
            "        \n",
            "        defined for :math:`x \\ge 0` if :math:`c \\ge 0`, and for\n",
            "        :math:`0 \\le x \\le -1/c` if :math:`c < 0`.\n",
            "        \n",
            "        `genpareto` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        For :math:`c=0`, `genpareto` reduces to the exponential\n",
            "        distribution, `expon`:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, 0) = \\exp(-x)\n",
            "        \n",
            "        For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, -1) = 1\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``genpareto.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``genpareto.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import genpareto\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.1\n",
            "        >>> mean, var, skew, kurt = genpareto.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(genpareto.ppf(0.01, c),\n",
            "        ...                 genpareto.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, genpareto.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='genpareto pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = genpareto(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = genpareto.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], genpareto.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = genpareto.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    geom = <scipy.stats._discrete_distns.geom_gen object>\n",
            "        A geometric discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `geom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(p, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, p, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, p, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, p, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, p, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, p, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, p, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, p, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, p, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(p, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(p, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(p, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(p, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(p, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(p, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, p, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `geom` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = (1-p)^{k-1} p\n",
            "        \n",
            "        for :math:`k \\ge 1`, :math:`0 < p \\leq 1`\n",
            "        \n",
            "        `geom` takes :math:`p` as shape parameter,\n",
            "        where :math:`p` is the probability of a single success\n",
            "        and :math:`1-p` is the probability of a single failure.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``geom.pmf(k, p, loc)`` is identically\n",
            "        equivalent to ``geom.pmf(k - loc, p)``.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        planck\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import geom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> p = 0.5\n",
            "        >>> mean, var, skew, kurt = geom.stats(p, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(geom.ppf(0.01, p),\n",
            "        ...               geom.ppf(0.99, p))\n",
            "        >>> ax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\n",
            "        >>> ax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = geom(p)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = geom.cdf(x, p)\n",
            "        >>> np.allclose(x, geom.ppf(prob, p))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = geom.rvs(p, size=1000)\n",
            "    \n",
            "    gibrat = <scipy.stats._continuous_distns.gibrat_gen object>\n",
            "        A Gibrat continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gibrat` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gibrat` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{x \\sqrt{2\\pi}} \\exp(-\\frac{1}{2} (\\log(x))^2)\n",
            "        \n",
            "        `gibrat` is a special case of `lognorm` with ``s=1``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gibrat.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``gibrat.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gibrat\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = gibrat.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gibrat.ppf(0.01),\n",
            "        ...                 gibrat.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, gibrat.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gibrat pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gibrat()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gibrat.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gibrat.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gibrat.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gompertz = <scipy.stats._continuous_distns.gompertz_gen object>\n",
            "        A Gompertz (or truncated Gumbel) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gompertz` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gompertz` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c \\exp(x) \\exp(-c (e^x-1))\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`c > 0`.\n",
            "        \n",
            "        `gompertz` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gompertz.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``gompertz.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gompertz\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.947\n",
            "        >>> mean, var, skew, kurt = gompertz.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gompertz.ppf(0.01, c),\n",
            "        ...                 gompertz.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, gompertz.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gompertz pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gompertz(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gompertz.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gompertz.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gompertz.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gumbel_l = <scipy.stats._continuous_distns.gumbel_l_gen object>\n",
            "        A left-skewed Gumbel continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gumbel_l` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gumbel_r, gompertz, genextreme\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gumbel_l` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\exp(x - e^x)\n",
            "        \n",
            "        The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n",
            "        distribution.  It is also related to the extreme value distribution,\n",
            "        log-Weibull and Gompertz distributions.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gumbel_l.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``gumbel_l.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gumbel_l\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = gumbel_l.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gumbel_l.ppf(0.01),\n",
            "        ...                 gumbel_l.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, gumbel_l.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gumbel_l pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gumbel_l()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gumbel_l.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gumbel_l.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gumbel_l.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    gumbel_r = <scipy.stats._continuous_distns.gumbel_r_gen object>\n",
            "        A right-skewed Gumbel continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `gumbel_r` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gumbel_l, gompertz, genextreme\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `gumbel_r` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\exp(-(x + e^{-x}))\n",
            "        \n",
            "        The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n",
            "        distribution.  It is also related to the extreme value distribution,\n",
            "        log-Weibull and Gompertz distributions.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``gumbel_r.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``gumbel_r.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import gumbel_r\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = gumbel_r.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(gumbel_r.ppf(0.01),\n",
            "        ...                 gumbel_r.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, gumbel_r.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='gumbel_r pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = gumbel_r()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = gumbel_r.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], gumbel_r.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = gumbel_r.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    halfcauchy = <scipy.stats._continuous_distns.halfcauchy_gen object>\n",
            "        A Half-Cauchy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `halfcauchy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `halfcauchy` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{2}{\\pi (1 + x^2)}\n",
            "        \n",
            "        for :math:`x \\ge 0`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``halfcauchy.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``halfcauchy.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import halfcauchy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = halfcauchy.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(halfcauchy.ppf(0.01),\n",
            "        ...                 halfcauchy.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, halfcauchy.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='halfcauchy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = halfcauchy()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = halfcauchy.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], halfcauchy.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = halfcauchy.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    halfgennorm = <scipy.stats._continuous_distns.halfgennorm_gen object>\n",
            "        The upper half of a generalized normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `halfgennorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(beta, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, beta, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, beta, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, beta, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, beta, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, beta, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, beta, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, beta, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, beta, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, beta, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(beta, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(beta, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(beta, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(beta, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(beta, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(beta, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, beta, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        gennorm : generalized normal distribution\n",
            "        expon : exponential distribution\n",
            "        halfnorm : half normal distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `halfgennorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\beta) = \\frac{\\beta}{\\Gamma(1/\\beta)} \\exp(-|x|^\\beta)\n",
            "        \n",
            "        for :math:`x, \\beta > 0`. :math:`\\Gamma` is the gamma function\n",
            "        (`scipy.special.gamma`).\n",
            "        \n",
            "        `halfgennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n",
            "        For :math:`\\beta = 1`, it is identical to an exponential distribution.\n",
            "        For :math:`\\beta = 2`, it is identical to a half normal distribution\n",
            "        (with ``scale=1/sqrt(2)``).\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] \"Generalized normal distribution, Version 1\",\n",
            "               https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import halfgennorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> beta = 0.675\n",
            "        >>> mean, var, skew, kurt = halfgennorm.stats(beta, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(halfgennorm.ppf(0.01, beta),\n",
            "        ...                 halfgennorm.ppf(0.99, beta), 100)\n",
            "        >>> ax.plot(x, halfgennorm.pdf(x, beta),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='halfgennorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = halfgennorm(beta)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = halfgennorm.ppf([0.001, 0.5, 0.999], beta)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], halfgennorm.cdf(vals, beta))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = halfgennorm.rvs(beta, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    halflogistic = <scipy.stats._continuous_distns.halflogistic_gen object...\n",
            "        A half-logistic continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `halflogistic` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `halflogistic` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{ 2 e^{-x} }{ (1+e^{-x})^2 }\n",
            "                 = \\frac{1}{2} \\text{sech}(x/2)^2\n",
            "        \n",
            "        for :math:`x \\ge 0`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``halflogistic.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``halflogistic.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Asgharzadeh et al (2011). \"Comparisons of Methods of Estimation for the\n",
            "               Half-Logistic Distribution\". Selcuk J. Appl. Math. 93-108.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import halflogistic\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = halflogistic.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(halflogistic.ppf(0.01),\n",
            "        ...                 halflogistic.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, halflogistic.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='halflogistic pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = halflogistic()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = halflogistic.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], halflogistic.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = halflogistic.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    halfnorm = <scipy.stats._continuous_distns.halfnorm_gen object>\n",
            "        A half-normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `halfnorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `halfnorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\sqrt{2/\\pi} \\exp(-x^2 / 2)\n",
            "        \n",
            "        for :math:`x >= 0`.\n",
            "        \n",
            "        `halfnorm` is a special case of `chi` with ``df=1``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``halfnorm.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``halfnorm.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import halfnorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = halfnorm.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(halfnorm.ppf(0.01),\n",
            "        ...                 halfnorm.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, halfnorm.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='halfnorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = halfnorm()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = halfnorm.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], halfnorm.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = halfnorm.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    hypergeom = <scipy.stats._discrete_distns.hypergeom_gen object>\n",
            "        A hypergeometric discrete random variable.\n",
            "        \n",
            "        The hypergeometric distribution models drawing objects from a bin.\n",
            "        `M` is the total number of objects, `n` is total number of Type I objects.\n",
            "        The random variate represents the number of Type I objects in `N` drawn\n",
            "        without replacement from the total population.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `hypergeom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(M, n, N, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, M, n, N, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, M, n, N, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, M, n, N, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, M, n, N, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, M, n, N, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, M, n, N, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, M, n, N, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, M, n, N, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(M, n, N, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(M, n, N, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(M, n, N), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(M, n, N, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(M, n, N, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(M, n, N, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(M, n, N, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, M, n, N, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not\n",
            "        universally accepted.  See the Examples for a clarification of the\n",
            "        definitions used here.\n",
            "        \n",
            "        The probability mass function is defined as,\n",
            "        \n",
            "        .. math:: p(k, M, n, N) = \\frac{\\binom{n}{k} \\binom{M - n}{N - k}}\n",
            "                                       {\\binom{M}{N}}\n",
            "        \n",
            "        for :math:`k \\in [\\max(0, N - M + n), \\min(n, N)]`, where the binomial\n",
            "        coefficients are defined as,\n",
            "        \n",
            "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``hypergeom.pmf(k, M, n, N, loc)`` is identically\n",
            "        equivalent to ``hypergeom.pmf(k - loc, M, n, N)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import hypergeom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if\n",
            "        we want to know the probability of finding a given number of dogs if we\n",
            "        choose at random 12 of the 20 animals, we can initialize a frozen\n",
            "        distribution and plot the probability mass function:\n",
            "        \n",
            "        >>> [M, n, N] = [20, 7, 12]\n",
            "        >>> rv = hypergeom(M, n, N)\n",
            "        >>> x = np.arange(0, n+1)\n",
            "        >>> pmf_dogs = rv.pmf(x)\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(x, pmf_dogs, 'bo')\n",
            "        >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n",
            "        >>> ax.set_xlabel('# of dogs in our group of chosen animals')\n",
            "        >>> ax.set_ylabel('hypergeom PMF')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Instead of using a frozen distribution we can also use `hypergeom`\n",
            "        methods directly.  To for example obtain the cumulative distribution\n",
            "        function, use:\n",
            "        \n",
            "        >>> prb = hypergeom.cdf(x, M, n, N)\n",
            "        \n",
            "        And to generate random numbers:\n",
            "        \n",
            "        >>> R = hypergeom.rvs(M, n, N, size=10)\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        nhypergeom, binom, nbinom\n",
            "    \n",
            "    hypsecant = <scipy.stats._continuous_distns.hypsecant_gen object>\n",
            "        A hyperbolic secant continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `hypsecant` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `hypsecant` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\pi} \\text{sech}(x)\n",
            "        \n",
            "        for a real number :math:`x`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``hypsecant.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``hypsecant.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import hypsecant\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = hypsecant.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(hypsecant.ppf(0.01),\n",
            "        ...                 hypsecant.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, hypsecant.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='hypsecant pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = hypsecant()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = hypsecant.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], hypsecant.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = hypsecant.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    invgamma = <scipy.stats._continuous_distns.invgamma_gen object>\n",
            "        An inverted gamma continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `invgamma` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `invgamma` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = \\frac{x^{-a-1}}{\\Gamma(a)} \\exp(-\\frac{1}{x})\n",
            "        \n",
            "        for :math:`x >= 0`, :math:`a > 0`. :math:`\\Gamma` is the gamma function\n",
            "        (`scipy.special.gamma`).\n",
            "        \n",
            "        `invgamma` takes ``a`` as a shape parameter for :math:`a`.\n",
            "        \n",
            "        `invgamma` is a special case of `gengamma` with ``c=-1``, and it is a\n",
            "        different parameterization of the scaled inverse chi-squared distribution.\n",
            "        Specifically, if the scaled inverse chi-squared distribution is\n",
            "        parameterized with degrees of freedom :math:`\\nu` and scaling parameter\n",
            "        :math:`\\tau^2`, then it can be modeled using `invgamma` with\n",
            "        ``a=`` :math:`\\nu/2` and ``scale=`` :math:`\\nu \\tau^2/2`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``invgamma.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``invgamma.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import invgamma\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 4.07\n",
            "        >>> mean, var, skew, kurt = invgamma.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(invgamma.ppf(0.01, a),\n",
            "        ...                 invgamma.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, invgamma.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='invgamma pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = invgamma(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = invgamma.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], invgamma.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = invgamma.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    invgauss = <scipy.stats._continuous_distns.invgauss_gen object>\n",
            "        An inverse Gaussian continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `invgauss` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(mu, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, mu, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, mu, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, mu, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, mu, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, mu, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, mu, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, mu, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, mu, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, mu, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(mu, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(mu, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(mu, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(mu, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(mu, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(mu, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, mu, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `invgauss` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x; \\mu) = \\frac{1}{\\sqrt{2 \\pi x^3}}\n",
            "                        \\exp\\left(-\\frac{(x-\\mu)^2}{2 \\mu^2 x}\\right)\n",
            "        \n",
            "        for :math:`x \\ge 0` and :math:`\\mu > 0`.\n",
            "        \n",
            "        `invgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``invgauss.pdf(x, mu, loc, scale)`` is identically\n",
            "        equivalent to ``invgauss.pdf(y, mu) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        A common shape-scale parameterization of the inverse Gaussian distribution\n",
            "        has density\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x; \\nu, \\lambda) = \\sqrt{\\frac{\\lambda}{2 \\pi x^3}}\n",
            "                        \\exp\\left( -\\frac{\\lambda(x-\\nu)^2}{2 \\nu^2 x}\\right)\n",
            "        \n",
            "        Using ``nu`` for :math:`\\nu` and ``lam`` for :math:`\\lambda`, this\n",
            "        parameterization is equivalent to the one above with ``mu = nu/lam``,\n",
            "        ``loc = 0``, and ``scale = lam``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import invgauss\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mu = 0.145\n",
            "        >>> mean, var, skew, kurt = invgauss.stats(mu, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(invgauss.ppf(0.01, mu),\n",
            "        ...                 invgauss.ppf(0.99, mu), 100)\n",
            "        >>> ax.plot(x, invgauss.pdf(x, mu),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='invgauss pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = invgauss(mu)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = invgauss.ppf([0.001, 0.5, 0.999], mu)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], invgauss.cdf(vals, mu))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = invgauss.rvs(mu, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    invweibull = <scipy.stats._continuous_distns.invweibull_gen object>\n",
            "        An inverted Weibull continuous random variable.\n",
            "        \n",
            "        This distribution is also known as the Frchet distribution or the\n",
            "        type II extreme value distribution.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `invweibull` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `invweibull` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c x^{-c-1} \\exp(-x^{-c})\n",
            "        \n",
            "        for :math:`x > 0`, :math:`c > 0`.\n",
            "        \n",
            "        `invweibull` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``invweibull.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``invweibull.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, \"The generalized inverse\n",
            "        Weibull distribution\", Stat. Papers, vol. 52, pp. 591-619, 2011.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import invweibull\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 10.6\n",
            "        >>> mean, var, skew, kurt = invweibull.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(invweibull.ppf(0.01, c),\n",
            "        ...                 invweibull.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, invweibull.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='invweibull pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = invweibull(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = invweibull.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], invweibull.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = invweibull.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    invwishart = <scipy.stats._multivariate.invwishart_gen object>\n",
            "        An inverse Wishart random variable.\n",
            "        \n",
            "        The `df` keyword specifies the degrees of freedom. The `scale` keyword\n",
            "        specifies the scale matrix, which must be symmetric and positive definite.\n",
            "        In this context, the scale matrix is often interpreted in terms of a\n",
            "        multivariate normal covariance matrix.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(x, df, scale)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, scale)\n",
            "            Log of the probability density function.\n",
            "        rvs(df, scale, size=1, random_state=None)\n",
            "            Draw random samples from an inverse Wishart distribution.\n",
            "        entropy(df, scale)\n",
            "            Differential entropy of the distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        df : int\n",
            "            Degrees of freedom, must be greater than or equal to dimension of the\n",
            "            scale matrix\n",
            "        scale : array_like\n",
            "            Symmetric positive definite scale matrix of the distribution\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Raises\n",
            "        ------\n",
            "        scipy.linalg.LinAlgError\n",
            "            If the scale matrix `scale` is not positive definite.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        wishart\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        \n",
            "        The scale matrix `scale` must be a symmetric positive definite\n",
            "        matrix. Singular matrices, including the symmetric positive semi-definite\n",
            "        case, are not supported. Symmetry is not checked; only the lower triangular\n",
            "        portion is used.\n",
            "        \n",
            "        The inverse Wishart distribution is often denoted\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            W_p^{-1}(\\nu, \\Psi)\n",
            "        \n",
            "        where :math:`\\nu` is the degrees of freedom and :math:`\\Psi` is the\n",
            "        :math:`p \\times p` scale matrix.\n",
            "        \n",
            "        The probability density function for `invwishart` has support over positive\n",
            "        definite matrices :math:`S`; if :math:`S \\sim W^{-1}_p(\\nu, \\Sigma)`,\n",
            "        then its PDF is given by:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(S) = \\frac{|\\Sigma|^\\frac{\\nu}{2}}{2^{ \\frac{\\nu p}{2} }\n",
            "                   |S|^{\\frac{\\nu + p + 1}{2}} \\Gamma_p \\left(\\frac{\\nu}{2} \\right)}\n",
            "                   \\exp\\left( -tr(\\Sigma S^{-1}) / 2 \\right)\n",
            "        \n",
            "        If :math:`S \\sim W_p^{-1}(\\nu, \\Psi)` (inverse Wishart) then\n",
            "        :math:`S^{-1} \\sim W_p(\\nu, \\Psi^{-1})` (Wishart).\n",
            "        \n",
            "        If the scale matrix is 1-dimensional and equal to one, then the inverse\n",
            "        Wishart distribution :math:`W_1(\\nu, 1)` collapses to the\n",
            "        inverse Gamma distribution with parameters shape = :math:`\\frac{\\nu}{2}`\n",
            "        and scale = :math:`\\frac{1}{2}`.\n",
            "        \n",
            "        Instead of inverting a randomly generated Wishart matrix as described in [2],\n",
            "        here the algorithm in [4] is used to directly generate a random inverse-Wishart\n",
            "        matrix without inversion.\n",
            "        \n",
            "        .. versionadded:: 0.16.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n",
            "               Wiley, 1983.\n",
            "        .. [2] M.C. Jones, \"Generating Inverse Wishart Matrices\", Communications\n",
            "               in Statistics - Simulation and Computation, vol. 14.2, pp.511-514,\n",
            "               1985.\n",
            "        .. [3] Gupta, M. and Srivastava, S. \"Parametric Bayesian Estimation of\n",
            "               Differential Entropy and Relative Entropy\". Entropy 12, 818 - 843.\n",
            "               2010.\n",
            "        .. [4] S.D. Axen, \"Efficiently generating inverse-Wishart matrices and\n",
            "               their Cholesky factors\", :arXiv:`2310.15884v1`. 2023.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import invwishart, invgamma\n",
            "        >>> x = np.linspace(0.01, 1, 100)\n",
            "        >>> iw = invwishart.pdf(x, df=6, scale=1)\n",
            "        >>> iw[:3]\n",
            "        array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n",
            "        >>> ig = invgamma.pdf(x, 6/2., scale=1./2)\n",
            "        >>> ig[:3]\n",
            "        array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n",
            "        >>> plt.plot(x, iw)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The input quantiles can be any shape of array, as long as the last\n",
            "        axis labels the components.\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the degrees\n",
            "        of freedom and scale parameters, returning a \"frozen\" inverse Wishart\n",
            "        random variable:\n",
            "        \n",
            "        >>> rv = invwishart(df=1, scale=1)\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # degrees of freedom and scale fixed.\n",
            "    \n",
            "    jf_skew_t = <scipy.stats._continuous_distns.jf_skew_t_gen object>\n",
            "        Jones and Faddy skew-t distribution.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `jf_skew_t` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `jf_skew_t` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x; a, b) = C_{a,b}^{-1}\n",
            "                        \\left(1+\\frac{x}{\\left(a+b+x^2\\right)^{1/2}}\\right)^{a+1/2}\n",
            "                        \\left(1-\\frac{x}{\\left(a+b+x^2\\right)^{1/2}}\\right)^{b+1/2}\n",
            "        \n",
            "        for real numbers :math:`a>0` and :math:`b>0`, where\n",
            "        :math:`C_{a,b} = 2^{a+b-1}B(a,b)(a+b)^{1/2}`, and :math:`B` denotes the\n",
            "        beta function (`scipy.special.beta`).\n",
            "        \n",
            "        When :math:`a<b`, the distribution is negatively skewed, and when\n",
            "        :math:`a>b`, the distribution is positively skewed. If :math:`a=b`, then\n",
            "        we recover the `t` distribution with :math:`2a` degrees of freedom.\n",
            "        \n",
            "        `jf_skew_t` takes :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``jf_skew_t.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``jf_skew_t.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] M.C. Jones and M.J. Faddy. \"A skew extension of the t distribution,\n",
            "               with applications\" *Journal of the Royal Statistical Society*.\n",
            "               Series B (Statistical Methodology) 65, no. 1 (2003): 159-174.\n",
            "               :doi:`10.1111/1467-9868.00378`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import jf_skew_t\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 8, 4\n",
            "        >>> mean, var, skew, kurt = jf_skew_t.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(jf_skew_t.ppf(0.01, a, b),\n",
            "        ...                 jf_skew_t.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, jf_skew_t.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='jf_skew_t pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = jf_skew_t(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = jf_skew_t.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], jf_skew_t.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = jf_skew_t.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    johnsonsb = <scipy.stats._continuous_distns.johnsonsb_gen object>\n",
            "        A Johnson SB continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `johnsonsb` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        johnsonsu\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `johnsonsb` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{b}{x(1-x)}  \\phi(a + b \\log \\frac{x}{1-x} )\n",
            "        \n",
            "        where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`\n",
            "        and :math:`x \\in [0,1]`.  :math:`\\phi` is the pdf of the normal\n",
            "        distribution.\n",
            "        \n",
            "        `johnsonsb` takes :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``johnsonsb.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``johnsonsb.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import johnsonsb\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 4.32, 3.18\n",
            "        >>> mean, var, skew, kurt = johnsonsb.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(johnsonsb.ppf(0.01, a, b),\n",
            "        ...                 johnsonsb.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, johnsonsb.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='johnsonsb pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = johnsonsb(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = johnsonsb.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], johnsonsb.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = johnsonsb.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    johnsonsu = <scipy.stats._continuous_distns.johnsonsu_gen object>\n",
            "        A Johnson SU continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `johnsonsu` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        johnsonsb\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `johnsonsu` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{b}{\\sqrt{x^2 + 1}}\n",
            "                         \\phi(a + b \\log(x + \\sqrt{x^2 + 1}))\n",
            "        \n",
            "        where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`.\n",
            "        :math:`\\phi` is the pdf of the normal distribution.\n",
            "        \n",
            "        `johnsonsu` takes :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The first four central moments are calculated according to the formulas\n",
            "        in [1]_.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``johnsonsu.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``johnsonsu.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Taylor Enterprises. \"Johnson Family of Distributions\".\n",
            "           https://variation.com/wp-content/distribution_analyzer_help/hs126.htm\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import johnsonsu\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 2.55, 2.25\n",
            "        >>> mean, var, skew, kurt = johnsonsu.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(johnsonsu.ppf(0.01, a, b),\n",
            "        ...                 johnsonsu.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, johnsonsu.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='johnsonsu pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = johnsonsu(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = johnsonsu.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], johnsonsu.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = johnsonsu.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    kappa3 = <scipy.stats._continuous_distns.kappa3_gen object>\n",
            "        Kappa 3 parameter distribution.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `kappa3` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `kappa3` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = a (a + x^a)^{-(a + 1)/a}\n",
            "        \n",
            "        for :math:`x > 0` and :math:`a > 0`.\n",
            "        \n",
            "        `kappa3` takes ``a`` as a shape parameter for :math:`a`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        P.W. Mielke and E.S. Johnson, \"Three-Parameter Kappa Distribution Maximum\n",
            "        Likelihood and Likelihood Ratio Tests\", Methods in Weather Research,\n",
            "        701-707, (September, 1973),\n",
            "        :doi:`10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2`\n",
            "        \n",
            "        B. Kumphon, \"Maximum Entropy and Maximum Likelihood Estimation for the\n",
            "        Three-Parameter Kappa Distribution\", Open Journal of Statistics, vol 2,\n",
            "        415-419 (2012), :doi:`10.4236/ojs.2012.24050`\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``kappa3.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``kappa3.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import kappa3\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 1\n",
            "        >>> mean, var, skew, kurt = kappa3.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(kappa3.ppf(0.01, a),\n",
            "        ...                 kappa3.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, kappa3.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='kappa3 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = kappa3(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = kappa3.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], kappa3.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = kappa3.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    kappa4 = <scipy.stats._continuous_distns.kappa4_gen object>\n",
            "        Kappa 4 parameter distribution.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `kappa4` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(h, k, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, h, k, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, h, k, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, h, k, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, h, k, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, h, k, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, h, k, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, h, k, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, h, k, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, h, k, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(h, k, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(h, k, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(h, k), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(h, k, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(h, k, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(h, k, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(h, k, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, h, k, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for kappa4 is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}\n",
            "        \n",
            "        if :math:`h` and :math:`k` are not equal to 0.\n",
            "        \n",
            "        If :math:`h` or :math:`k` are zero then the pdf can be simplified:\n",
            "        \n",
            "        h = 0 and k != 0::\n",
            "        \n",
            "            kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*\n",
            "                                  exp(-(1.0 - k*x)**(1.0/k))\n",
            "        \n",
            "        h != 0 and k = 0::\n",
            "        \n",
            "            kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)\n",
            "        \n",
            "        h = 0 and k = 0::\n",
            "        \n",
            "            kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))\n",
            "        \n",
            "        kappa4 takes :math:`h` and :math:`k` as shape parameters.\n",
            "        \n",
            "        The kappa4 distribution returns other distributions when certain\n",
            "        :math:`h` and :math:`k` values are used.\n",
            "        \n",
            "        +------+-------------+----------------+------------------+\n",
            "        | h    | k=0.0       | k=1.0          | -inf<=k<=inf     |\n",
            "        +======+=============+================+==================+\n",
            "        | -1.0 | Logistic    |                | Generalized      |\n",
            "        |      |             |                | Logistic(1)      |\n",
            "        |      |             |                |                  |\n",
            "        |      | logistic(x) |                |                  |\n",
            "        +------+-------------+----------------+------------------+\n",
            "        |  0.0 | Gumbel      | Reverse        | Generalized      |\n",
            "        |      |             | Exponential(2) | Extreme Value    |\n",
            "        |      |             |                |                  |\n",
            "        |      | gumbel_r(x) |                | genextreme(x, k) |\n",
            "        +------+-------------+----------------+------------------+\n",
            "        |  1.0 | Exponential | Uniform        | Generalized      |\n",
            "        |      |             |                | Pareto           |\n",
            "        |      |             |                |                  |\n",
            "        |      | expon(x)    | uniform(x)     | genpareto(x, -k) |\n",
            "        +------+-------------+----------------+------------------+\n",
            "        \n",
            "        (1) There are at least five generalized logistic distributions.\n",
            "            Four are described here:\n",
            "            https://en.wikipedia.org/wiki/Generalized_logistic_distribution\n",
            "            The \"fifth\" one is the one kappa4 should match which currently\n",
            "            isn't implemented in scipy:\n",
            "            https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution\n",
            "            https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html\n",
            "        (2) This distribution is currently not in scipy.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        J.C. Finney, \"Optimization of a Skewed Logistic Distribution With Respect\n",
            "        to the Kolmogorov-Smirnov Test\", A Dissertation Submitted to the Graduate\n",
            "        Faculty of the Louisiana State University and Agricultural and Mechanical\n",
            "        College, (August, 2004),\n",
            "        https://digitalcommons.lsu.edu/gradschool_dissertations/3672\n",
            "        \n",
            "        J.R.M. Hosking, \"The four-parameter kappa distribution\". IBM J. Res.\n",
            "        Develop. 38 (3), 25 1-258 (1994).\n",
            "        \n",
            "        B. Kumphon, A. Kaew-Man, P. Seenoi, \"A Rainfall Distribution for the Lampao\n",
            "        Site in the Chi River Basin, Thailand\", Journal of Water Resource and\n",
            "        Protection, vol. 4, 866-869, (2012).\n",
            "        :doi:`10.4236/jwarp.2012.410101`\n",
            "        \n",
            "        C. Winchester, \"On Estimation of the Four-Parameter Kappa Distribution\", A\n",
            "        Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March\n",
            "        2000).\n",
            "        http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``kappa4.pdf(x, h, k, loc, scale)`` is identically\n",
            "        equivalent to ``kappa4.pdf(y, h, k) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import kappa4\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> h, k = 0.1, 0\n",
            "        >>> mean, var, skew, kurt = kappa4.stats(h, k, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(kappa4.ppf(0.01, h, k),\n",
            "        ...                 kappa4.ppf(0.99, h, k), 100)\n",
            "        >>> ax.plot(x, kappa4.pdf(x, h, k),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='kappa4 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = kappa4(h, k)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = kappa4.ppf([0.001, 0.5, 0.999], h, k)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], kappa4.cdf(vals, h, k))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = kappa4.rvs(h, k, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    ksone = <scipy.stats._continuous_distns.ksone_gen object>\n",
            "        Kolmogorov-Smirnov one-sided test statistic distribution.\n",
            "        \n",
            "        This is the distribution of the one-sided Kolmogorov-Smirnov (KS)\n",
            "        statistics :math:`D_n^+` and :math:`D_n^-`\n",
            "        for a finite sample size ``n >= 1`` (the shape parameter).\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `ksone` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, n, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, n, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, n, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, n, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, n, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, n, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, n, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(n, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(n, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(n, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(n, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kstwobign, kstwo, kstest\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        :math:`D_n^+` and :math:`D_n^-` are given by\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D_n^+ &= \\text{sup}_x (F_n(x) - F(x)),\\\\\n",
            "            D_n^- &= \\text{sup}_x (F(x) - F_n(x)),\\\\\n",
            "        \n",
            "        where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n",
            "        `ksone` describes the distribution under the null hypothesis of the KS test\n",
            "        that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n",
            "        with CDF :math:`F`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``ksone.pdf(x, n, loc, scale)`` is identically\n",
            "        equivalent to ``ksone.pdf(y, n) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Birnbaum, Z. W. and Tingey, F.H. \"One-sided confidence contours\n",
            "           for probability distribution functions\", The Annals of Mathematical\n",
            "           Statistics, 22(4), pp 592-596 (1951).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ksone\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n = 1e+03\n",
            "        >>> mean, var, skew, kurt = ksone.stats(n, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(ksone.ppf(0.01, n),\n",
            "        ...                 ksone.ppf(0.99, n), 100)\n",
            "        >>> ax.plot(x, ksone.pdf(x, n),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='ksone pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = ksone(n)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = ksone.ppf([0.001, 0.5, 0.999], n)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], ksone.cdf(vals, n))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = ksone.rvs(n, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    kstwo = <scipy.stats._continuous_distns.kstwo_gen object>\n",
            "        Kolmogorov-Smirnov two-sided test statistic distribution.\n",
            "        \n",
            "        This is the distribution of the two-sided Kolmogorov-Smirnov (KS)\n",
            "        statistic :math:`D_n` for a finite sample size ``n >= 1``\n",
            "        (the shape parameter).\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `kstwo` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, n, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, n, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, n, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, n, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, n, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, n, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, n, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(n, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(n, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(n, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(n, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        kstwobign, ksone, kstest\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        :math:`D_n` is given by\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D_n = \\text{sup}_x |F_n(x) - F(x)|\n",
            "        \n",
            "        where :math:`F` is a (continuous) CDF and :math:`F_n` is an empirical CDF.\n",
            "        `kstwo` describes the distribution under the null hypothesis of the KS test\n",
            "        that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n",
            "        with CDF :math:`F`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``kstwo.pdf(x, n, loc, scale)`` is identically\n",
            "        equivalent to ``kstwo.pdf(y, n) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Simard, R., L'Ecuyer, P. \"Computing the Two-Sided\n",
            "           Kolmogorov-Smirnov Distribution\",  Journal of Statistical Software,\n",
            "           Vol 39, 11, 1-18 (2011).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import kstwo\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n = 10\n",
            "        >>> mean, var, skew, kurt = kstwo.stats(n, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(kstwo.ppf(0.01, n),\n",
            "        ...                 kstwo.ppf(0.99, n), 100)\n",
            "        >>> ax.plot(x, kstwo.pdf(x, n),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='kstwo pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = kstwo(n)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = kstwo.ppf([0.001, 0.5, 0.999], n)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], kstwo.cdf(vals, n))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = kstwo.rvs(n, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    kstwobign = <scipy.stats._continuous_distns.kstwobign_gen object>\n",
            "        Limiting distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n",
            "        \n",
            "        This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov\n",
            "        statistic :math:`\\sqrt{n} D_n` that measures the maximum absolute\n",
            "        distance of the theoretical (continuous) CDF from the empirical CDF.\n",
            "        (see `kstest`).\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `kstwobign` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        ksone, kstwo, kstest\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        :math:`\\sqrt{n} D_n` is given by\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D_n = \\text{sup}_x |F_n(x) - F(x)|\n",
            "        \n",
            "        where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n",
            "        `kstwobign`  describes the asymptotic distribution (i.e. the limit of\n",
            "        :math:`\\sqrt{n} D_n`) under the null hypothesis of the KS test that the\n",
            "        empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``kstwobign.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``kstwobign.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Feller, W. \"On the Kolmogorov-Smirnov Limit Theorems for Empirical\n",
            "           Distributions\",  Ann. Math. Statist. Vol 19, 177-189 (1948).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import kstwobign\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = kstwobign.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(kstwobign.ppf(0.01),\n",
            "        ...                 kstwobign.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, kstwobign.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='kstwobign pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = kstwobign()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = kstwobign.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], kstwobign.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = kstwobign.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    laplace = <scipy.stats._continuous_distns.laplace_gen object>\n",
            "        A Laplace continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `laplace` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `laplace` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{2} \\exp(-|x|)\n",
            "        \n",
            "        for a real number :math:`x`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``laplace.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``laplace.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import laplace\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = laplace.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(laplace.ppf(0.01),\n",
            "        ...                 laplace.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, laplace.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='laplace pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = laplace()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = laplace.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], laplace.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = laplace.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    laplace_asymmetric = <scipy.stats._continuous_distns.laplace_asymmetri...\n",
            "        An asymmetric Laplace continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `laplace_asymmetric` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, kappa, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, kappa, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, kappa, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, kappa, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, kappa, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, kappa, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, kappa, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(kappa, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(kappa, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(kappa, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(kappa, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(kappa, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(kappa, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, kappa, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        laplace : Laplace distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `laplace_asymmetric` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(x, \\kappa) &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(-x\\kappa),\\quad x\\ge0\\\\\n",
            "                        &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(x/\\kappa),\\quad x<0\\\\\n",
            "        \n",
            "        for :math:`-\\infty < x < \\infty`, :math:`\\kappa > 0`.\n",
            "        \n",
            "        `laplace_asymmetric` takes ``kappa`` as a shape parameter for\n",
            "        :math:`\\kappa`. For :math:`\\kappa = 1`, it is identical to a\n",
            "        Laplace distribution.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``laplace_asymmetric.pdf(x, kappa, loc, scale)`` is identically\n",
            "        equivalent to ``laplace_asymmetric.pdf(y, kappa) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Note that the scale parameter of some references is the reciprocal of\n",
            "        SciPy's ``scale``. For example, :math:`\\lambda = 1/2` in the\n",
            "        parameterization of [1]_ is equivalent to ``scale = 2`` with\n",
            "        `laplace_asymmetric`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Asymmetric Laplace distribution\", Wikipedia\n",
            "                https://en.wikipedia.org/wiki/Asymmetric_Laplace_distribution\n",
            "        \n",
            "        .. [2] Kozubowski TJ and Podgrski K. A Multivariate and\n",
            "               Asymmetric Generalization of Laplace Distribution,\n",
            "               Computational Statistics 15, 531--540 (2000).\n",
            "               :doi:`10.1007/PL00022717`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import laplace_asymmetric\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> kappa = 2\n",
            "        >>> mean, var, skew, kurt = laplace_asymmetric.stats(kappa, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(laplace_asymmetric.ppf(0.01, kappa),\n",
            "        ...                 laplace_asymmetric.ppf(0.99, kappa), 100)\n",
            "        >>> ax.plot(x, laplace_asymmetric.pdf(x, kappa),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='laplace_asymmetric pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = laplace_asymmetric(kappa)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = laplace_asymmetric.ppf([0.001, 0.5, 0.999], kappa)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], laplace_asymmetric.cdf(vals, kappa))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = laplace_asymmetric.rvs(kappa, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    levy = <scipy.stats._continuous_distns.levy_gen object>\n",
            "        A Levy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `levy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        levy_stable, levy_l\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `levy` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp\\left(-\\frac{1}{2x}\\right)\n",
            "        \n",
            "        for :math:`x > 0`.\n",
            "        \n",
            "        This is the same as the Levy-stable distribution with :math:`a=1/2` and\n",
            "        :math:`b=1`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``levy.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``levy.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import levy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mean, var, skew, kurt = levy.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> # `levy` is very heavy-tailed.\n",
            "        >>> # To show a nice plot, let's cut off the upper 40 percent.\n",
            "        >>> a, b = levy.ppf(0), levy.ppf(0.6)\n",
            "        >>> x = np.linspace(a, b, 100)\n",
            "        >>> ax.plot(x, levy.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='levy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = levy()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = levy.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], levy.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = levy.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> # manual binning to ignore the tail\n",
            "        >>> bins = np.concatenate((np.linspace(a, b, 20), [np.max(r)]))\n",
            "        >>> ax.hist(r, bins=bins, density=True, histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    levy_l = <scipy.stats._continuous_distns.levy_l_gen object>\n",
            "        A left-skewed Levy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `levy_l` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        levy, levy_stable\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `levy_l` is:\n",
            "        \n",
            "        .. math::\n",
            "            f(x) = \\frac{1}{|x| \\sqrt{2\\pi |x|}} \\exp{ \\left(-\\frac{1}{2|x|} \\right)}\n",
            "        \n",
            "        for :math:`x < 0`.\n",
            "        \n",
            "        This is the same as the Levy-stable distribution with :math:`a=1/2` and\n",
            "        :math:`b=-1`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``levy_l.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``levy_l.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import levy_l\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mean, var, skew, kurt = levy_l.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> # `levy_l` is very heavy-tailed.\n",
            "        >>> # To show a nice plot, let's cut off the lower 40 percent.\n",
            "        >>> a, b = levy_l.ppf(0.4), levy_l.ppf(1)\n",
            "        >>> x = np.linspace(a, b, 100)\n",
            "        >>> ax.plot(x, levy_l.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='levy_l pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = levy_l()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = levy_l.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], levy_l.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = levy_l.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> # manual binning to ignore the tail\n",
            "        >>> bins = np.concatenate(([np.min(r)], np.linspace(a, b, 20)))\n",
            "        >>> ax.hist(r, bins=bins, density=True, histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    levy_stable = <scipy.stats._levy_stable.levy_stable_gen object>\n",
            "        A Levy-stable continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `levy_stable` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(alpha, beta, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, alpha, beta, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, alpha, beta, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, alpha, beta, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, alpha, beta, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, alpha, beta, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, alpha, beta, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, alpha, beta, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, alpha, beta, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, alpha, beta, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(alpha, beta, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(alpha, beta, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(alpha, beta), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(alpha, beta, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(alpha, beta, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(alpha, beta, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(alpha, beta, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, alpha, beta, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        levy, levy_l, cauchy, norm\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The distribution for `levy_stable` has characteristic function:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\varphi(t, \\alpha, \\beta, c, \\mu) =\n",
            "            e^{it\\mu -|ct|^{\\alpha}(1-i\\beta\\operatorname{sign}(t)\\Phi(\\alpha, t))}\n",
            "        \n",
            "        where two different parameterizations are supported. The first :math:`S_1`:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\Phi = \\begin{cases}\n",
            "                    \\tan \\left({\\frac {\\pi \\alpha }{2}}\\right)&\\alpha \\neq 1\\\\\n",
            "                    -{\\frac {2}{\\pi }}\\log |t|&\\alpha =1\n",
            "                    \\end{cases}\n",
            "        \n",
            "        The second :math:`S_0`:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            \\Phi = \\begin{cases}\n",
            "                    -\\tan \\left({\\frac {\\pi \\alpha }{2}}\\right)(|ct|^{1-\\alpha}-1)\n",
            "                    &\\alpha \\neq 1\\\\\n",
            "                    -{\\frac {2}{\\pi }}\\log |ct|&\\alpha =1\n",
            "                    \\end{cases}\n",
            "        \n",
            "        \n",
            "        The probability density function for `levy_stable` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi(t)e^{-ixt}\\,dt\n",
            "        \n",
            "        where :math:`-\\infty < t < \\infty`. This integral does not have a known\n",
            "        closed form.\n",
            "        \n",
            "        `levy_stable` generalizes several distributions.  Where possible, they\n",
            "        should be used instead.  Specifically, when the shape parameters\n",
            "        assume the values in the table below, the corresponding equivalent\n",
            "        distribution should be used.\n",
            "        \n",
            "        =========  ========  ===========\n",
            "        ``alpha``  ``beta``   Equivalent\n",
            "        =========  ========  ===========\n",
            "         1/2       -1        `levy_l`\n",
            "         1/2       1         `levy`\n",
            "         1         0         `cauchy`\n",
            "         2         any       `norm` (with ``scale=sqrt(2)``)\n",
            "        =========  ========  ===========\n",
            "        \n",
            "        Evaluation of the pdf uses Nolan's piecewise integration approach with the\n",
            "        Zolotarev :math:`M` parameterization by default. There is also the option\n",
            "        to use direct numerical integration of the standard parameterization of the\n",
            "        characteristic function or to evaluate by taking the FFT of the\n",
            "        characteristic function.\n",
            "        \n",
            "        The default method can changed by setting the class variable\n",
            "        ``levy_stable.pdf_default_method`` to one of 'piecewise' for Nolan's\n",
            "        approach, 'dni' for direct numerical integration, or 'fft-simpson' for the\n",
            "        FFT based approach. For the sake of backwards compatibility, the methods\n",
            "        'best' and 'zolotarev' are equivalent to 'piecewise' and the method\n",
            "        'quadrature' is equivalent to 'dni'.\n",
            "        \n",
            "        The parameterization can be changed  by setting the class variable\n",
            "        ``levy_stable.parameterization`` to either 'S0' or 'S1'.\n",
            "        The default is 'S1'.\n",
            "        \n",
            "        To improve performance of piecewise and direct numerical integration one\n",
            "        can specify ``levy_stable.quad_eps`` (defaults to 1.2e-14). This is used\n",
            "        as both the absolute and relative quadrature tolerance for direct numerical\n",
            "        integration and as the relative quadrature tolerance for the piecewise\n",
            "        method. One can also specify ``levy_stable.piecewise_x_tol_near_zeta``\n",
            "        (defaults to 0.005) for how close x is to zeta before it is considered the\n",
            "        same as x [NO]. The exact check is\n",
            "        ``abs(x0 - zeta) < piecewise_x_tol_near_zeta*alpha**(1/alpha)``. One can\n",
            "        also specify ``levy_stable.piecewise_alpha_tol_near_one`` (defaults to\n",
            "        0.005) for how close alpha is to 1 before being considered equal to 1.\n",
            "        \n",
            "        To increase accuracy of FFT calculation one can specify\n",
            "        ``levy_stable.pdf_fft_grid_spacing`` (defaults to 0.001) and\n",
            "        ``pdf_fft_n_points_two_power`` (defaults to None which means a value is\n",
            "        calculated that sufficiently covers the input range).\n",
            "        \n",
            "        Further control over FFT calculation is available by setting\n",
            "        ``pdf_fft_interpolation_degree`` (defaults to 3) for spline order and\n",
            "        ``pdf_fft_interpolation_level`` for determining the number of points to use\n",
            "        in the Newton-Cotes formula when approximating the characteristic function\n",
            "        (considered experimental).\n",
            "        \n",
            "        Evaluation of the cdf uses Nolan's piecewise integration approach with the\n",
            "        Zolatarev :math:`S_0` parameterization by default. There is also the option\n",
            "        to evaluate through integration of an interpolated spline of the pdf\n",
            "        calculated by means of the FFT method. The settings affecting FFT\n",
            "        calculation are the same as for pdf calculation. The default cdf method can\n",
            "        be changed by setting ``levy_stable.cdf_default_method`` to either\n",
            "        'piecewise' or 'fft-simpson'.  For cdf calculations the Zolatarev method is\n",
            "        superior in accuracy, so FFT is disabled by default.\n",
            "        \n",
            "        Fitting estimate uses quantile estimation method in [MC]. MLE estimation of\n",
            "        parameters in fit method uses this quantile estimate initially. Note that\n",
            "        MLE doesn't always converge if using FFT for pdf calculations; this will be\n",
            "        the case if alpha <= 1 where the FFT approach doesn't give good\n",
            "        approximations.\n",
            "        \n",
            "        Any non-missing value for the attribute\n",
            "        ``levy_stable.pdf_fft_min_points_threshold`` will set\n",
            "        ``levy_stable.pdf_default_method`` to 'fft-simpson' if a valid\n",
            "        default method is not otherwise set.\n",
            "        \n",
            "        \n",
            "        \n",
            "        .. warning::\n",
            "        \n",
            "            For pdf calculations FFT calculation is considered experimental.\n",
            "        \n",
            "            For cdf calculations FFT calculation is considered experimental. Use\n",
            "            Zolatarev's method instead (default).\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To\n",
            "        shift and/or scale the distribution use the ``loc`` and ``scale``\n",
            "        parameters.\n",
            "        Generally ``levy_stable.pdf(x, alpha, beta, loc, scale)`` is identically\n",
            "        equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with\n",
            "        ``y = (x - loc) / scale``, except in the ``S1`` parameterization if\n",
            "        ``alpha == 1``.  In that case ``levy_stable.pdf(x, alpha, beta, loc, scale)``\n",
            "        is identically equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with\n",
            "        ``y = (x - loc - 2 * beta * scale * np.log(scale) / np.pi) / scale``.\n",
            "        See [NO2]_ Definition 1.8 for more information.\n",
            "        Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [MC] McCulloch, J., 1986. Simple consistent estimators of stable\n",
            "            distribution parameters. Communications in Statistics - Simulation and\n",
            "            Computation 15, 11091136.\n",
            "        .. [WZ] Wang, Li and Zhang, Ji-Hong, 2008. Simpson's rule based FFT method\n",
            "            to compute densities of stable distribution.\n",
            "        .. [NO] Nolan, J., 1997. Numerical Calculation of Stable Densities and\n",
            "            distributions Functions.\n",
            "        .. [NO2] Nolan, J., 2018. Stable Distributions: Models for Heavy Tailed\n",
            "            Data.\n",
            "        .. [HO] Hopcraft, K. I., Jakeman, E., Tanner, R. M. J., 1999. Lvy random\n",
            "            walks with fluctuating step number and multiscale behavior.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import levy_stable\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> alpha, beta = 1.8, -0.5\n",
            "        >>> mean, var, skew, kurt = levy_stable.stats(alpha, beta, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(levy_stable.ppf(0.01, alpha, beta),\n",
            "        ...                 levy_stable.ppf(0.99, alpha, beta), 100)\n",
            "        >>> ax.plot(x, levy_stable.pdf(x, alpha, beta),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='levy_stable pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = levy_stable(alpha, beta)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = levy_stable.ppf([0.001, 0.5, 0.999], alpha, beta)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], levy_stable.cdf(vals, alpha, beta))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = levy_stable.rvs(alpha, beta, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    loggamma = <scipy.stats._continuous_distns.loggamma_gen object>\n",
            "        A log gamma continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `loggamma` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `loggamma` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{\\exp(c x - \\exp(x))}\n",
            "                           {\\Gamma(c)}\n",
            "        \n",
            "        for all :math:`x, c > 0`. Here, :math:`\\Gamma` is the\n",
            "        gamma function (`scipy.special.gamma`).\n",
            "        \n",
            "        `loggamma` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``loggamma.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``loggamma.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import loggamma\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.414\n",
            "        >>> mean, var, skew, kurt = loggamma.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(loggamma.ppf(0.01, c),\n",
            "        ...                 loggamma.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, loggamma.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='loggamma pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = loggamma(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = loggamma.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], loggamma.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = loggamma.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    logistic = <scipy.stats._continuous_distns.logistic_gen object>\n",
            "        A logistic (or Sech-squared) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `logistic` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `logistic` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{\\exp(-x)}\n",
            "                        {(1+\\exp(-x))^2}\n",
            "        \n",
            "        `logistic` is a special case of `genlogistic` with ``c=1``.\n",
            "        \n",
            "        Remark that the survival function (``logistic.sf``) is equal to the\n",
            "        Fermi-Dirac distribution describing fermionic statistics.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``logistic.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``logistic.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import logistic\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = logistic.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(logistic.ppf(0.01),\n",
            "        ...                 logistic.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, logistic.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='logistic pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = logistic()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = logistic.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], logistic.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = logistic.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    loglaplace = <scipy.stats._continuous_distns.loglaplace_gen object>\n",
            "        A log-Laplace continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `loglaplace` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `loglaplace` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\begin{cases}\\frac{c}{2} x^{ c-1}  &\\text{for } 0 < x < 1\\\\\n",
            "                                   \\frac{c}{2} x^{-c-1}  &\\text{for } x \\ge 1\n",
            "                      \\end{cases}\n",
            "        \n",
            "        for :math:`c > 0`.\n",
            "        \n",
            "        `loglaplace` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``loglaplace.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``loglaplace.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Suppose a random variable ``X`` follows the Laplace distribution with\n",
            "        location ``a`` and scale ``b``.  Then ``Y = exp(X)`` follows the\n",
            "        log-Laplace distribution with ``c = 1 / b`` and ``scale = exp(a)``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        T.J. Kozubowski and K. Podgorski, \"A log-Laplace growth rate model\",\n",
            "        The Mathematical Scientist, vol. 28, pp. 49-60, 2003.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import loglaplace\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 3.25\n",
            "        >>> mean, var, skew, kurt = loglaplace.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(loglaplace.ppf(0.01, c),\n",
            "        ...                 loglaplace.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, loglaplace.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='loglaplace pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = loglaplace(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = loglaplace.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], loglaplace.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = loglaplace.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    lognorm = <scipy.stats._continuous_distns.lognorm_gen object>\n",
            "        A lognormal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `lognorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(s, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, s, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, s, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, s, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, s, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, s, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, s, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, s, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, s, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, s, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(s, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(s, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(s,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(s, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(s, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(s, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(s, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, s, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `lognorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, s) = \\frac{1}{s x \\sqrt{2\\pi}}\n",
            "                      \\exp\\left(-\\frac{\\log^2(x)}{2s^2}\\right)\n",
            "        \n",
            "        for :math:`x > 0`, :math:`s > 0`.\n",
            "        \n",
            "        `lognorm` takes ``s`` as a shape parameter for :math:`s`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``lognorm.pdf(x, s, loc, scale)`` is identically\n",
            "        equivalent to ``lognorm.pdf(y, s) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Suppose a normally distributed random variable ``X`` has  mean ``mu`` and\n",
            "        standard deviation ``sigma``. Then ``Y = exp(X)`` is lognormally\n",
            "        distributed with ``s = sigma`` and ``scale = exp(mu)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import lognorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> s = 0.954\n",
            "        >>> mean, var, skew, kurt = lognorm.stats(s, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(lognorm.ppf(0.01, s),\n",
            "        ...                 lognorm.ppf(0.99, s), 100)\n",
            "        >>> ax.plot(x, lognorm.pdf(x, s),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='lognorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = lognorm(s)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = lognorm.ppf([0.001, 0.5, 0.999], s)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], lognorm.cdf(vals, s))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = lognorm.rvs(s, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        The logarithm of a log-normally distributed random variable is\n",
            "        normally distributed:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy import stats\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> mu, sigma = 2, 0.5\n",
            "        >>> X = stats.norm(loc=mu, scale=sigma)\n",
            "        >>> Y = stats.lognorm(s=sigma, scale=np.exp(mu))\n",
            "        >>> x = np.linspace(*X.interval(0.999))\n",
            "        >>> y = Y.rvs(size=10000)\n",
            "        >>> ax.plot(x, X.pdf(x), label='X (pdf)')\n",
            "        >>> ax.hist(np.log(y), density=True, bins=x, label='log(Y) (histogram)')\n",
            "        >>> ax.legend()\n",
            "        >>> plt.show()\n",
            "    \n",
            "    logser = <scipy.stats._discrete_distns.logser_gen object>\n",
            "        A Logarithmic (Log-Series, Series) discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `logser` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(p, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, p, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, p, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, p, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, p, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, p, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, p, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, p, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, p, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(p, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(p, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(p, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(p, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(p, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(p, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, p, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `logser` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = - \\frac{p^k}{k \\log(1-p)}\n",
            "        \n",
            "        for :math:`k \\ge 1`, :math:`0 < p < 1`\n",
            "        \n",
            "        `logser` takes :math:`p` as shape parameter,\n",
            "        where :math:`p` is the probability of a single success\n",
            "        and :math:`1-p` is the probability of a single failure.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``logser.pmf(k, p, loc)`` is identically\n",
            "        equivalent to ``logser.pmf(k - loc, p)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import logser\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> p = 0.6\n",
            "        >>> mean, var, skew, kurt = logser.stats(p, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(logser.ppf(0.01, p),\n",
            "        ...               logser.ppf(0.99, p))\n",
            "        >>> ax.plot(x, logser.pmf(x, p), 'bo', ms=8, label='logser pmf')\n",
            "        >>> ax.vlines(x, 0, logser.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = logser(p)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = logser.cdf(x, p)\n",
            "        >>> np.allclose(x, logser.ppf(prob, p))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = logser.rvs(p, size=1000)\n",
            "    \n",
            "    loguniform = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
            "        A loguniform or reciprocal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `loguniform` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for this class is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{1}{x \\log(b/a)}\n",
            "        \n",
            "        for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n",
            "        :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``loguniform.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``loguniform.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import loguniform\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 0.01, 1.25\n",
            "        >>> mean, var, skew, kurt = loguniform.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(loguniform.ppf(0.01, a, b),\n",
            "        ...                 loguniform.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, loguniform.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='loguniform pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = loguniform(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = loguniform.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], loguniform.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = loguniform.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        This doesn't show the equal probability of ``0.01``, ``0.1`` and\n",
            "        ``1``. This is best when the x-axis is log-scaled:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.hist(np.log10(r))\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> ax.set_xlabel(\"Value of random variable\")\n",
            "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
            "        >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
            "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "        \n",
            "        This random variable will be log-uniform regardless of the base chosen for\n",
            "        ``a`` and ``b``. Let's specify with base ``2`` instead:\n",
            "        \n",
            "        >>> rvs = loguniform(2**-2, 2**0).rvs(size=1000)\n",
            "        \n",
            "        Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n",
            "        variable.  Here's the histogram:\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.hist(np.log2(rvs))\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> ax.set_xlabel(\"Value of random variable\")\n",
            "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
            "        >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
            "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "    \n",
            "    lomax = <scipy.stats._continuous_distns.lomax_gen object>\n",
            "        A Lomax (Pareto of the second kind) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `lomax` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `lomax` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{c}{(1+x)^{c+1}}\n",
            "        \n",
            "        for :math:`x \\ge 0`, :math:`c > 0`.\n",
            "        \n",
            "        `lomax` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        `lomax` is a special case of `pareto` with ``loc=-1.0``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``lomax.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``lomax.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import lomax\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 1.88\n",
            "        >>> mean, var, skew, kurt = lomax.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(lomax.ppf(0.01, c),\n",
            "        ...                 lomax.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, lomax.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='lomax pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = lomax(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = lomax.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], lomax.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = lomax.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    matrix_normal = <scipy.stats._multivariate.matrix_normal_gen object>\n",
            "        A matrix normal random variable.\n",
            "        \n",
            "        The `mean` keyword specifies the mean. The `rowcov` keyword specifies the\n",
            "        among-row covariance matrix. The 'colcov' keyword specifies the\n",
            "        among-column covariance matrix.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(X, mean=None, rowcov=1, colcov=1)\n",
            "            Probability density function.\n",
            "        logpdf(X, mean=None, rowcov=1, colcov=1)\n",
            "            Log of the probability density function.\n",
            "        rvs(mean=None, rowcov=1, colcov=1, size=1, random_state=None)\n",
            "            Draw random samples.\n",
            "        entropy(rowcol=1, colcov=1)\n",
            "            Differential entropy.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        mean : array_like, optional\n",
            "            Mean of the distribution (default: `None`)\n",
            "        rowcov : array_like, optional\n",
            "            Among-row covariance matrix of the distribution (default: `1`)\n",
            "        colcov : array_like, optional\n",
            "            Among-column covariance matrix of the distribution (default: `1`)\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        If `mean` is set to `None` then a matrix of zeros is used for the mean.\n",
            "        The dimensions of this matrix are inferred from the shape of `rowcov` and\n",
            "        `colcov`, if these are provided, or set to `1` if ambiguous.\n",
            "        \n",
            "        `rowcov` and `colcov` can be two-dimensional array_likes specifying the\n",
            "        covariance matrices directly. Alternatively, a one-dimensional array will\n",
            "        be be interpreted as the entries of a diagonal matrix, and a scalar or\n",
            "        zero-dimensional array will be interpreted as this value times the\n",
            "        identity matrix.\n",
            "        \n",
            "        The covariance matrices specified by `rowcov` and `colcov` must be\n",
            "        (symmetric) positive definite. If the samples in `X` are\n",
            "        :math:`m \\times n`, then `rowcov` must be :math:`m \\times m` and\n",
            "        `colcov` must be :math:`n \\times n`. `mean` must be the same shape as `X`.\n",
            "        \n",
            "        The probability density function for `matrix_normal` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(X) = (2 \\pi)^{-\\frac{mn}{2}}|U|^{-\\frac{n}{2}} |V|^{-\\frac{m}{2}}\n",
            "                   \\exp\\left( -\\frac{1}{2} \\mathrm{Tr}\\left[ U^{-1} (X-M) V^{-1}\n",
            "                   (X-M)^T \\right] \\right),\n",
            "        \n",
            "        where :math:`M` is the mean, :math:`U` the among-row covariance matrix,\n",
            "        :math:`V` the among-column covariance matrix.\n",
            "        \n",
            "        The `allow_singular` behaviour of the `multivariate_normal`\n",
            "        distribution is not currently supported. Covariance matrices must be\n",
            "        full rank.\n",
            "        \n",
            "        The `matrix_normal` distribution is closely related to the\n",
            "        `multivariate_normal` distribution. Specifically, :math:`\\mathrm{Vec}(X)`\n",
            "        (the vector formed by concatenating the columns  of :math:`X`) has a\n",
            "        multivariate normal distribution with mean :math:`\\mathrm{Vec}(M)`\n",
            "        and covariance :math:`V \\otimes U` (where :math:`\\otimes` is the Kronecker\n",
            "        product). Sampling and pdf evaluation are\n",
            "        :math:`\\mathcal{O}(m^3 + n^3 + m^2 n + m n^2)` for the matrix normal, but\n",
            "        :math:`\\mathcal{O}(m^3 n^3)` for the equivalent multivariate normal,\n",
            "        making this equivalent form algorithmically inefficient.\n",
            "        \n",
            "        .. versionadded:: 0.17.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import matrix_normal\n",
            "        \n",
            "        >>> M = np.arange(6).reshape(3,2); M\n",
            "        array([[0, 1],\n",
            "               [2, 3],\n",
            "               [4, 5]])\n",
            "        >>> U = np.diag([1,2,3]); U\n",
            "        array([[1, 0, 0],\n",
            "               [0, 2, 0],\n",
            "               [0, 0, 3]])\n",
            "        >>> V = 0.3*np.identity(2); V\n",
            "        array([[ 0.3,  0. ],\n",
            "               [ 0. ,  0.3]])\n",
            "        >>> X = M + 0.1; X\n",
            "        array([[ 0.1,  1.1],\n",
            "               [ 2.1,  3.1],\n",
            "               [ 4.1,  5.1]])\n",
            "        >>> matrix_normal.pdf(X, mean=M, rowcov=U, colcov=V)\n",
            "        0.023410202050005054\n",
            "        \n",
            "        >>> # Equivalent multivariate normal\n",
            "        >>> from scipy.stats import multivariate_normal\n",
            "        >>> vectorised_X = X.T.flatten()\n",
            "        >>> equiv_mean = M.T.flatten()\n",
            "        >>> equiv_cov = np.kron(V,U)\n",
            "        >>> multivariate_normal.pdf(vectorised_X, mean=equiv_mean, cov=equiv_cov)\n",
            "        0.023410202050005054\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the mean\n",
            "        and covariance parameters, returning a \"frozen\" matrix normal\n",
            "        random variable:\n",
            "        \n",
            "        >>> rv = matrix_normal(mean=None, rowcov=1, colcov=1)\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # mean and covariance fixed.\n",
            "    \n",
            "    maxwell = <scipy.stats._continuous_distns.maxwell_gen object>\n",
            "        A Maxwell continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `maxwell` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,\n",
            "        and given ``scale = a``, where ``a`` is the parameter used in the\n",
            "        Mathworld description [1]_.\n",
            "        \n",
            "        The probability density function for `maxwell` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\sqrt{2/\\pi}x^2 \\exp(-x^2/2)\n",
            "        \n",
            "        for :math:`x >= 0`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``maxwell.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``maxwell.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] http://mathworld.wolfram.com/MaxwellDistribution.html\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import maxwell\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = maxwell.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(maxwell.ppf(0.01),\n",
            "        ...                 maxwell.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, maxwell.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='maxwell pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = maxwell()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = maxwell.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], maxwell.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = maxwell.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    mielke = <scipy.stats._continuous_distns.mielke_gen object>\n",
            "        A Mielke Beta-Kappa / Dagum continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `mielke` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(k, s, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, k, s, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, k, s, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, k, s, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, k, s, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, k, s, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, k, s, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, k, s, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, k, s, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, k, s, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(k, s, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(k, s, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(k, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(k, s, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(k, s, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(k, s, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(k, s, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, k, s, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `mielke` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, k, s) = \\frac{k x^{k-1}}{(1+x^s)^{1+k/s}}\n",
            "        \n",
            "        for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes\n",
            "        called Dagum distribution ([2]_). It was already defined in [3]_, called\n",
            "        a Burr Type III distribution (`burr` with parameters ``c=s`` and\n",
            "        ``d=k/s``).\n",
            "        \n",
            "        `mielke` takes ``k`` and ``s`` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``mielke.pdf(x, k, s, loc, scale)`` is identically\n",
            "        equivalent to ``mielke.pdf(y, k, s) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Mielke, P.W., 1973 \"Another Family of Distributions for Describing\n",
            "               and Analyzing Precipitation Data.\" J. Appl. Meteor., 12, 275-280\n",
            "        .. [2] Dagum, C., 1977 \"A new model for personal income distribution.\"\n",
            "               Economie Appliquee, 33, 327-367.\n",
            "        .. [3] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
            "               Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import mielke\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> k, s = 10.4, 4.6\n",
            "        >>> mean, var, skew, kurt = mielke.stats(k, s, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(mielke.ppf(0.01, k, s),\n",
            "        ...                 mielke.ppf(0.99, k, s), 100)\n",
            "        >>> ax.plot(x, mielke.pdf(x, k, s),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='mielke pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = mielke(k, s)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = mielke.ppf([0.001, 0.5, 0.999], k, s)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], mielke.cdf(vals, k, s))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = mielke.rvs(k, s, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    moyal = <scipy.stats._continuous_distns.moyal_gen object>\n",
            "        A Moyal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `moyal` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `moyal` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\exp(-(x + \\exp(-x))/2) / \\sqrt{2\\pi}\n",
            "        \n",
            "        for a real number :math:`x`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``moyal.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``moyal.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        This distribution has utility in high-energy physics and radiation\n",
            "        detection. It describes the energy loss of a charged relativistic\n",
            "        particle due to ionization of the medium [1]_. It also provides an\n",
            "        approximation for the Landau distribution. For an in depth description\n",
            "        see [2]_. For additional description, see [3]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] J.E. Moyal, \"XXX. Theory of ionization fluctuations\",\n",
            "               The London, Edinburgh, and Dublin Philosophical Magazine\n",
            "               and Journal of Science, vol 46, 263-280, (1955).\n",
            "               :doi:`10.1080/14786440308521076` (gated)\n",
            "        .. [2] G. Cordeiro et al., \"The beta Moyal: a useful skew distribution\",\n",
            "               International Journal of Research and Reviews in Applied Sciences,\n",
            "               vol 10, 171-192, (2012).\n",
            "               http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf\n",
            "        .. [3] C. Walck, \"Handbook on Statistical Distributions for\n",
            "               Experimentalists; International Report SUF-PFY/96-01\", Chapter 26,\n",
            "               University of Stockholm: Stockholm, Sweden, (2007).\n",
            "               http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf\n",
            "        \n",
            "        .. versionadded:: 1.1.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import moyal\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = moyal.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(moyal.ppf(0.01),\n",
            "        ...                 moyal.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, moyal.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='moyal pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = moyal()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = moyal.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], moyal.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = moyal.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    multinomial = <scipy.stats._multivariate.multinomial_gen object>\n",
            "        A multinomial random variable.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pmf(x, n, p)\n",
            "            Probability mass function.\n",
            "        logpmf(x, n, p)\n",
            "            Log of the probability mass function.\n",
            "        rvs(n, p, size=1, random_state=None)\n",
            "            Draw random samples from a multinomial distribution.\n",
            "        entropy(n, p)\n",
            "            Compute the entropy of the multinomial distribution.\n",
            "        cov(n, p)\n",
            "            Compute the covariance matrix of the multinomial distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        n : int\n",
            "            Number of trials\n",
            "        p : array_like\n",
            "            Probability of a trial falling into each category; should sum to 1\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `n` should be a nonnegative integer. Each element of `p` should be in the\n",
            "        interval :math:`[0,1]` and the elements should sum to 1. If they do not sum to\n",
            "        1, the last element of the `p` array is not used and is replaced with the\n",
            "        remaining probability left over from the earlier elements.\n",
            "        \n",
            "        The probability mass function for `multinomial` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k},\n",
            "        \n",
            "        supported on :math:`x=(x_1, \\ldots, x_k)` where each :math:`x_i` is a\n",
            "        nonnegative integer and their sum is :math:`n`.\n",
            "        \n",
            "        .. versionadded:: 0.19.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        \n",
            "        >>> from scipy.stats import multinomial\n",
            "        >>> rv = multinomial(8, [0.3, 0.2, 0.5])\n",
            "        >>> rv.pmf([1, 3, 4])\n",
            "        0.042000000000000072\n",
            "        \n",
            "        The multinomial distribution for :math:`k=2` is identical to the\n",
            "        corresponding binomial distribution (tiny numerical differences\n",
            "        notwithstanding):\n",
            "        \n",
            "        >>> from scipy.stats import binom\n",
            "        >>> multinomial.pmf([3, 4], n=7, p=[0.4, 0.6])\n",
            "        0.29030399999999973\n",
            "        >>> binom.pmf(3, 7, 0.4)\n",
            "        0.29030400000000012\n",
            "        \n",
            "        The functions ``pmf``, ``logpmf``, ``entropy``, and ``cov`` support\n",
            "        broadcasting, under the convention that the vector parameters (``x`` and\n",
            "        ``p``) are interpreted as if each row along the last axis is a single\n",
            "        object. For instance:\n",
            "        \n",
            "        >>> multinomial.pmf([[3, 4], [3, 5]], n=[7, 8], p=[.3, .7])\n",
            "        array([0.2268945,  0.25412184])\n",
            "        \n",
            "        Here, ``x.shape == (2, 2)``, ``n.shape == (2,)``, and ``p.shape == (2,)``,\n",
            "        but following the rules mentioned above they behave as if the rows\n",
            "        ``[3, 4]`` and ``[3, 5]`` in ``x`` and ``[.3, .7]`` in ``p`` were a single\n",
            "        object, and as if we had ``x.shape = (2,)``, ``n.shape = (2,)``, and\n",
            "        ``p.shape = ()``. To obtain the individual elements without broadcasting,\n",
            "        we would do this:\n",
            "        \n",
            "        >>> multinomial.pmf([3, 4], n=7, p=[.3, .7])\n",
            "        0.2268945\n",
            "        >>> multinomial.pmf([3, 5], 8, p=[.3, .7])\n",
            "        0.25412184\n",
            "        \n",
            "        This broadcasting also works for ``cov``, where the output objects are\n",
            "        square matrices of size ``p.shape[-1]``. For example:\n",
            "        \n",
            "        >>> multinomial.cov([4, 5], [[.3, .7], [.4, .6]])\n",
            "        array([[[ 0.84, -0.84],\n",
            "                [-0.84,  0.84]],\n",
            "               [[ 1.2 , -1.2 ],\n",
            "                [-1.2 ,  1.2 ]]])\n",
            "        \n",
            "        In this example, ``n.shape == (2,)`` and ``p.shape == (2, 2)``, and\n",
            "        following the rules above, these broadcast as if ``p.shape == (2,)``.\n",
            "        Thus the result should also be of shape ``(2,)``, but since each output is\n",
            "        a :math:`2 \\times 2` matrix, the result in fact has shape ``(2, 2, 2)``,\n",
            "        where ``result[0]`` is equal to ``multinomial.cov(n=4, p=[.3, .7])`` and\n",
            "        ``result[1]`` is equal to ``multinomial.cov(n=5, p=[.4, .6])``.\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the `n` and\n",
            "        `p` parameters, returning a \"frozen\" multinomial random variable:\n",
            "        \n",
            "        >>> rv = multinomial(n=7, p=[.3, .7])\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # degrees of freedom and scale fixed.\n",
            "        \n",
            "        See also\n",
            "        --------\n",
            "        scipy.stats.binom : The binomial distribution.\n",
            "        numpy.random.Generator.multinomial : Sampling from the multinomial distribution.\n",
            "        scipy.stats.multivariate_hypergeom :\n",
            "            The multivariate hypergeometric distribution.\n",
            "    \n",
            "    multivariate_hypergeom = <scipy.stats._multivariate.multivariate_hyper...\n",
            "        A multivariate hypergeometric random variable.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pmf(x, m, n)\n",
            "            Probability mass function.\n",
            "        logpmf(x, m, n)\n",
            "            Log of the probability mass function.\n",
            "        rvs(m, n, size=1, random_state=None)\n",
            "            Draw random samples from a multivariate hypergeometric\n",
            "            distribution.\n",
            "        mean(m, n)\n",
            "            Mean of the multivariate hypergeometric distribution.\n",
            "        var(m, n)\n",
            "            Variance of the multivariate hypergeometric distribution.\n",
            "        cov(m, n)\n",
            "            Compute the covariance matrix of the multivariate\n",
            "            hypergeometric distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        m : array_like\n",
            "            The number of each type of object in the population.\n",
            "            That is, :math:`m[i]` is the number of objects of\n",
            "            type :math:`i`.\n",
            "        n : array_like\n",
            "            The number of samples taken from the population.\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        `m` must be an array of positive integers. If the quantile\n",
            "        :math:`i` contains values out of the range :math:`[0, m_i]`\n",
            "        where :math:`m_i` is the number of objects of type :math:`i`\n",
            "        in the population or if the parameters are inconsistent with one\n",
            "        another (e.g. ``x.sum() != n``), methods return the appropriate\n",
            "        value (e.g. ``0`` for ``pmf``). If `m` or `n` contain negative\n",
            "        values, the result will contain ``nan`` there.\n",
            "        \n",
            "        The probability mass function for `multivariate_hypergeom` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            P(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k) = \\frac{\\binom{m_1}{x_1}\n",
            "            \\binom{m_2}{x_2} \\cdots \\binom{m_k}{x_k}}{\\binom{M}{n}}, \\\\ \\quad\n",
            "            (x_1, x_2, \\ldots, x_k) \\in \\mathbb{N}^k \\text{ with }\n",
            "            \\sum_{i=1}^k x_i = n\n",
            "        \n",
            "        where :math:`m_i` are the number of objects of type :math:`i`, :math:`M`\n",
            "        is the total number of objects in the population (sum of all the\n",
            "        :math:`m_i`), and :math:`n` is the size of the sample to be taken\n",
            "        from the population.\n",
            "        \n",
            "        .. versionadded:: 1.6.0\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        To evaluate the probability mass function of the multivariate\n",
            "        hypergeometric distribution, with a dichotomous population of size\n",
            "        :math:`10` and :math:`20`, at a sample of size :math:`12` with\n",
            "        :math:`8` objects of the first type and :math:`4` objects of the\n",
            "        second type, use:\n",
            "        \n",
            "        >>> from scipy.stats import multivariate_hypergeom\n",
            "        >>> multivariate_hypergeom.pmf(x=[8, 4], m=[10, 20], n=12)\n",
            "        0.0025207176631464523\n",
            "        \n",
            "        The `multivariate_hypergeom` distribution is identical to the\n",
            "        corresponding `hypergeom` distribution (tiny numerical differences\n",
            "        notwithstanding) when only two types (good and bad) of objects\n",
            "        are present in the population as in the example above. Consider\n",
            "        another example for a comparison with the hypergeometric distribution:\n",
            "        \n",
            "        >>> from scipy.stats import hypergeom\n",
            "        >>> multivariate_hypergeom.pmf(x=[3, 1], m=[10, 5], n=4)\n",
            "        0.4395604395604395\n",
            "        >>> hypergeom.pmf(k=3, M=15, n=4, N=10)\n",
            "        0.43956043956044005\n",
            "        \n",
            "        The functions ``pmf``, ``logpmf``, ``mean``, ``var``, ``cov``, and ``rvs``\n",
            "        support broadcasting, under the convention that the vector parameters\n",
            "        (``x``, ``m``, and ``n``) are interpreted as if each row along the last\n",
            "        axis is a single object. For instance, we can combine the previous two\n",
            "        calls to `multivariate_hypergeom` as\n",
            "        \n",
            "        >>> multivariate_hypergeom.pmf(x=[[8, 4], [3, 1]], m=[[10, 20], [10, 5]],\n",
            "        ...                            n=[12, 4])\n",
            "        array([0.00252072, 0.43956044])\n",
            "        \n",
            "        This broadcasting also works for ``cov``, where the output objects are\n",
            "        square matrices of size ``m.shape[-1]``. For example:\n",
            "        \n",
            "        >>> multivariate_hypergeom.cov(m=[[7, 9], [10, 15]], n=[8, 12])\n",
            "        array([[[ 1.05, -1.05],\n",
            "                [-1.05,  1.05]],\n",
            "               [[ 1.56, -1.56],\n",
            "                [-1.56,  1.56]]])\n",
            "        \n",
            "        That is, ``result[0]`` is equal to\n",
            "        ``multivariate_hypergeom.cov(m=[7, 9], n=8)`` and ``result[1]`` is equal\n",
            "        to ``multivariate_hypergeom.cov(m=[10, 15], n=12)``.\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the `m`\n",
            "        and `n` parameters, returning a \"frozen\" multivariate hypergeometric\n",
            "        random variable.\n",
            "        \n",
            "        >>> rv = multivariate_hypergeom(m=[10, 20], n=12)\n",
            "        >>> rv.pmf(x=[8, 4])\n",
            "        0.0025207176631464523\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.hypergeom : The hypergeometric distribution.\n",
            "        scipy.stats.multinomial : The multinomial distribution.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] The Multivariate Hypergeometric Distribution,\n",
            "               http://www.randomservices.org/random/urn/MultiHypergeometric.html\n",
            "        .. [2] Thomas J. Sargent and John Stachurski, 2020,\n",
            "               Multivariate Hypergeometric Distribution\n",
            "               https://python.quantecon.org/multi_hyper.html\n",
            "    \n",
            "    multivariate_normal = <scipy.stats._multivariate.multivariate_normal_g...\n",
            "        A multivariate normal random variable.\n",
            "        \n",
            "        The `mean` keyword specifies the mean. The `cov` keyword specifies the\n",
            "        covariance matrix.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(x, mean=None, cov=1, allow_singular=False)\n",
            "            Probability density function.\n",
            "        logpdf(x, mean=None, cov=1, allow_singular=False)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5, lower_limit=None)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)\n",
            "            Log of the cumulative distribution function.\n",
            "        rvs(mean=None, cov=1, size=1, random_state=None)\n",
            "            Draw random samples from a multivariate normal distribution.\n",
            "        entropy(mean=None, cov=1)\n",
            "            Compute the differential entropy of the multivariate normal.\n",
            "        fit(x, fix_mean=None, fix_cov=None)\n",
            "            Fit a multivariate normal distribution to data.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        mean : array_like, default: ``[0]``\n",
            "            Mean of the distribution.\n",
            "        cov : array_like or `Covariance`, default: ``[1]``\n",
            "            Symmetric positive (semi)definite covariance matrix of the distribution.\n",
            "        allow_singular : bool, default: ``False``\n",
            "            Whether to allow a singular covariance matrix. This is ignored if `cov` is\n",
            "            a `Covariance` object.\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Setting the parameter `mean` to `None` is equivalent to having `mean`\n",
            "        be the zero-vector. The parameter `cov` can be a scalar, in which case\n",
            "        the covariance matrix is the identity times that value, a vector of\n",
            "        diagonal entries for the covariance matrix, a two-dimensional array_like,\n",
            "        or a `Covariance` object.\n",
            "        \n",
            "        The covariance matrix `cov` may be an instance of a subclass of\n",
            "        `Covariance`, e.g. `scipy.stats.CovViaPrecision`. If so, `allow_singular`\n",
            "        is ignored.\n",
            "        \n",
            "        Otherwise, `cov` must be a symmetric positive semidefinite\n",
            "        matrix when `allow_singular` is True; it must be (strictly) positive\n",
            "        definite when `allow_singular` is False.\n",
            "        Symmetry is not checked; only the lower triangular portion is used.\n",
            "        The determinant and inverse of `cov` are computed\n",
            "        as the pseudo-determinant and pseudo-inverse, respectively, so\n",
            "        that `cov` does not need to have full rank.\n",
            "        \n",
            "        The probability density function for `multivariate_normal` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\sqrt{(2 \\pi)^k \\det \\Sigma}}\n",
            "                   \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right),\n",
            "        \n",
            "        where :math:`\\mu` is the mean, :math:`\\Sigma` the covariance matrix,\n",
            "        :math:`k` the rank of :math:`\\Sigma`. In case of singular :math:`\\Sigma`,\n",
            "        SciPy extends this definition according to [1]_.\n",
            "        \n",
            "        .. versionadded:: 0.14.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Multivariate Normal Distribution - Degenerate Case, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Degenerate_case\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import multivariate_normal\n",
            "        \n",
            "        >>> x = np.linspace(0, 5, 10, endpoint=False)\n",
            "        >>> y = multivariate_normal.pdf(x, mean=2.5, cov=0.5); y\n",
            "        array([ 0.00108914,  0.01033349,  0.05946514,  0.20755375,  0.43939129,\n",
            "                0.56418958,  0.43939129,  0.20755375,  0.05946514,  0.01033349])\n",
            "        >>> fig1 = plt.figure()\n",
            "        >>> ax = fig1.add_subplot(111)\n",
            "        >>> ax.plot(x, y)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the mean\n",
            "        and covariance parameters, returning a \"frozen\" multivariate normal\n",
            "        random variable:\n",
            "        \n",
            "        >>> rv = multivariate_normal(mean=None, cov=1, allow_singular=False)\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # mean and covariance fixed.\n",
            "        \n",
            "        The input quantiles can be any shape of array, as long as the last\n",
            "        axis labels the components.  This allows us for instance to\n",
            "        display the frozen pdf for a non-isotropic random variable in 2D as\n",
            "        follows:\n",
            "        \n",
            "        >>> x, y = np.mgrid[-1:1:.01, -1:1:.01]\n",
            "        >>> pos = np.dstack((x, y))\n",
            "        >>> rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\n",
            "        >>> fig2 = plt.figure()\n",
            "        >>> ax2 = fig2.add_subplot(111)\n",
            "        >>> ax2.contourf(x, y, rv.pdf(pos))\n",
            "    \n",
            "    multivariate_t = <scipy.stats._multivariate.multivariate_t_gen object>\n",
            "        A multivariate t-distributed random variable.\n",
            "        \n",
            "        The `loc` parameter specifies the location. The `shape` parameter specifies\n",
            "        the positive semidefinite shape matrix. The `df` parameter specifies the\n",
            "        degrees of freedom.\n",
            "        \n",
            "        In addition to calling the methods below, the object itself may be called\n",
            "        as a function to fix the location, shape matrix, and degrees of freedom\n",
            "        parameters, returning a \"frozen\" multivariate t-distribution random.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(x, loc=None, shape=1, df=1, allow_singular=False)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=None, shape=1, df=1, allow_singular=False)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=None, shape=1, df=1, allow_singular=False, *,\n",
            "            maxpts=None, lower_limit=None, random_state=None)\n",
            "            Cumulative distribution function.\n",
            "        rvs(loc=None, shape=1, df=1, size=1, random_state=None)\n",
            "            Draw random samples from a multivariate t-distribution.\n",
            "        entropy(loc=None, shape=1, df=1)\n",
            "            Differential entropy of a multivariate t-distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        loc : array_like, optional\n",
            "            Location of the distribution. (default ``0``)\n",
            "        shape : array_like, optional\n",
            "            Positive semidefinite matrix of the distribution. (default ``1``)\n",
            "        df : float, optional\n",
            "            Degrees of freedom of the distribution; must be greater than zero.\n",
            "            If ``np.inf`` then results are multivariate normal. The default is ``1``.\n",
            "        allow_singular : bool, optional\n",
            "            Whether to allow a singular matrix. (default ``False``)\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Setting the parameter `loc` to ``None`` is equivalent to having `loc`\n",
            "        be the zero-vector. The parameter `shape` can be a scalar, in which case\n",
            "        the shape matrix is the identity times that value, a vector of\n",
            "        diagonal entries for the shape matrix, or a two-dimensional array_like.\n",
            "        The matrix `shape` must be a (symmetric) positive semidefinite matrix. The\n",
            "        determinant and inverse of `shape` are computed as the pseudo-determinant\n",
            "        and pseudo-inverse, respectively, so that `shape` does not need to have\n",
            "        full rank.\n",
            "        \n",
            "        The probability density function for `multivariate_t` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{\\Gamma((\\nu + p)/2)}{\\Gamma(\\nu/2)\\nu^{p/2}\\pi^{p/2}|\\Sigma|^{1/2}}\n",
            "                   \\left[1 + \\frac{1}{\\nu} (\\mathbf{x} - \\boldsymbol{\\mu})^{\\top}\n",
            "                   \\boldsymbol{\\Sigma}^{-1}\n",
            "                   (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]^{-(\\nu + p)/2},\n",
            "        \n",
            "        where :math:`p` is the dimension of :math:`\\mathbf{x}`,\n",
            "        :math:`\\boldsymbol{\\mu}` is the :math:`p`-dimensional location,\n",
            "        :math:`\\boldsymbol{\\Sigma}` the :math:`p \\times p`-dimensional shape\n",
            "        matrix, and :math:`\\nu` is the degrees of freedom.\n",
            "        \n",
            "        .. versionadded:: 1.6.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Arellano-Valle et al. \"Shannon Entropy and Mutual Information for\n",
            "               Multivariate Skew-Elliptical Distributions\". Scandinavian Journal\n",
            "               of Statistics. Vol. 40, issue 1.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        The object may be called (as a function) to fix the `loc`, `shape`,\n",
            "        `df`, and `allow_singular` parameters, returning a \"frozen\"\n",
            "        multivariate_t random variable:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import multivariate_t\n",
            "        >>> rv = multivariate_t([1.0, -0.5], [[2.1, 0.3], [0.3, 1.5]], df=2)\n",
            "        >>> # Frozen object with the same methods but holding the given location,\n",
            "        >>> # scale, and degrees of freedom fixed.\n",
            "        \n",
            "        Create a contour plot of the PDF.\n",
            "        \n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> x, y = np.mgrid[-1:3:.01, -2:1.5:.01]\n",
            "        >>> pos = np.dstack((x, y))\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.set_aspect('equal')\n",
            "        >>> plt.contourf(x, y, rv.pdf(pos))\n",
            "    \n",
            "    nakagami = <scipy.stats._continuous_distns.nakagami_gen object>\n",
            "        A Nakagami continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `nakagami` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(nu, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, nu, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, nu, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, nu, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, nu, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, nu, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, nu, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, nu, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, nu, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, nu, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(nu, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(nu, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(nu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(nu, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(nu, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(nu, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(nu, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, nu, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `nakagami` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\nu) = \\frac{2 \\nu^\\nu}{\\Gamma(\\nu)} x^{2\\nu-1} \\exp(-\\nu x^2)\n",
            "        \n",
            "        for :math:`x >= 0`, :math:`\\nu > 0`. The distribution was introduced in\n",
            "        [2]_, see also [1]_ for further information.\n",
            "        \n",
            "        `nakagami` takes ``nu`` as a shape parameter for :math:`\\nu`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``nakagami.pdf(x, nu, loc, scale)`` is identically\n",
            "        equivalent to ``nakagami.pdf(y, nu) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Nakagami distribution\", Wikipedia\n",
            "               https://en.wikipedia.org/wiki/Nakagami_distribution\n",
            "        .. [2] M. Nakagami, \"The m-distribution - A general formula of intensity\n",
            "               distribution of rapid fading\", Statistical methods in radio wave\n",
            "               propagation, Pergamon Press, 1960, 3-36.\n",
            "               :doi:`10.1016/B978-0-08-009306-2.50005-4`\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nakagami\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> nu = 4.97\n",
            "        >>> mean, var, skew, kurt = nakagami.stats(nu, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(nakagami.ppf(0.01, nu),\n",
            "        ...                 nakagami.ppf(0.99, nu), 100)\n",
            "        >>> ax.plot(x, nakagami.pdf(x, nu),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='nakagami pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = nakagami(nu)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = nakagami.ppf([0.001, 0.5, 0.999], nu)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], nakagami.cdf(vals, nu))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = nakagami.rvs(nu, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    nbinom = <scipy.stats._discrete_distns.nbinom_gen object>\n",
            "        A negative binomial discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `nbinom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(n, p, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, n, p, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, n, p, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, n, p, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, n, p, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, n, p, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, n, p, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, n, p, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, n, p, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(n, p, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(n, p, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(n, p, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(n, p, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(n, p, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(n, p, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, n, p, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Negative binomial distribution describes a sequence of i.i.d. Bernoulli\n",
            "        trials, repeated until a predefined, non-random number of successes occurs.\n",
            "        \n",
            "        The probability mass function of the number of failures for `nbinom` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           f(k) = \\binom{k+n-1}{n-1} p^n (1-p)^k\n",
            "        \n",
            "        for :math:`k \\ge 0`, :math:`0 < p \\leq 1`\n",
            "        \n",
            "        `nbinom` takes :math:`n` and :math:`p` as shape parameters where :math:`n`\n",
            "        is the number of successes, :math:`p` is the probability of a single\n",
            "        success, and :math:`1-p` is the probability of a single failure.\n",
            "        \n",
            "        Another common parameterization of the negative binomial distribution is\n",
            "        in terms of the mean number of failures :math:`\\mu` to achieve :math:`n`\n",
            "        successes. The mean :math:`\\mu` is related to the probability of success\n",
            "        as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           p = \\frac{n}{n + \\mu}\n",
            "        \n",
            "        The number of successes :math:`n` may also be specified in terms of a\n",
            "        \"dispersion\", \"heterogeneity\", or \"aggregation\" parameter :math:`\\alpha`,\n",
            "        which relates the mean :math:`\\mu` to the variance :math:`\\sigma^2`,\n",
            "        e.g. :math:`\\sigma^2 = \\mu + \\alpha \\mu^2`. Regardless of the convention\n",
            "        used for :math:`\\alpha`,\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "           p &= \\frac{\\mu}{\\sigma^2} \\\\\n",
            "           n &= \\frac{\\mu^2}{\\sigma^2 - \\mu}\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``nbinom.pmf(k, n, p, loc)`` is identically\n",
            "        equivalent to ``nbinom.pmf(k - loc, n, p)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nbinom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> n, p = 5, 0.5\n",
            "        >>> mean, var, skew, kurt = nbinom.stats(n, p, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(nbinom.ppf(0.01, n, p),\n",
            "        ...               nbinom.ppf(0.99, n, p))\n",
            "        >>> ax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')\n",
            "        >>> ax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = nbinom(n, p)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = nbinom.cdf(x, n, p)\n",
            "        >>> np.allclose(x, nbinom.ppf(prob, n, p))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = nbinom.rvs(n, p, size=1000)\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        hypergeom, binom, nhypergeom\n",
            "    \n",
            "    ncf = <scipy.stats._continuous_distns.ncf_gen object>\n",
            "        A non-central F distribution continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `ncf` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(dfn, dfd, nc, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(dfn, dfd, nc, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(dfn, dfd, nc, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(dfn, dfd, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(dfn, dfd, nc, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(dfn, dfd, nc, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(dfn, dfd, nc, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(dfn, dfd, nc, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, dfn, dfd, nc, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.f : Fisher distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `ncf` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, n_1, n_2, \\lambda) =\n",
            "                \\exp\\left(\\frac{\\lambda}{2} +\n",
            "                          \\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\n",
            "                    \\right)\n",
            "                n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\\\\n",
            "                (n_2 + n_1 x)^{-(n_1 + n_2)/2}\n",
            "                \\gamma(n_1/2) \\gamma(1 + n_2/2) \\\\\n",
            "                \\frac{L^{\\frac{n_1}{2}-1}_{n_2/2}\n",
            "                    \\left(-\\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\\right)}\n",
            "                {B(n_1/2, n_2/2)\n",
            "                    \\gamma\\left(\\frac{n_1 + n_2}{2}\\right)}\n",
            "        \n",
            "        for :math:`n_1, n_2 > 0`, :math:`\\lambda \\ge 0`.  Here :math:`n_1` is the\n",
            "        degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in\n",
            "        the denominator, :math:`\\lambda` the non-centrality parameter,\n",
            "        :math:`\\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a\n",
            "        generalized Laguerre polynomial and :math:`B` is the beta function.\n",
            "        \n",
            "        `ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters. If ``nc=0``,\n",
            "        the distribution becomes equivalent to the Fisher distribution.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``ncf.pdf(x, dfn, dfd, nc, loc, scale)`` is identically\n",
            "        equivalent to ``ncf.pdf(y, dfn, dfd, nc) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ncf\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> dfn, dfd, nc = 27, 27, 0.416\n",
            "        >>> mean, var, skew, kurt = ncf.stats(dfn, dfd, nc, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(ncf.ppf(0.01, dfn, dfd, nc),\n",
            "        ...                 ncf.ppf(0.99, dfn, dfd, nc), 100)\n",
            "        >>> ax.plot(x, ncf.pdf(x, dfn, dfd, nc),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='ncf pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = ncf(dfn, dfd, nc)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = ncf.ppf([0.001, 0.5, 0.999], dfn, dfd, nc)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], ncf.cdf(vals, dfn, dfd, nc))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = ncf.rvs(dfn, dfd, nc, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    nchypergeom_fisher = <scipy.stats._discrete_distns.nchypergeom_fisher_...\n",
            "        A Fisher's noncentral hypergeometric discrete random variable.\n",
            "        \n",
            "        Fisher's noncentral hypergeometric distribution models drawing objects of\n",
            "        two types from a bin. `M` is the total number of objects, `n` is the\n",
            "        number of Type I objects, and `odds` is the odds ratio: the odds of\n",
            "        selecting a Type I object rather than a Type II object when there is only\n",
            "        one object of each type.\n",
            "        The random variate represents the number of Type I objects drawn if we\n",
            "        take a handful of objects from the bin at once and find out afterwards\n",
            "        that we took `N` objects.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `nchypergeom_fisher` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, M, n, N, odds, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, M, n, N, odds, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, M, n, N, odds, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, M, n, N, odds, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, M, n, N, odds, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, M, n, N, odds, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, M, n, N, odds, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, M, n, N, odds, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(M, n, N, odds, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(M, n, N, odds, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(M, n, N, odds, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(M, n, N, odds, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(M, n, N, odds, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(M, n, N, odds, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, M, n, N, odds, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        nchypergeom_wallenius, hypergeom, nhypergeom\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n",
            "        with parameters `N`, `n`, and `M` (respectively) as defined above.\n",
            "        \n",
            "        The probability mass function is defined as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            p(x; M, n, N, \\omega) =\n",
            "            \\frac{\\binom{n}{x}\\binom{M - n}{N-x}\\omega^x}{P_0},\n",
            "        \n",
            "        for\n",
            "        :math:`x \\in [x_l, x_u]`,\n",
            "        :math:`M \\in {\\mathbb N}`,\n",
            "        :math:`n \\in [0, M]`,\n",
            "        :math:`N \\in [0, M]`,\n",
            "        :math:`\\omega > 0`,\n",
            "        where\n",
            "        :math:`x_l = \\max(0, N - (M - n))`,\n",
            "        :math:`x_u = \\min(N, n)`,\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            P_0 = \\sum_{y=x_l}^{x_u} \\binom{n}{y}\\binom{M - n}{N-y}\\omega^y,\n",
            "        \n",
            "        and the binomial coefficients are defined as\n",
            "        \n",
            "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
            "        \n",
            "        `nchypergeom_fisher` uses the BiasedUrn package by Agner Fog with\n",
            "        permission for it to be distributed under SciPy's license.\n",
            "        \n",
            "        The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n",
            "        universally accepted; they are chosen for consistency with `hypergeom`.\n",
            "        \n",
            "        Note that Fisher's noncentral hypergeometric distribution is distinct\n",
            "        from Wallenius' noncentral hypergeometric distribution, which models\n",
            "        drawing a pre-determined `N` objects from a bin one by one.\n",
            "        When the odds ratio is unity, however, both distributions reduce to the\n",
            "        ordinary hypergeometric distribution.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``nchypergeom_fisher.pmf(k, M, n, N, odds, loc)`` is identically\n",
            "        equivalent to ``nchypergeom_fisher.pmf(k - loc, M, n, N, odds)``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Agner Fog, \"Biased Urn Theory\".\n",
            "               https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n",
            "        \n",
            "        .. [2] \"Fisher's noncentral hypergeometric distribution\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Fisher's_noncentral_hypergeometric_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nchypergeom_fisher\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> M, n, N, odds = 140, 80, 60, 0.5\n",
            "        >>> mean, var, skew, kurt = nchypergeom_fisher.stats(M, n, N, odds, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(nchypergeom_fisher.ppf(0.01, M, n, N, odds),\n",
            "        ...               nchypergeom_fisher.ppf(0.99, M, n, N, odds))\n",
            "        >>> ax.plot(x, nchypergeom_fisher.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_fisher pmf')\n",
            "        >>> ax.vlines(x, 0, nchypergeom_fisher.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = nchypergeom_fisher(M, n, N, odds)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = nchypergeom_fisher.cdf(x, M, n, N, odds)\n",
            "        >>> np.allclose(x, nchypergeom_fisher.ppf(prob, M, n, N, odds))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = nchypergeom_fisher.rvs(M, n, N, odds, size=1000)\n",
            "    \n",
            "    nchypergeom_wallenius = <scipy.stats._discrete_distns.nchypergeom_wall...\n",
            "        A Wallenius' noncentral hypergeometric discrete random variable.\n",
            "        \n",
            "        Wallenius' noncentral hypergeometric distribution models drawing objects of\n",
            "        two types from a bin. `M` is the total number of objects, `n` is the\n",
            "        number of Type I objects, and `odds` is the odds ratio: the odds of\n",
            "        selecting a Type I object rather than a Type II object when there is only\n",
            "        one object of each type.\n",
            "        The random variate represents the number of Type I objects drawn if we\n",
            "        draw a pre-determined `N` objects from a bin one by one.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `nchypergeom_wallenius` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, M, n, N, odds, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, M, n, N, odds, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, M, n, N, odds, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, M, n, N, odds, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, M, n, N, odds, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, M, n, N, odds, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, M, n, N, odds, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, M, n, N, odds, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(M, n, N, odds, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(M, n, N, odds, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(M, n, N, odds, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(M, n, N, odds, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(M, n, N, odds, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(M, n, N, odds, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, M, n, N, odds, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        nchypergeom_fisher, hypergeom, nhypergeom\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n",
            "        with parameters `N`, `n`, and `M` (respectively) as defined above.\n",
            "        \n",
            "        The probability mass function is defined as\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            p(x; N, n, M) = \\binom{n}{x} \\binom{M - n}{N-x}\n",
            "            \\int_0^1 \\left(1-t^{\\omega/D}\\right)^x\\left(1-t^{1/D}\\right)^{N-x} dt\n",
            "        \n",
            "        for\n",
            "        :math:`x \\in [x_l, x_u]`,\n",
            "        :math:`M \\in {\\mathbb N}`,\n",
            "        :math:`n \\in [0, M]`,\n",
            "        :math:`N \\in [0, M]`,\n",
            "        :math:`\\omega > 0`,\n",
            "        where\n",
            "        :math:`x_l = \\max(0, N - (M - n))`,\n",
            "        :math:`x_u = \\min(N, n)`,\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            D = \\omega(n - x) + ((M - n)-(N-x)),\n",
            "        \n",
            "        and the binomial coefficients are defined as\n",
            "        \n",
            "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
            "        \n",
            "        `nchypergeom_wallenius` uses the BiasedUrn package by Agner Fog with\n",
            "        permission for it to be distributed under SciPy's license.\n",
            "        \n",
            "        The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n",
            "        universally accepted; they are chosen for consistency with `hypergeom`.\n",
            "        \n",
            "        Note that Wallenius' noncentral hypergeometric distribution is distinct\n",
            "        from Fisher's noncentral hypergeometric distribution, which models\n",
            "        take a handful of objects from the bin at once, finding out afterwards\n",
            "        that `N` objects were taken.\n",
            "        When the odds ratio is unity, however, both distributions reduce to the\n",
            "        ordinary hypergeometric distribution.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``nchypergeom_wallenius.pmf(k, M, n, N, odds, loc)`` is identically\n",
            "        equivalent to ``nchypergeom_wallenius.pmf(k - loc, M, n, N, odds)``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Agner Fog, \"Biased Urn Theory\".\n",
            "               https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n",
            "        \n",
            "        .. [2] \"Wallenius' noncentral hypergeometric distribution\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Wallenius'_noncentral_hypergeometric_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nchypergeom_wallenius\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> M, n, N, odds = 140, 80, 60, 0.5\n",
            "        >>> mean, var, skew, kurt = nchypergeom_wallenius.stats(M, n, N, odds, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(nchypergeom_wallenius.ppf(0.01, M, n, N, odds),\n",
            "        ...               nchypergeom_wallenius.ppf(0.99, M, n, N, odds))\n",
            "        >>> ax.plot(x, nchypergeom_wallenius.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_wallenius pmf')\n",
            "        >>> ax.vlines(x, 0, nchypergeom_wallenius.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = nchypergeom_wallenius(M, n, N, odds)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = nchypergeom_wallenius.cdf(x, M, n, N, odds)\n",
            "        >>> np.allclose(x, nchypergeom_wallenius.ppf(prob, M, n, N, odds))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = nchypergeom_wallenius.rvs(M, n, N, odds, size=1000)\n",
            "    \n",
            "    nct = <scipy.stats._continuous_distns.nct_gen object>\n",
            "        A non-central Student's t continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `nct` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, df, nc, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, df, nc, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, df, nc, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, df, nc, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, df, nc, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, df, nc, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(df, nc, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(df, nc, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(df, nc, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(df, nc, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(df, nc, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(df, nc, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, df, nc, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        If :math:`Y` is a standard normal random variable and :math:`V` is\n",
            "        an independent chi-square random variable (`chi2`) with :math:`k` degrees\n",
            "        of freedom, then\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            X = \\frac{Y + c}{\\sqrt{V/k}}\n",
            "        \n",
            "        has a non-central Student's t distribution on the real line.\n",
            "        The degrees of freedom parameter :math:`k` (denoted ``df`` in the\n",
            "        implementation) satisfies :math:`k > 0` and the noncentrality parameter\n",
            "        :math:`c` (denoted ``nc`` in the implementation) is a real number.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``nct.pdf(x, df, nc, loc, scale)`` is identically\n",
            "        equivalent to ``nct.pdf(y, df, nc) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nct\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> df, nc = 14, 0.24\n",
            "        >>> mean, var, skew, kurt = nct.stats(df, nc, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(nct.ppf(0.01, df, nc),\n",
            "        ...                 nct.ppf(0.99, df, nc), 100)\n",
            "        >>> ax.plot(x, nct.pdf(x, df, nc),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='nct pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = nct(df, nc)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = nct.ppf([0.001, 0.5, 0.999], df, nc)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], nct.cdf(vals, df, nc))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = nct.rvs(df, nc, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    ncx2 = <scipy.stats._continuous_distns.ncx2_gen object>\n",
            "        A non-central chi-squared continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `ncx2` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, df, nc, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, df, nc, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, df, nc, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, df, nc, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, df, nc, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, df, nc, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, df, nc, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(df, nc, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(df, nc, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(df, nc, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(df, nc, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(df, nc, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(df, nc, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, df, nc, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `ncx2` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, k, \\lambda) = \\frac{1}{2} \\exp(-(\\lambda+x)/2)\n",
            "                (x/\\lambda)^{(k-2)/4}  I_{(k-2)/2}(\\sqrt{\\lambda x})\n",
            "        \n",
            "        for :math:`x >= 0`, :math:`k > 0` and :math:`\\lambda \\ge 0`.\n",
            "        :math:`k` specifies the degrees of freedom (denoted ``df`` in the\n",
            "        implementation) and :math:`\\lambda` is the non-centrality parameter\n",
            "        (denoted ``nc`` in the implementation). :math:`I_\\nu` denotes the\n",
            "        modified Bessel function of first order of degree :math:`\\nu`\n",
            "        (`scipy.special.iv`).\n",
            "        \n",
            "        `ncx2` takes ``df`` and ``nc`` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``ncx2.pdf(x, df, nc, loc, scale)`` is identically\n",
            "        equivalent to ``ncx2.pdf(y, df, nc) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import ncx2\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> df, nc = 21, 1.06\n",
            "        >>> mean, var, skew, kurt = ncx2.stats(df, nc, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(ncx2.ppf(0.01, df, nc),\n",
            "        ...                 ncx2.ppf(0.99, df, nc), 100)\n",
            "        >>> ax.plot(x, ncx2.pdf(x, df, nc),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='ncx2 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = ncx2(df, nc)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = ncx2.ppf([0.001, 0.5, 0.999], df, nc)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], ncx2.cdf(vals, df, nc))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = ncx2.rvs(df, nc, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    nhypergeom = <scipy.stats._discrete_distns.nhypergeom_gen object>\n",
            "        A negative hypergeometric discrete random variable.\n",
            "        \n",
            "        Consider a box containing :math:`M` balls:, :math:`n` red and\n",
            "        :math:`M-n` blue. We randomly sample balls from the box, one\n",
            "        at a time and *without* replacement, until we have picked :math:`r`\n",
            "        blue balls. `nhypergeom` is the distribution of the number of\n",
            "        red balls :math:`k` we have picked.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `nhypergeom` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(M, n, r, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, M, n, r, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, M, n, r, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, M, n, r, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, M, n, r, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, M, n, r, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, M, n, r, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, M, n, r, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, M, n, r, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(M, n, r, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(M, n, r, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(M, n, r), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(M, n, r, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(M, n, r, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(M, n, r, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(M, n, r, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, M, n, r, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The symbols used to denote the shape parameters (`M`, `n`, and `r`) are not\n",
            "        universally accepted. See the Examples for a clarification of the\n",
            "        definitions used here.\n",
            "        \n",
            "        The probability mass function is defined as,\n",
            "        \n",
            "        .. math:: f(k; M, n, r) = \\frac{{{k+r-1}\\choose{k}}{{M-r-k}\\choose{n-k}}}\n",
            "                                       {{M \\choose n}}\n",
            "        \n",
            "        for :math:`k \\in [0, n]`, :math:`n \\in [0, M]`, :math:`r \\in [0, M-n]`,\n",
            "        and the binomial coefficient is:\n",
            "        \n",
            "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
            "        \n",
            "        It is equivalent to observing :math:`k` successes in :math:`k+r-1`\n",
            "        samples with :math:`k+r`'th sample being a failure. The former\n",
            "        can be modelled as a hypergeometric distribution. The probability\n",
            "        of the latter is simply the number of failures remaining\n",
            "        :math:`M-n-(r-1)` divided by the size of the remaining population\n",
            "        :math:`M-(k+r-1)`. This relationship can be shown as:\n",
            "        \n",
            "        .. math:: NHG(k;M,n,r) = HG(k;M,n,k+r-1)\\frac{(M-n-(r-1))}{(M-(k+r-1))}\n",
            "        \n",
            "        where :math:`NHG` is probability mass function (PMF) of the\n",
            "        negative hypergeometric distribution and :math:`HG` is the\n",
            "        PMF of the hypergeometric distribution.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``nhypergeom.pmf(k, M, n, r, loc)`` is identically\n",
            "        equivalent to ``nhypergeom.pmf(k - loc, M, n, r)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import nhypergeom\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        \n",
            "        Suppose we have a collection of 20 animals, of which 7 are dogs.\n",
            "        Then if we want to know the probability of finding a given number\n",
            "        of dogs (successes) in a sample with exactly 12 animals that\n",
            "        aren't dogs (failures), we can initialize a frozen distribution\n",
            "        and plot the probability mass function:\n",
            "        \n",
            "        >>> M, n, r = [20, 7, 12]\n",
            "        >>> rv = nhypergeom(M, n, r)\n",
            "        >>> x = np.arange(0, n+2)\n",
            "        >>> pmf_dogs = rv.pmf(x)\n",
            "        \n",
            "        >>> fig = plt.figure()\n",
            "        >>> ax = fig.add_subplot(111)\n",
            "        >>> ax.plot(x, pmf_dogs, 'bo')\n",
            "        >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n",
            "        >>> ax.set_xlabel('# of dogs in our group with given 12 failures')\n",
            "        >>> ax.set_ylabel('nhypergeom PMF')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Instead of using a frozen distribution we can also use `nhypergeom`\n",
            "        methods directly.  To for example obtain the probability mass\n",
            "        function, use:\n",
            "        \n",
            "        >>> prb = nhypergeom.pmf(x, M, n, r)\n",
            "        \n",
            "        And to generate random numbers:\n",
            "        \n",
            "        >>> R = nhypergeom.rvs(M, n, r, size=10)\n",
            "        \n",
            "        To verify the relationship between `hypergeom` and `nhypergeom`, use:\n",
            "        \n",
            "        >>> from scipy.stats import hypergeom, nhypergeom\n",
            "        >>> M, n, r = 45, 13, 8\n",
            "        >>> k = 6\n",
            "        >>> nhypergeom.pmf(k, M, n, r)\n",
            "        0.06180776620271643\n",
            "        >>> hypergeom.pmf(k, M, n, k+r-1) * (M - n - (r-1)) / (M - (k+r-1))\n",
            "        0.06180776620271644\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        hypergeom, binom, nbinom\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Negative Hypergeometric Distribution on Wikipedia\n",
            "               https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution\n",
            "        \n",
            "        .. [2] Negative Hypergeometric Distribution from\n",
            "               http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Negativehypergeometric.pdf\n",
            "    \n",
            "    norm = <scipy.stats._continuous_distns.norm_gen object>\n",
            "        A normal continuous random variable.\n",
            "        \n",
            "        The location (``loc``) keyword specifies the mean.\n",
            "        The scale (``scale``) keyword specifies the standard deviation.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `norm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `norm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\n",
            "        \n",
            "        for a real number :math:`x`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``norm.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``norm.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import norm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = norm.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(norm.ppf(0.01),\n",
            "        ...                 norm.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, norm.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='norm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = norm()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = norm.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = norm.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    norminvgauss = <scipy.stats._continuous_distns.norminvgauss_gen object...\n",
            "        A Normal Inverse Gaussian continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `norminvgauss` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `norminvgauss` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{a \\, K_1(a \\sqrt{1 + x^2})}{\\pi \\sqrt{1 + x^2}} \\,\n",
            "                         \\exp(\\sqrt{a^2 - b^2} + b x)\n",
            "        \n",
            "        where :math:`x` is a real number, the parameter :math:`a` is the tail\n",
            "        heaviness and :math:`b` is the asymmetry parameter satisfying\n",
            "        :math:`a > 0` and :math:`|b| <= a`.\n",
            "        :math:`K_1` is the modified Bessel function of second kind\n",
            "        (`scipy.special.k1`).\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``norminvgauss.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``norminvgauss.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        A normal inverse Gaussian random variable `Y` with parameters `a` and `b`\n",
            "        can be expressed as a normal mean-variance mixture:\n",
            "        `Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is\n",
            "        `invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used\n",
            "        to generate random variates.\n",
            "        \n",
            "        Another common parametrization of the distribution (see Equation 2.1 in\n",
            "        [2]_) is given by the following expression of the pdf:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            g(x, \\alpha, \\beta, \\delta, \\mu) =\n",
            "            \\frac{\\alpha\\delta K_1\\left(\\alpha\\sqrt{\\delta^2 + (x - \\mu)^2}\\right)}\n",
            "            {\\pi \\sqrt{\\delta^2 + (x - \\mu)^2}} \\,\n",
            "            e^{\\delta \\sqrt{\\alpha^2 - \\beta^2} + \\beta (x - \\mu)}\n",
            "        \n",
            "        In SciPy, this corresponds to\n",
            "        `a = alpha * delta, b = beta * delta, loc = mu, scale=delta`.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions on\n",
            "               Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n",
            "               pp. 151-157, 1978.\n",
            "        \n",
            "        .. [2] O. Barndorff-Nielsen, \"Normal Inverse Gaussian Distributions and\n",
            "               Stochastic Volatility Modelling\", Scandinavian Journal of\n",
            "               Statistics, Vol. 24, pp. 1-13, 1997.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import norminvgauss\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 1.25, 0.5\n",
            "        >>> mean, var, skew, kurt = norminvgauss.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(norminvgauss.ppf(0.01, a, b),\n",
            "        ...                 norminvgauss.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, norminvgauss.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='norminvgauss pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = norminvgauss(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = norminvgauss.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], norminvgauss.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = norminvgauss.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    ortho_group = <scipy.stats._multivariate.ortho_group_gen object>\n",
            "    pareto = <scipy.stats._continuous_distns.pareto_gen object>\n",
            "        A Pareto continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `pareto` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `pareto` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, b) = \\frac{b}{x^{b+1}}\n",
            "        \n",
            "        for :math:`x \\ge 1`, :math:`b > 0`.\n",
            "        \n",
            "        `pareto` takes ``b`` as a shape parameter for :math:`b`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``pareto.pdf(x, b, loc, scale)`` is identically\n",
            "        equivalent to ``pareto.pdf(y, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import pareto\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> b = 2.62\n",
            "        >>> mean, var, skew, kurt = pareto.stats(b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(pareto.ppf(0.01, b),\n",
            "        ...                 pareto.ppf(0.99, b), 100)\n",
            "        >>> ax.plot(x, pareto.pdf(x, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='pareto pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = pareto(b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = pareto.ppf([0.001, 0.5, 0.999], b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], pareto.cdf(vals, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = pareto.rvs(b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    pearson3 = <scipy.stats._continuous_distns.pearson3_gen object>\n",
            "        A pearson type III continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `pearson3` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(skew, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, skew, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, skew, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, skew, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, skew, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, skew, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, skew, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, skew, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, skew, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, skew, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(skew, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(skew, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(skew,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(skew, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(skew, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(skew, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(skew, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, skew, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `pearson3` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\kappa) = \\frac{|\\beta|}{\\Gamma(\\alpha)}\n",
            "                           (\\beta (x - \\zeta))^{\\alpha - 1}\n",
            "                           \\exp(-\\beta (x - \\zeta))\n",
            "        \n",
            "        where:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "                \\beta = \\frac{2}{\\kappa}\n",
            "        \n",
            "                \\alpha = \\beta^2 = \\frac{4}{\\kappa^2}\n",
            "        \n",
            "                \\zeta = -\\frac{\\alpha}{\\beta} = -\\beta\n",
            "        \n",
            "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
            "        Pass the skew :math:`\\kappa` into `pearson3` as the shape parameter\n",
            "        ``skew``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``pearson3.pdf(x, skew, loc, scale)`` is identically\n",
            "        equivalent to ``pearson3.pdf(y, skew) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import pearson3\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> skew = -2\n",
            "        >>> mean, var, skew, kurt = pearson3.stats(skew, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(pearson3.ppf(0.01, skew),\n",
            "        ...                 pearson3.ppf(0.99, skew), 100)\n",
            "        >>> ax.plot(x, pearson3.pdf(x, skew),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='pearson3 pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = pearson3(skew)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = pearson3.ppf([0.001, 0.5, 0.999], skew)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], pearson3.cdf(vals, skew))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = pearson3.rvs(skew, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        R.W. Vogel and D.E. McMartin, \"Probability Plot Goodness-of-Fit and\n",
            "        Skewness Estimation Procedures for the Pearson Type 3 Distribution\", Water\n",
            "        Resources Research, Vol.27, 3149-3158 (1991).\n",
            "        \n",
            "        L.R. Salvosa, \"Tables of Pearson's Type III Function\", Ann. Math. Statist.,\n",
            "        Vol.1, 191-198 (1930).\n",
            "        \n",
            "        \"Using Modern Computing Tools to Fit the Pearson Type III Distribution to\n",
            "        Aviation Loads Data\", Office of Aviation Research (2003).\n",
            "    \n",
            "    planck = <scipy.stats._discrete_distns.planck_gen object>\n",
            "        A Planck discrete exponential random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `planck` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(lambda_, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, lambda_, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, lambda_, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, lambda_, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, lambda_, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, lambda_, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, lambda_, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, lambda_, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, lambda_, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(lambda_, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(lambda_, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(lambda_,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(lambda_, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(lambda_, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(lambda_, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(lambda_, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, lambda_, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `planck` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k)\n",
            "        \n",
            "        for :math:`k \\ge 0` and :math:`\\lambda > 0`.\n",
            "        \n",
            "        `planck` takes :math:`\\lambda` as shape parameter. The Planck distribution\n",
            "        can be written as a geometric distribution (`geom`) with\n",
            "        :math:`p = 1 - \\exp(-\\lambda)` shifted by ``loc = -1``.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``planck.pmf(k, lambda_, loc)`` is identically\n",
            "        equivalent to ``planck.pmf(k - loc, lambda_)``.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        geom\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import planck\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> lambda_ = 0.51\n",
            "        >>> mean, var, skew, kurt = planck.stats(lambda_, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(planck.ppf(0.01, lambda_),\n",
            "        ...               planck.ppf(0.99, lambda_))\n",
            "        >>> ax.plot(x, planck.pmf(x, lambda_), 'bo', ms=8, label='planck pmf')\n",
            "        >>> ax.vlines(x, 0, planck.pmf(x, lambda_), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = planck(lambda_)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = planck.cdf(x, lambda_)\n",
            "        >>> np.allclose(x, planck.ppf(prob, lambda_))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = planck.rvs(lambda_, size=1000)\n",
            "    \n",
            "    poisson = <scipy.stats._discrete_distns.poisson_gen object>\n",
            "        A Poisson discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `poisson` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(mu, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, mu, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, mu, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, mu, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, mu, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, mu, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, mu, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, mu, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, mu, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(mu, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(mu, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(mu,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(mu, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(mu, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(mu, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(mu, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, mu, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `poisson` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = \\exp(-\\mu) \\frac{\\mu^k}{k!}\n",
            "        \n",
            "        for :math:`k \\ge 0`.\n",
            "        \n",
            "        `poisson` takes :math:`\\mu \\geq 0` as shape parameter.\n",
            "        When :math:`\\mu = 0`, the ``pmf`` method\n",
            "        returns ``1.0`` at quantile :math:`k = 0`.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``poisson.pmf(k, mu, loc)`` is identically\n",
            "        equivalent to ``poisson.pmf(k - loc, mu)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import poisson\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mu = 0.6\n",
            "        >>> mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(poisson.ppf(0.01, mu),\n",
            "        ...               poisson.ppf(0.99, mu))\n",
            "        >>> ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
            "        >>> ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = poisson(mu)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = poisson.cdf(x, mu)\n",
            "        >>> np.allclose(x, poisson.ppf(prob, mu))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = poisson.rvs(mu, size=1000)\n",
            "    \n",
            "    powerlaw = <scipy.stats._continuous_distns.powerlaw_gen object>\n",
            "        A power-function continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `powerlaw` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        pareto\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `powerlaw` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a) = a x^{a-1}\n",
            "        \n",
            "        for :math:`0 \\le x \\le 1`, :math:`a > 0`.\n",
            "        \n",
            "        `powerlaw` takes ``a`` as a shape parameter for :math:`a`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``powerlaw.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``powerlaw.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        For example, the support of `powerlaw` can be adjusted from the default\n",
            "        interval ``[0, 1]`` to the interval ``[c, c+d]`` by setting ``loc=c`` and\n",
            "        ``scale=d``. For a power-law distribution with infinite support, see\n",
            "        `pareto`.\n",
            "        \n",
            "        `powerlaw` is a special case of `beta` with ``b=1``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import powerlaw\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 0.659\n",
            "        >>> mean, var, skew, kurt = powerlaw.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(powerlaw.ppf(0.01, a),\n",
            "        ...                 powerlaw.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, powerlaw.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='powerlaw pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = powerlaw(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = powerlaw.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], powerlaw.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = powerlaw.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    powerlognorm = <scipy.stats._continuous_distns.powerlognorm_gen object...\n",
            "        A power log-normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `powerlognorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, s, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, s, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, s, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, s, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, s, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, s, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, s, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, s, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, s, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, s, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, s, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, s, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, s, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, s, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, s, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, s, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, s, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `powerlognorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c, s) = \\frac{c}{x s} \\phi(\\log(x)/s)\n",
            "                         (\\Phi(-\\log(x)/s))^{c-1}\n",
            "        \n",
            "        where :math:`\\phi` is the normal pdf, and :math:`\\Phi` is the normal cdf,\n",
            "        and :math:`x > 0`, :math:`s, c > 0`.\n",
            "        \n",
            "        `powerlognorm` takes :math:`c` and :math:`s` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``powerlognorm.pdf(x, c, s, loc, scale)`` is identically\n",
            "        equivalent to ``powerlognorm.pdf(y, c, s) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import powerlognorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c, s = 2.14, 0.446\n",
            "        >>> mean, var, skew, kurt = powerlognorm.stats(c, s, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(powerlognorm.ppf(0.01, c, s),\n",
            "        ...                 powerlognorm.ppf(0.99, c, s), 100)\n",
            "        >>> ax.plot(x, powerlognorm.pdf(x, c, s),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='powerlognorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = powerlognorm(c, s)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = powerlognorm.ppf([0.001, 0.5, 0.999], c, s)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], powerlognorm.cdf(vals, c, s))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = powerlognorm.rvs(c, s, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    powernorm = <scipy.stats._continuous_distns.powernorm_gen object>\n",
            "        A power normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `powernorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `powernorm` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c \\phi(x) (\\Phi(-x))^{c-1}\n",
            "        \n",
            "        where :math:`\\phi` is the normal pdf, :math:`\\Phi` is the normal cdf,\n",
            "        :math:`x` is any real, and :math:`c > 0` [1]_.\n",
            "        \n",
            "        `powernorm` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``powernorm.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``powernorm.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] NIST Engineering Statistics Handbook, Section 1.3.6.6.13,\n",
            "               https://www.itl.nist.gov/div898/handbook//eda/section3/eda366d.htm\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import powernorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 4.45\n",
            "        >>> mean, var, skew, kurt = powernorm.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(powernorm.ppf(0.01, c),\n",
            "        ...                 powernorm.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, powernorm.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='powernorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = powernorm(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = powernorm.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], powernorm.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = powernorm.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    randint = <scipy.stats._discrete_distns.randint_gen object>\n",
            "        A uniform discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `randint` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(low, high, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, low, high, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, low, high, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, low, high, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, low, high, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, low, high, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, low, high, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, low, high, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, low, high, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(low, high, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(low, high, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(low, high), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(low, high, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(low, high, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(low, high, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(low, high, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, low, high, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `randint` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) = \\frac{1}{\\texttt{high} - \\texttt{low}}\n",
            "        \n",
            "        for :math:`k \\in \\{\\texttt{low}, \\dots, \\texttt{high} - 1\\}`.\n",
            "        \n",
            "        `randint` takes :math:`\\texttt{low}` and :math:`\\texttt{high}` as shape\n",
            "        parameters.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``randint.pmf(k, low, high, loc)`` is identically\n",
            "        equivalent to ``randint.pmf(k - loc, low, high)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import randint\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> low, high = 7, 31\n",
            "        >>> mean, var, skew, kurt = randint.stats(low, high, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(low - 5, high + 5)\n",
            "        >>> ax.plot(x, randint.pmf(x, low, high), 'bo', ms=8, label='randint pmf')\n",
            "        >>> ax.vlines(x, 0, randint.pmf(x, low, high), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function) to \n",
            "        fix the shape and location. This returns a \"frozen\" RV object holding the\n",
            "        given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = randint(low, high)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-',\n",
            "        ...           lw=1, label='frozen pmf')\n",
            "        >>> ax.legend(loc='lower center')\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check the relationship between the cumulative distribution function\n",
            "        (``cdf``) and its inverse, the percent point function (``ppf``):\n",
            "        \n",
            "        >>> q = np.arange(low, high)\n",
            "        >>> p = randint.cdf(q, low, high)\n",
            "        >>> np.allclose(q, randint.ppf(p, low, high))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = randint.rvs(low, high, size=1000)\n",
            "    \n",
            "    random_correlation = <scipy.stats._multivariate.random_correlation_gen...\n",
            "    random_table = <scipy.stats._multivariate.random_table_gen object>\n",
            "    rayleigh = <scipy.stats._continuous_distns.rayleigh_gen object>\n",
            "        A Rayleigh continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `rayleigh` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `rayleigh` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = x \\exp(-x^2/2)\n",
            "        \n",
            "        for :math:`x \\ge 0`.\n",
            "        \n",
            "        `rayleigh` is a special case of `chi` with ``df=2``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``rayleigh.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``rayleigh.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import rayleigh\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = rayleigh.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(rayleigh.ppf(0.01),\n",
            "        ...                 rayleigh.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, rayleigh.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='rayleigh pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = rayleigh()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = rayleigh.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], rayleigh.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = rayleigh.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    rdist = <scipy.stats._continuous_distns.rdist_gen object>\n",
            "        An R-distributed (symmetric beta) continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `rdist` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `rdist` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}\n",
            "        \n",
            "        for :math:`-1 \\le x \\le 1`, :math:`c > 0`. `rdist` is also called the\n",
            "        symmetric beta distribution: if B has a `beta` distribution with\n",
            "        parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with\n",
            "        parameter c.\n",
            "        \n",
            "        `rdist` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        This distribution includes the following distribution kernels as\n",
            "        special cases::\n",
            "        \n",
            "            c = 2:  uniform\n",
            "            c = 3:  `semicircular`\n",
            "            c = 4:  Epanechnikov (parabolic)\n",
            "            c = 6:  quartic (biweight)\n",
            "            c = 8:  triweight\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``rdist.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``rdist.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import rdist\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 1.6\n",
            "        >>> mean, var, skew, kurt = rdist.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(rdist.ppf(0.01, c),\n",
            "        ...                 rdist.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, rdist.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='rdist pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = rdist(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = rdist.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], rdist.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = rdist.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    recipinvgauss = <scipy.stats._continuous_distns.recipinvgauss_gen obje...\n",
            "        A reciprocal inverse Gaussian continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `recipinvgauss` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(mu, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, mu, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, mu, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, mu, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, mu, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, mu, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, mu, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, mu, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, mu, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, mu, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(mu, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(mu, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(mu, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(mu, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(mu, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(mu, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, mu, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `recipinvgauss` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\mu) = \\frac{1}{\\sqrt{2\\pi x}}\n",
            "                        \\exp\\left(\\frac{-(1-\\mu x)^2}{2\\mu^2x}\\right)\n",
            "        \n",
            "        for :math:`x \\ge 0`.\n",
            "        \n",
            "        `recipinvgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``recipinvgauss.pdf(x, mu, loc, scale)`` is identically\n",
            "        equivalent to ``recipinvgauss.pdf(y, mu) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import recipinvgauss\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mu = 0.63\n",
            "        >>> mean, var, skew, kurt = recipinvgauss.stats(mu, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(recipinvgauss.ppf(0.01, mu),\n",
            "        ...                 recipinvgauss.ppf(0.99, mu), 100)\n",
            "        >>> ax.plot(x, recipinvgauss.pdf(x, mu),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='recipinvgauss pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = recipinvgauss(mu)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = recipinvgauss.ppf([0.001, 0.5, 0.999], mu)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], recipinvgauss.cdf(vals, mu))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = recipinvgauss.rvs(mu, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    reciprocal = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
            "        A loguniform or reciprocal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `reciprocal` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for this class is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b) = \\frac{1}{x \\log(b/a)}\n",
            "        \n",
            "        for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n",
            "        :math:`a` and :math:`b` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``reciprocal.pdf(x, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``reciprocal.pdf(y, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import reciprocal\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 0.01, 1.25\n",
            "        >>> mean, var, skew, kurt = reciprocal.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(reciprocal.ppf(0.01, a, b),\n",
            "        ...                 reciprocal.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, reciprocal.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='reciprocal pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = reciprocal(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = reciprocal.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], reciprocal.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = reciprocal.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        This doesn't show the equal probability of ``0.01``, ``0.1`` and\n",
            "        ``1``. This is best when the x-axis is log-scaled:\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.hist(np.log10(r))\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> ax.set_xlabel(\"Value of random variable\")\n",
            "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
            "        >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
            "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "        \n",
            "        This random variable will be log-uniform regardless of the base chosen for\n",
            "        ``a`` and ``b``. Let's specify with base ``2`` instead:\n",
            "        \n",
            "        >>> rvs = reciprocal(2**-2, 2**0).rvs(size=1000)\n",
            "        \n",
            "        Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n",
            "        variable.  Here's the histogram:\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.hist(np.log2(rvs))\n",
            "        >>> ax.set_ylabel(\"Frequency\")\n",
            "        >>> ax.set_xlabel(\"Value of random variable\")\n",
            "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
            "        >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
            "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
            "        >>> plt.show()\n",
            "    \n",
            "    rel_breitwigner = <scipy.stats._continuous_distns.rel_breitwigner_gen ...\n",
            "        A relativistic Breit-Wigner random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `rel_breitwigner` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(rho, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, rho, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, rho, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, rho, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, rho, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, rho, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, rho, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, rho, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, rho, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, rho, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(rho, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(rho, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(rho,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(rho, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(rho, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(rho, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(rho, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, rho, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        cauchy: Cauchy distribution, also known as the Breit-Wigner distribution.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        The probability density function for `rel_breitwigner` is\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\rho) = \\frac{k}{(x^2 - \\rho^2)^2 + \\rho^2}\n",
            "        \n",
            "        where\n",
            "        \n",
            "        .. math::\n",
            "            k = \\frac{2\\sqrt{2}\\rho^2\\sqrt{\\rho^2 + 1}}\n",
            "                {\\pi\\sqrt{\\rho^2 + \\rho\\sqrt{\\rho^2 + 1}}}\n",
            "        \n",
            "        The relativistic Breit-Wigner distribution is used in high energy physics\n",
            "        to model resonances [1]_. It gives the uncertainty in the invariant mass,\n",
            "        :math:`M` [2]_, of a resonance with characteristic mass :math:`M_0` and\n",
            "        decay-width :math:`\\Gamma`, where :math:`M`, :math:`M_0` and :math:`\\Gamma`\n",
            "        are expressed in natural units. In SciPy's parametrization, the shape\n",
            "        parameter :math:`\\rho` is equal to :math:`M_0/\\Gamma` and takes values in\n",
            "        :math:`(0, \\infty)`.\n",
            "        \n",
            "        Equivalently, the relativistic Breit-Wigner distribution is said to give\n",
            "        the uncertainty in the center-of-mass energy :math:`E_{\\text{cm}}`. In\n",
            "        natural units, the speed of light :math:`c` is equal to 1 and the invariant\n",
            "        mass :math:`M` is equal to the rest energy :math:`Mc^2`. In the\n",
            "        center-of-mass frame, the rest energy is equal to the total energy [3]_.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``rel_breitwigner.pdf(x, rho, loc, scale)`` is identically\n",
            "        equivalent to ``rel_breitwigner.pdf(y, rho) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        :math:`\\rho = M/\\Gamma` and :math:`\\Gamma` is the scale parameter. For\n",
            "        example, if one seeks to model the :math:`Z^0` boson with :math:`M_0\n",
            "        \\approx 91.1876 \\text{ GeV}` and :math:`\\Gamma \\approx 2.4952\\text{ GeV}`\n",
            "        [4]_ one can set ``rho=91.1876/2.4952`` and ``scale=2.4952``.\n",
            "        \n",
            "        To ensure a physically meaningful result when using the `fit` method, one\n",
            "        should set ``floc=0`` to fix the location parameter to 0.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Relativistic Breit-Wigner distribution, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Relativistic_Breit-Wigner_distribution\n",
            "        .. [2] Invariant mass, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Invariant_mass\n",
            "        .. [3] Center-of-momentum frame, Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Center-of-momentum_frame\n",
            "        .. [4] M. Tanabashi et al. (Particle Data Group) Phys. Rev. D 98, 030001 -\n",
            "               Published 17 August 2018\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import rel_breitwigner\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> rho = 36.5\n",
            "        >>> mean, var, skew, kurt = rel_breitwigner.stats(rho, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(rel_breitwigner.ppf(0.01, rho),\n",
            "        ...                 rel_breitwigner.ppf(0.99, rho), 100)\n",
            "        >>> ax.plot(x, rel_breitwigner.pdf(x, rho),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='rel_breitwigner pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = rel_breitwigner(rho)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = rel_breitwigner.ppf([0.001, 0.5, 0.999], rho)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], rel_breitwigner.cdf(vals, rho))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = rel_breitwigner.rvs(rho, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    rice = <scipy.stats._continuous_distns.rice_gen object>\n",
            "        A Rice continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `rice` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `rice` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, b) = x \\exp(- \\frac{x^2 + b^2}{2}) I_0(x b)\n",
            "        \n",
            "        for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel\n",
            "        function of order zero (`scipy.special.i0`).\n",
            "        \n",
            "        `rice` takes ``b`` as a shape parameter for :math:`b`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``rice.pdf(x, b, loc, scale)`` is identically\n",
            "        equivalent to ``rice.pdf(y, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The Rice distribution describes the length, :math:`r`, of a 2-D vector with\n",
            "        components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,\n",
            "        v` are independent Gaussian random variables with standard deviation\n",
            "        :math:`s`.  Let :math:`R = \\sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is\n",
            "        ``rice.pdf(x, R/s, scale=s)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import rice\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> b = 0.775\n",
            "        >>> mean, var, skew, kurt = rice.stats(b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(rice.ppf(0.01, b),\n",
            "        ...                 rice.ppf(0.99, b), 100)\n",
            "        >>> ax.plot(x, rice.pdf(x, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='rice pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = rice(b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = rice.ppf([0.001, 0.5, 0.999], b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], rice.cdf(vals, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = rice.rvs(b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    semicircular = <scipy.stats._continuous_distns.semicircular_gen object...\n",
            "        A semicircular continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `semicircular` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        rdist\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `semicircular` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{2}{\\pi} \\sqrt{1-x^2}\n",
            "        \n",
            "        for :math:`-1 \\le x \\le 1`.\n",
            "        \n",
            "        The distribution is a special case of `rdist` with `c = 3`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``semicircular.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``semicircular.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Wigner semicircle distribution\",\n",
            "               https://en.wikipedia.org/wiki/Wigner_semicircle_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import semicircular\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = semicircular.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(semicircular.ppf(0.01),\n",
            "        ...                 semicircular.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, semicircular.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='semicircular pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = semicircular()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = semicircular.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], semicircular.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = semicircular.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    skellam = <scipy.stats._discrete_distns.skellam_gen object>\n",
            "        A  Skellam discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `skellam` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(mu1, mu2, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, mu1, mu2, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, mu1, mu2, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, mu1, mu2, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, mu1, mu2, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, mu1, mu2, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, mu1, mu2, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, mu1, mu2, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, mu1, mu2, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(mu1, mu2, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(mu1, mu2, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(mu1, mu2), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(mu1, mu2, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(mu1, mu2, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(mu1, mu2, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(mu1, mu2, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, mu1, mu2, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        Probability distribution of the difference of two correlated or\n",
            "        uncorrelated Poisson random variables.\n",
            "        \n",
            "        Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with\n",
            "        expected values :math:`\\lambda_1` and :math:`\\lambda_2`. Then,\n",
            "        :math:`k_1 - k_2` follows a Skellam distribution with parameters\n",
            "        :math:`\\mu_1 = \\lambda_1 - \\rho \\sqrt{\\lambda_1 \\lambda_2}` and\n",
            "        :math:`\\mu_2 = \\lambda_2 - \\rho \\sqrt{\\lambda_1 \\lambda_2}`, where\n",
            "        :math:`\\rho` is the correlation coefficient between :math:`k_1` and\n",
            "        :math:`k_2`. If the two Poisson-distributed r.v. are independent then\n",
            "        :math:`\\rho = 0`.\n",
            "        \n",
            "        Parameters :math:`\\mu_1` and :math:`\\mu_2` must be strictly positive.\n",
            "        \n",
            "        For details see: https://en.wikipedia.org/wiki/Skellam_distribution\n",
            "        \n",
            "        `skellam` takes :math:`\\mu_1` and :math:`\\mu_2` as shape parameters.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``skellam.pmf(k, mu1, mu2, loc)`` is identically\n",
            "        equivalent to ``skellam.pmf(k - loc, mu1, mu2)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import skellam\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> mu1, mu2 = 15, 8\n",
            "        >>> mean, var, skew, kurt = skellam.stats(mu1, mu2, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(skellam.ppf(0.01, mu1, mu2),\n",
            "        ...               skellam.ppf(0.99, mu1, mu2))\n",
            "        >>> ax.plot(x, skellam.pmf(x, mu1, mu2), 'bo', ms=8, label='skellam pmf')\n",
            "        >>> ax.vlines(x, 0, skellam.pmf(x, mu1, mu2), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = skellam(mu1, mu2)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = skellam.cdf(x, mu1, mu2)\n",
            "        >>> np.allclose(x, skellam.ppf(prob, mu1, mu2))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = skellam.rvs(mu1, mu2, size=1000)\n",
            "    \n",
            "    skewcauchy = <scipy.stats._continuous_distns.skewcauchy_gen object>\n",
            "        A skewed Cauchy random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `skewcauchy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        cauchy : Cauchy distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        The probability density function for `skewcauchy` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\pi \\left(\\frac{x^2}{\\left(a\\, \\text{sign}(x) + 1\n",
            "                                                       \\right)^2} + 1 \\right)}\n",
            "        \n",
            "        for a real number :math:`x` and skewness parameter :math:`-1 < a < 1`.\n",
            "        \n",
            "        When :math:`a=0`, the distribution reduces to the usual Cauchy\n",
            "        distribution.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``skewcauchy.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``skewcauchy.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Skewed generalized *t* distribution\", Wikipedia\n",
            "           https://en.wikipedia.org/wiki/Skewed_generalized_t_distribution#Skewed_Cauchy_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import skewcauchy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 0.5\n",
            "        >>> mean, var, skew, kurt = skewcauchy.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(skewcauchy.ppf(0.01, a),\n",
            "        ...                 skewcauchy.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, skewcauchy.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='skewcauchy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = skewcauchy(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = skewcauchy.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], skewcauchy.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = skewcauchy.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    skewnorm = <scipy.stats._continuous_distns.skewnorm_gen object>\n",
            "        A skew-normal random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `skewnorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The pdf is::\n",
            "        \n",
            "            skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)\n",
            "        \n",
            "        `skewnorm` takes a real number :math:`a` as a skewness parameter\n",
            "        When ``a = 0`` the distribution is identical to a normal distribution\n",
            "        (`norm`). `rvs` implements the method of [1]_.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``skewnorm.pdf(x, a, loc, scale)`` is identically\n",
            "        equivalent to ``skewnorm.pdf(y, a) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import skewnorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 4\n",
            "        >>> mean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(skewnorm.ppf(0.01, a),\n",
            "        ...                 skewnorm.ppf(0.99, a), 100)\n",
            "        >>> ax.plot(x, skewnorm.pdf(x, a),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='skewnorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = skewnorm(a)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = skewnorm.ppf([0.001, 0.5, 0.999], a)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], skewnorm.cdf(vals, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = skewnorm.rvs(a, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of\n",
            "            the multivariate skew-normal distribution. J. Roy. Statist. Soc.,\n",
            "            B 61, 579-602. :arxiv:`0911.2093`\n",
            "    \n",
            "    special_ortho_group = <scipy.stats._multivariate.special_ortho_group_g...\n",
            "    studentized_range = <scipy.stats._continuous_distns.studentized_range_...\n",
            "        A studentized range continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `studentized_range` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(k, df, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, k, df, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, k, df, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, k, df, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, k, df, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, k, df, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, k, df, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, k, df, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, k, df, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, k, df, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(k, df, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(k, df, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(k, df), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(k, df, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(k, df, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(k, df, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(k, df, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, k, df, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        t: Student's t distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `studentized_range` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "             f(x; k, \\nu) = \\frac{k(k-1)\\nu^{\\nu/2}}{\\Gamma(\\nu/2)\n",
            "                            2^{\\nu/2-1}} \\int_{0}^{\\infty} \\int_{-\\infty}^{\\infty}\n",
            "                            s^{\\nu} e^{-\\nu s^2/2} \\phi(z) \\phi(sx + z)\n",
            "                            [\\Phi(sx + z) - \\Phi(z)]^{k-2} \\,dz \\,ds\n",
            "        \n",
            "        for :math:`x  0`, :math:`k > 1`, and :math:`\\nu > 0`.\n",
            "        \n",
            "        `studentized_range` takes ``k`` for :math:`k` and ``df`` for :math:`\\nu`\n",
            "        as shape parameters.\n",
            "        \n",
            "        When :math:`\\nu` exceeds 100,000, an asymptotic approximation (infinite\n",
            "        degrees of freedom) is used to compute the cumulative distribution\n",
            "        function [4]_ and probability distribution function.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``studentized_range.pdf(x, k, df, loc, scale)`` is identically\n",
            "        equivalent to ``studentized_range.pdf(y, k, df) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] \"Studentized range distribution\",\n",
            "               https://en.wikipedia.org/wiki/Studentized_range_distribution\n",
            "        .. [2] Batista, Ben Divide, et al. \"Externally Studentized Normal Midrange\n",
            "               Distribution.\" Cincia e Agrotecnologia, vol. 41, no. 4, 2017, pp.\n",
            "               378-389., doi:10.1590/1413-70542017414047716.\n",
            "        .. [3] Harter, H. Leon. \"Tables of Range and Studentized Range.\" The Annals\n",
            "               of Mathematical Statistics, vol. 31, no. 4, 1960, pp. 1122-1147.\n",
            "               JSTOR, www.jstor.org/stable/2237810. Accessed 18 Feb. 2021.\n",
            "        .. [4] Lund, R. E., and J. R. Lund. \"Algorithm AS 190: Probabilities and\n",
            "               Upper Quantiles for the Studentized Range.\" Journal of the Royal\n",
            "               Statistical Society. Series C (Applied Statistics), vol. 32, no. 2,\n",
            "               1983, pp. 204-210. JSTOR, www.jstor.org/stable/2347300. Accessed 18\n",
            "               Feb. 2021.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import studentized_range\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> k, df = 3, 10\n",
            "        >>> mean, var, skew, kurt = studentized_range.stats(k, df, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(studentized_range.ppf(0.01, k, df),\n",
            "        ...                 studentized_range.ppf(0.99, k, df), 100)\n",
            "        >>> ax.plot(x, studentized_range.pdf(x, k, df),\n",
            "        ...         'r-', lw=5, alpha=0.6, label='studentized_range pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = studentized_range(k, df)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = studentized_range.ppf([0.001, 0.5, 0.999], k, df)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], studentized_range.cdf(vals, k, df))\n",
            "        True\n",
            "        \n",
            "        Rather than using (``studentized_range.rvs``) to generate random variates,\n",
            "        which is very slow for this distribution, we can approximate the inverse\n",
            "        CDF using an interpolator, and then perform inverse transform sampling\n",
            "        with this approximate inverse CDF.\n",
            "        \n",
            "        This distribution has an infinite but thin right tail, so we focus our\n",
            "        attention on the leftmost 99.9 percent.\n",
            "        \n",
            "        >>> a, b = studentized_range.ppf([0, .999], k, df)\n",
            "        >>> a, b\n",
            "        0, 7.41058083802274\n",
            "        \n",
            "        >>> from scipy.interpolate import interp1d\n",
            "        >>> rng = np.random.default_rng()\n",
            "        >>> xs = np.linspace(a, b, 50)\n",
            "        >>> cdf = studentized_range.cdf(xs, k, df)\n",
            "        # Create an interpolant of the inverse CDF\n",
            "        >>> ppf = interp1d(cdf, xs, fill_value='extrapolate')\n",
            "        # Perform inverse transform sampling using the interpolant\n",
            "        >>> r = ppf(rng.uniform(size=1000))\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    t = <scipy.stats._continuous_distns.t_gen object>\n",
            "        A Student's t continuous random variable.\n",
            "        \n",
            "        For the noncentral t distribution, see `nct`.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `t` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, df, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, df, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, df, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, df, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, df, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, df, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, df, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, df, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(df, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(df, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(df, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(df, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(df, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(df, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, df, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        nct\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `t` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n",
            "                            {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n",
            "                        (1+x^2/\\nu)^{-(\\nu+1)/2}\n",
            "        \n",
            "        where :math:`x` is a real number and the degrees of freedom parameter\n",
            "        :math:`\\nu` (denoted ``df`` in the implementation) satisfies\n",
            "        :math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n",
            "        (`scipy.special.gamma`).\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``t.pdf(x, df, loc, scale)`` is identically\n",
            "        equivalent to ``t.pdf(y, df) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import t\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> df = 2.74\n",
            "        >>> mean, var, skew, kurt = t.stats(df, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(t.ppf(0.01, df),\n",
            "        ...                 t.ppf(0.99, df), 100)\n",
            "        >>> ax.plot(x, t.pdf(x, df),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='t pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = t(df)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = t.ppf([0.001, 0.5, 0.999], df)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = t.rvs(df, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    trapezoid = <scipy.stats._continuous_distns.trapezoid_gen object>\n",
            "        A trapezoidal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `trapezoid` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, d, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, d, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, d, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, d, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, d, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, d, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, d, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, d, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, d, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, d, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, d, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, d, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, d, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, d, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, d, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The trapezoidal distribution can be represented with an up-sloping line\n",
            "        from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``\n",
            "        and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.  This\n",
            "        defines the trapezoid base from ``loc`` to ``(loc+scale)`` and the flat\n",
            "        top from ``c`` to ``d`` proportional to the position along the base\n",
            "        with ``0 <= c <= d <= 1``.  When ``c=d``, this is equivalent to `triang`\n",
            "        with the same values for `loc`, `scale` and `c`.\n",
            "        The method of [1]_ is used for computing moments.\n",
            "        \n",
            "        `trapezoid` takes :math:`c` and :math:`d` as shape parameters.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``trapezoid.pdf(x, c, d, loc, scale)`` is identically\n",
            "        equivalent to ``trapezoid.pdf(y, c, d) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The standard form is in the range [0, 1] with c the mode.\n",
            "        The location parameter shifts the start to `loc`.\n",
            "        The scale parameter changes the width from 1 to `scale`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import trapezoid\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c, d = 0.2, 0.8\n",
            "        >>> mean, var, skew, kurt = trapezoid.stats(c, d, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(trapezoid.ppf(0.01, c, d),\n",
            "        ...                 trapezoid.ppf(0.99, c, d), 100)\n",
            "        >>> ax.plot(x, trapezoid.pdf(x, c, d),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='trapezoid pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = trapezoid(c, d)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = trapezoid.ppf([0.001, 0.5, 0.999], c, d)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], trapezoid.cdf(vals, c, d))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = trapezoid.rvs(c, d, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Kacker, R.N. and Lawrence, J.F. (2007). Trapezoidal and triangular\n",
            "           distributions for Type B evaluation of standard uncertainty.\n",
            "           Metrologia 44, 117-127. :doi:`10.1088/0026-1394/44/2/003`\n",
            "    \n",
            "    trapz = <scipy.stats._continuous_distns.trapezoid_gen object>\n",
            "        trapz is an alias for `trapezoid`\n",
            "    \n",
            "    triang = <scipy.stats._continuous_distns.triang_gen object>\n",
            "        A triangular continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `triang` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The triangular distribution can be represented with an up-sloping line from\n",
            "        ``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``\n",
            "        to ``(loc + scale)``.\n",
            "        \n",
            "        `triang` takes ``c`` as a shape parameter for :math:`0 \\le c \\le 1`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``triang.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``triang.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        The standard form is in the range [0, 1] with c the mode.\n",
            "        The location parameter shifts the start to `loc`.\n",
            "        The scale parameter changes the width from 1 to `scale`.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import triang\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.158\n",
            "        >>> mean, var, skew, kurt = triang.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(triang.ppf(0.01, c),\n",
            "        ...                 triang.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, triang.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='triang pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = triang(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = triang.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], triang.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = triang.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    truncexpon = <scipy.stats._continuous_distns.truncexpon_gen object>\n",
            "        A truncated exponential continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `truncexpon` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `truncexpon` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, b) = \\frac{\\exp(-x)}{1 - \\exp(-b)}\n",
            "        \n",
            "        for :math:`0 <= x <= b`.\n",
            "        \n",
            "        `truncexpon` takes ``b`` as a shape parameter for :math:`b`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``truncexpon.pdf(x, b, loc, scale)`` is identically\n",
            "        equivalent to ``truncexpon.pdf(y, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import truncexpon\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> b = 4.69\n",
            "        >>> mean, var, skew, kurt = truncexpon.stats(b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(truncexpon.ppf(0.01, b),\n",
            "        ...                 truncexpon.ppf(0.99, b), 100)\n",
            "        >>> ax.plot(x, truncexpon.pdf(x, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='truncexpon pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = truncexpon(b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = truncexpon.ppf([0.001, 0.5, 0.999], b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], truncexpon.cdf(vals, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = truncexpon.rvs(b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    truncnorm = <scipy.stats._continuous_distns.truncnorm_gen object>\n",
            "        A truncated normal continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `truncnorm` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        This distribution is the normal distribution centered on ``loc`` (default\n",
            "        0), with standard deviation ``scale`` (default 1), and truncated at ``a``\n",
            "        and ``b`` *standard deviations* from ``loc``. For arbitrary ``loc`` and\n",
            "        ``scale``, ``a`` and ``b`` are *not* the abscissae at which the shifted\n",
            "        and scaled distribution is truncated.\n",
            "        \n",
            "        .. note::\n",
            "            If ``a_trunc`` and ``b_trunc`` are the abscissae at which we wish\n",
            "            to truncate the distribution (as opposed to the number of standard\n",
            "            deviations from ``loc``), then we can calculate the distribution\n",
            "            parameters ``a`` and ``b`` as follows::\n",
            "        \n",
            "                a, b = (a_trunc - loc) / scale, (b_trunc - loc) / scale\n",
            "        \n",
            "            This is a common point of confusion. For additional clarification,\n",
            "            please see the example below.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import truncnorm\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, b = 0.1, 2\n",
            "        >>> mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n",
            "        ...                 truncnorm.ppf(0.99, a, b), 100)\n",
            "        >>> ax.plot(x, truncnorm.pdf(x, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='truncnorm pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = truncnorm(a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = truncnorm.ppf([0.001, 0.5, 0.999], a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], truncnorm.cdf(vals, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = truncnorm.rvs(a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        \n",
            "        In the examples above, ``loc=0`` and ``scale=1``, so the plot is truncated\n",
            "        at ``a`` on the left and ``b`` on the right. However, suppose we were to\n",
            "        produce the same histogram with ``loc = 1`` and ``scale=0.5``.\n",
            "        \n",
            "        >>> loc, scale = 1, 0.5\n",
            "        >>> rv = truncnorm(a, b, loc=loc, scale=scale)\n",
            "        >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n",
            "        ...                 truncnorm.ppf(0.99, a, b), 100)\n",
            "        >>> r = rv.rvs(size=1000)\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim(a, b)\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Note that the distribution is no longer appears to be truncated at\n",
            "        abscissae ``a`` and ``b``. That is because the *standard* normal\n",
            "        distribution is first truncated at ``a`` and ``b``, *then* the resulting\n",
            "        distribution is scaled by ``scale`` and shifted by ``loc``. If we instead\n",
            "        want the shifted and scaled distribution to be truncated at ``a`` and\n",
            "        ``b``, we need to transform these values before passing them as the\n",
            "        distribution parameters.\n",
            "        \n",
            "        >>> a_transformed, b_transformed = (a - loc) / scale, (b - loc) / scale\n",
            "        >>> rv = truncnorm(a_transformed, b_transformed, loc=loc, scale=scale)\n",
            "        >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n",
            "        ...                 truncnorm.ppf(0.99, a, b), 100)\n",
            "        >>> r = rv.rvs(size=10000)\n",
            "        \n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim(a-0.1, b+0.1)\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    truncpareto = <scipy.stats._continuous_distns.truncpareto_gen object>\n",
            "        An upper truncated Pareto continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `truncpareto` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(b, c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, b, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, b, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, b, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, b, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, b, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, b, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, b, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, b, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, b, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(b, c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(b, c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(b, c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(b, c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(b, c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(b, c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, b, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        pareto : Pareto distribution\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `truncpareto` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, b, c) = \\frac{b}{1 - c^{-b}} \\frac{1}{x^{b+1}}\n",
            "        \n",
            "        for :math:`b > 0`, :math:`c > 1` and :math:`1 \\le x \\le c`.\n",
            "        \n",
            "        `truncpareto` takes `b` and `c` as shape parameters for :math:`b` and\n",
            "        :math:`c`.\n",
            "        \n",
            "        Notice that the upper truncation value :math:`c` is defined in\n",
            "        standardized form so that random values of an unscaled, unshifted variable\n",
            "        are within the range ``[1, c]``.\n",
            "        If ``u_r`` is the upper bound to a scaled and/or shifted variable,\n",
            "        then ``c = (u_r - loc) / scale``. In other words, the support of the\n",
            "        distribution becomes ``(scale + loc) <= x <= (c*scale + loc)`` when\n",
            "        `scale` and/or `loc` are provided.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``truncpareto.pdf(x, b, c, loc, scale)`` is identically\n",
            "        equivalent to ``truncpareto.pdf(y, b, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Burroughs, S. M., and Tebbens S. F.\n",
            "            \"Upper-truncated power laws in natural systems.\"\n",
            "            Pure and Applied Geophysics 158.4 (2001): 741-757.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import truncpareto\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> b, c = 2, 5\n",
            "        >>> mean, var, skew, kurt = truncpareto.stats(b, c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(truncpareto.ppf(0.01, b, c),\n",
            "        ...                 truncpareto.ppf(0.99, b, c), 100)\n",
            "        >>> ax.plot(x, truncpareto.pdf(x, b, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='truncpareto pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = truncpareto(b, c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = truncpareto.ppf([0.001, 0.5, 0.999], b, c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], truncpareto.cdf(vals, b, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = truncpareto.rvs(b, c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    truncweibull_min = <scipy.stats._continuous_distns.truncweibull_min_ge...\n",
            "        A doubly truncated Weibull minimum continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `truncweibull_min` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, a, b, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, a, b, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, a, b, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, a, b, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, a, b, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, a, b, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, a, b, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, a, b, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, a, b, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, a, b, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, a, b, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, a, b, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c, a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, a, b, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, a, b, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, a, b, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, a, b, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, a, b, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        weibull_min, truncexpon\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `truncweibull_min` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, a, b, c) = \\frac{c x^{c-1} \\exp(-x^c)}{\\exp(-a^c) - \\exp(-b^c)}\n",
            "        \n",
            "        for :math:`a < x <= b`, :math:`0 \\le a < b` and :math:`c > 0`.\n",
            "        \n",
            "        `truncweibull_min` takes :math:`a`, :math:`b`, and :math:`c` as shape\n",
            "        parameters.\n",
            "        \n",
            "        Notice that the truncation values, :math:`a` and :math:`b`, are defined in\n",
            "        standardized form:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            a = (u_l - loc)/scale\n",
            "            b = (u_r - loc)/scale\n",
            "        \n",
            "        where :math:`u_l` and :math:`u_r` are the specific left and right\n",
            "        truncation values, respectively. In other words, the support of the\n",
            "        distribution becomes :math:`(a*scale + loc) < x <= (b*scale + loc)` when\n",
            "        :math:`loc` and/or :math:`scale` are provided.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``truncweibull_min.pdf(x, c, a, b, loc, scale)`` is identically\n",
            "        equivalent to ``truncweibull_min.pdf(y, c, a, b) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        \n",
            "        .. [1] Rinne, H. \"The Weibull Distribution: A Handbook\". CRC Press (2009).\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import truncweibull_min\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c, a, b = 2.5, 0.25, 1.75\n",
            "        >>> mean, var, skew, kurt = truncweibull_min.stats(c, a, b, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(truncweibull_min.ppf(0.01, c, a, b),\n",
            "        ...                 truncweibull_min.ppf(0.99, c, a, b), 100)\n",
            "        >>> ax.plot(x, truncweibull_min.pdf(x, c, a, b),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='truncweibull_min pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = truncweibull_min(c, a, b)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = truncweibull_min.ppf([0.001, 0.5, 0.999], c, a, b)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], truncweibull_min.cdf(vals, c, a, b))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = truncweibull_min.rvs(c, a, b, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    tukeylambda = <scipy.stats._continuous_distns.tukeylambda_gen object>\n",
            "        A Tukey-Lamdba continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `tukeylambda` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(lam, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, lam, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, lam, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, lam, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, lam, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, lam, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, lam, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, lam, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, lam, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, lam, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(lam, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(lam, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(lam,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(lam, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(lam, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(lam, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(lam, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, lam, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        A flexible distribution, able to represent and interpolate between the\n",
            "        following distributions:\n",
            "        \n",
            "        - Cauchy                (:math:`lambda = -1`)\n",
            "        - logistic              (:math:`lambda = 0`)\n",
            "        - approx Normal         (:math:`lambda = 0.14`)\n",
            "        - uniform from -1 to 1  (:math:`lambda = 1`)\n",
            "        \n",
            "        `tukeylambda` takes a real number :math:`lambda` (denoted ``lam``\n",
            "        in the implementation) as a shape parameter.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``tukeylambda.pdf(x, lam, loc, scale)`` is identically\n",
            "        equivalent to ``tukeylambda.pdf(y, lam) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import tukeylambda\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> lam = 3.13\n",
            "        >>> mean, var, skew, kurt = tukeylambda.stats(lam, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(tukeylambda.ppf(0.01, lam),\n",
            "        ...                 tukeylambda.ppf(0.99, lam), 100)\n",
            "        >>> ax.plot(x, tukeylambda.pdf(x, lam),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='tukeylambda pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = tukeylambda(lam)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = tukeylambda.ppf([0.001, 0.5, 0.999], lam)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], tukeylambda.cdf(vals, lam))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = tukeylambda.rvs(lam, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    uniform = <scipy.stats._continuous_distns.uniform_gen object>\n",
            "        A uniform continuous random variable.\n",
            "        \n",
            "        In the standard form, the distribution is uniform on ``[0, 1]``. Using\n",
            "        the parameters ``loc`` and ``scale``, one obtains the uniform distribution\n",
            "        on ``[loc, loc + scale]``.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `uniform` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import uniform\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = uniform.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(uniform.ppf(0.01),\n",
            "        ...                 uniform.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, uniform.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='uniform pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = uniform()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = uniform.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], uniform.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = uniform.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    uniform_direction = <scipy.stats._multivariate.uniform_direction_gen o...\n",
            "    unitary_group = <scipy.stats._multivariate.unitary_group_gen object>\n",
            "    vonmises = <scipy.stats._continuous_distns.vonmises_gen object>\n",
            "        A Von Mises continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `vonmises` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, kappa, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, kappa, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, kappa, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, kappa, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, kappa, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, kappa, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, kappa, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(kappa, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(kappa, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(kappa, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(kappa, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(kappa, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(kappa, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, kappa, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.vonmises_fisher : Von-Mises Fisher distribution on a\n",
            "                                      hypersphere\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `vonmises` and `vonmises_line` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n",
            "        \n",
            "        for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa \\ge 0`. :math:`I_0` is the\n",
            "        modified Bessel function of order zero (`scipy.special.i0`).\n",
            "        \n",
            "        `vonmises` is a circular distribution which does not restrict the\n",
            "        distribution to a fixed interval. Currently, there is no circular\n",
            "        distribution framework in SciPy. The ``cdf`` is implemented such that\n",
            "        ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n",
            "        \n",
            "        `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n",
            "        on the real line. This is a regular (i.e. non-circular) distribution.\n",
            "        \n",
            "        Note about distribution parameters: `vonmises` and `vonmises_line` take\n",
            "        ``kappa`` as a shape parameter (concentration) and ``loc`` as the location\n",
            "        (circular mean). A ``scale`` parameter is accepted but does not have any\n",
            "        effect.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Import the necessary modules.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import vonmises\n",
            "        \n",
            "        Define distribution parameters.\n",
            "        \n",
            "        >>> loc = 0.5 * np.pi  # circular mean\n",
            "        >>> kappa = 1  # concentration\n",
            "        \n",
            "        Compute the probability density at ``x=0`` via the ``pdf`` method.\n",
            "        \n",
            "        >>> vonmises.pdf(0, loc=loc, kappa=kappa)\n",
            "        0.12570826359722018\n",
            "        \n",
            "        Verify that the percentile function ``ppf`` inverts the cumulative\n",
            "        distribution function ``cdf`` up to floating point accuracy.\n",
            "        \n",
            "        >>> x = 1\n",
            "        >>> cdf_value = vonmises.cdf(x, loc=loc, kappa=kappa)\n",
            "        >>> ppf_value = vonmises.ppf(cdf_value, loc=loc, kappa=kappa)\n",
            "        >>> x, cdf_value, ppf_value\n",
            "        (1, 0.31489339900904967, 1.0000000000000004)\n",
            "        \n",
            "        Draw 1000 random variates by calling the ``rvs`` method.\n",
            "        \n",
            "        >>> sample_size = 1000\n",
            "        >>> sample = vonmises(loc=loc, kappa=kappa).rvs(sample_size)\n",
            "        \n",
            "        Plot the von Mises density on a Cartesian and polar grid to emphasize\n",
            "        that it is a circular distribution.\n",
            "        \n",
            "        >>> fig = plt.figure(figsize=(12, 6))\n",
            "        >>> left = plt.subplot(121)\n",
            "        >>> right = plt.subplot(122, projection='polar')\n",
            "        >>> x = np.linspace(-np.pi, np.pi, 500)\n",
            "        >>> vonmises_pdf = vonmises.pdf(x, loc=loc, kappa=kappa)\n",
            "        >>> ticks = [0, 0.15, 0.3]\n",
            "        \n",
            "        The left image contains the Cartesian plot.\n",
            "        \n",
            "        >>> left.plot(x, vonmises_pdf)\n",
            "        >>> left.set_yticks(ticks)\n",
            "        >>> number_of_bins = int(np.sqrt(sample_size))\n",
            "        >>> left.hist(sample, density=True, bins=number_of_bins)\n",
            "        >>> left.set_title(\"Cartesian plot\")\n",
            "        >>> left.set_xlim(-np.pi, np.pi)\n",
            "        >>> left.grid(True)\n",
            "        \n",
            "        The right image contains the polar plot.\n",
            "        \n",
            "        >>> right.plot(x, vonmises_pdf, label=\"PDF\")\n",
            "        >>> right.set_yticks(ticks)\n",
            "        >>> right.hist(sample, density=True, bins=number_of_bins,\n",
            "        ...            label=\"Histogram\")\n",
            "        >>> right.set_title(\"Polar plot\")\n",
            "        >>> right.legend(bbox_to_anchor=(0.15, 1.06))\n",
            "    \n",
            "    vonmises_fisher = <scipy.stats._multivariate.vonmises_fisher_gen objec...\n",
            "    vonmises_line = <scipy.stats._continuous_distns.vonmises_gen object>\n",
            "        A Von Mises continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `vonmises_line` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, kappa, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, kappa, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, kappa, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, kappa, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, kappa, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, kappa, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, kappa, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, kappa, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(kappa, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(kappa, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(kappa, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(kappa, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(kappa, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(kappa, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, kappa, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        scipy.stats.vonmises_fisher : Von-Mises Fisher distribution on a\n",
            "                                      hypersphere\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `vonmises` and `vonmises_line` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n",
            "        \n",
            "        for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa \\ge 0`. :math:`I_0` is the\n",
            "        modified Bessel function of order zero (`scipy.special.i0`).\n",
            "        \n",
            "        `vonmises` is a circular distribution which does not restrict the\n",
            "        distribution to a fixed interval. Currently, there is no circular\n",
            "        distribution framework in SciPy. The ``cdf`` is implemented such that\n",
            "        ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n",
            "        \n",
            "        `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n",
            "        on the real line. This is a regular (i.e. non-circular) distribution.\n",
            "        \n",
            "        Note about distribution parameters: `vonmises` and `vonmises_line` take\n",
            "        ``kappa`` as a shape parameter (concentration) and ``loc`` as the location\n",
            "        (circular mean). A ``scale`` parameter is accepted but does not have any\n",
            "        effect.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        Import the necessary modules.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import vonmises\n",
            "        \n",
            "        Define distribution parameters.\n",
            "        \n",
            "        >>> loc = 0.5 * np.pi  # circular mean\n",
            "        >>> kappa = 1  # concentration\n",
            "        \n",
            "        Compute the probability density at ``x=0`` via the ``pdf`` method.\n",
            "        \n",
            "        >>> vonmises.pdf(0, loc=loc, kappa=kappa)\n",
            "        0.12570826359722018\n",
            "        \n",
            "        Verify that the percentile function ``ppf`` inverts the cumulative\n",
            "        distribution function ``cdf`` up to floating point accuracy.\n",
            "        \n",
            "        >>> x = 1\n",
            "        >>> cdf_value = vonmises.cdf(x, loc=loc, kappa=kappa)\n",
            "        >>> ppf_value = vonmises.ppf(cdf_value, loc=loc, kappa=kappa)\n",
            "        >>> x, cdf_value, ppf_value\n",
            "        (1, 0.31489339900904967, 1.0000000000000004)\n",
            "        \n",
            "        Draw 1000 random variates by calling the ``rvs`` method.\n",
            "        \n",
            "        >>> sample_size = 1000\n",
            "        >>> sample = vonmises(loc=loc, kappa=kappa).rvs(sample_size)\n",
            "        \n",
            "        Plot the von Mises density on a Cartesian and polar grid to emphasize\n",
            "        that it is a circular distribution.\n",
            "        \n",
            "        >>> fig = plt.figure(figsize=(12, 6))\n",
            "        >>> left = plt.subplot(121)\n",
            "        >>> right = plt.subplot(122, projection='polar')\n",
            "        >>> x = np.linspace(-np.pi, np.pi, 500)\n",
            "        >>> vonmises_pdf = vonmises.pdf(x, loc=loc, kappa=kappa)\n",
            "        >>> ticks = [0, 0.15, 0.3]\n",
            "        \n",
            "        The left image contains the Cartesian plot.\n",
            "        \n",
            "        >>> left.plot(x, vonmises_pdf)\n",
            "        >>> left.set_yticks(ticks)\n",
            "        >>> number_of_bins = int(np.sqrt(sample_size))\n",
            "        >>> left.hist(sample, density=True, bins=number_of_bins)\n",
            "        >>> left.set_title(\"Cartesian plot\")\n",
            "        >>> left.set_xlim(-np.pi, np.pi)\n",
            "        >>> left.grid(True)\n",
            "        \n",
            "        The right image contains the polar plot.\n",
            "        \n",
            "        >>> right.plot(x, vonmises_pdf, label=\"PDF\")\n",
            "        >>> right.set_yticks(ticks)\n",
            "        >>> right.hist(sample, density=True, bins=number_of_bins,\n",
            "        ...            label=\"Histogram\")\n",
            "        >>> right.set_title(\"Polar plot\")\n",
            "        >>> right.legend(bbox_to_anchor=(0.15, 1.06))\n",
            "    \n",
            "    wald = <scipy.stats._continuous_distns.wald_gen object>\n",
            "        A Wald continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `wald` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `wald` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp(- \\frac{ (x-1)^2 }{ 2x })\n",
            "        \n",
            "        for :math:`x >= 0`.\n",
            "        \n",
            "        `wald` is a special case of `invgauss` with ``mu=1``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``wald.pdf(x, loc, scale)`` is identically\n",
            "        equivalent to ``wald.pdf(y) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import wald\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        \n",
            "        >>> mean, var, skew, kurt = wald.stats(moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(wald.ppf(0.01),\n",
            "        ...                 wald.ppf(0.99), 100)\n",
            "        >>> ax.plot(x, wald.pdf(x),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='wald pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = wald()\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = wald.ppf([0.001, 0.5, 0.999])\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], wald.cdf(vals))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = wald.rvs(size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    weibull_max = <scipy.stats._continuous_distns.weibull_max_gen object>\n",
            "        Weibull maximum continuous random variable.\n",
            "        \n",
            "        The Weibull Maximum Extreme Value distribution, from extreme value theory\n",
            "        (Fisher-Gnedenko theorem), is the limiting distribution of rescaled\n",
            "        maximum of iid random variables. This is the distribution of -X\n",
            "        if X is from the `weibull_min` function.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `weibull_max` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        weibull_min\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `weibull_max` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c (-x)^{c-1} \\exp(-(-x)^c)\n",
            "        \n",
            "        for :math:`x < 0`, :math:`c > 0`.\n",
            "        \n",
            "        `weibull_max` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``weibull_max.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``weibull_max.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        https://en.wikipedia.org/wiki/Weibull_distribution\n",
            "        \n",
            "        https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import weibull_max\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 2.87\n",
            "        >>> mean, var, skew, kurt = weibull_max.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(weibull_max.ppf(0.01, c),\n",
            "        ...                 weibull_max.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, weibull_max.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='weibull_max pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = weibull_max(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = weibull_max.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], weibull_max.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = weibull_max.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    weibull_min = <scipy.stats._continuous_distns.weibull_min_gen object>\n",
            "        Weibull minimum continuous random variable.\n",
            "        \n",
            "        The Weibull Minimum Extreme Value distribution, from extreme value theory\n",
            "        (Fisher-Gnedenko theorem), is also often simply called the Weibull\n",
            "        distribution. It arises as the limiting distribution of the rescaled\n",
            "        minimum of iid random variables.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `weibull_min` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        weibull_max, numpy.random.Generator.weibull, exponweib\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `weibull_min` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = c x^{c-1} \\exp(-x^c)\n",
            "        \n",
            "        for :math:`x > 0`, :math:`c > 0`.\n",
            "        \n",
            "        `weibull_min` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        (named :math:`k` in Wikipedia article and :math:`a` in\n",
            "        ``numpy.random.weibull``).  Special shape values are :math:`c=1` and\n",
            "        :math:`c=2` where Weibull distribution reduces to the `expon` and\n",
            "        `rayleigh` distributions respectively.\n",
            "        \n",
            "        Suppose ``X`` is an exponentially distributed random variable with\n",
            "        scale ``s``. Then ``Y = X**k`` is `weibull_min` distributed with shape\n",
            "        ``c = 1/k`` and scale ``s**k``.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``weibull_min.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``weibull_min.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        https://en.wikipedia.org/wiki/Weibull_distribution\n",
            "        \n",
            "        https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import weibull_min\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 1.79\n",
            "        >>> mean, var, skew, kurt = weibull_min.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(weibull_min.ppf(0.01, c),\n",
            "        ...                 weibull_min.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, weibull_min.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='weibull_min pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = weibull_min(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = weibull_min.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], weibull_min.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = weibull_min.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    wishart = <scipy.stats._multivariate.wishart_gen object>\n",
            "        A Wishart random variable.\n",
            "        \n",
            "        The `df` keyword specifies the degrees of freedom. The `scale` keyword\n",
            "        specifies the scale matrix, which must be symmetric and positive definite.\n",
            "        In this context, the scale matrix is often interpreted in terms of a\n",
            "        multivariate normal precision matrix (the inverse of the covariance\n",
            "        matrix). These arguments must satisfy the relationship\n",
            "        ``df > scale.ndim - 1``, but see notes on using the `rvs` method with\n",
            "        ``df < scale.ndim``.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        pdf(x, df, scale)\n",
            "            Probability density function.\n",
            "        logpdf(x, df, scale)\n",
            "            Log of the probability density function.\n",
            "        rvs(df, scale, size=1, random_state=None)\n",
            "            Draw random samples from a Wishart distribution.\n",
            "        entropy()\n",
            "            Compute the differential entropy of the Wishart distribution.\n",
            "        \n",
            "        Parameters\n",
            "        ----------\n",
            "        df : int\n",
            "            Degrees of freedom, must be greater than or equal to dimension of the\n",
            "            scale matrix\n",
            "        scale : array_like\n",
            "            Symmetric positive definite scale matrix of the distribution\n",
            "        seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n",
            "            Used for drawing random variates.\n",
            "            If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n",
            "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
            "            with seed.\n",
            "            If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
            "            then that object is used.\n",
            "            Default is `None`.\n",
            "        \n",
            "        Raises\n",
            "        ------\n",
            "        scipy.linalg.LinAlgError\n",
            "            If the scale matrix `scale` is not positive definite.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        invwishart, chi2\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        \n",
            "        The scale matrix `scale` must be a symmetric positive definite\n",
            "        matrix. Singular matrices, including the symmetric positive semi-definite\n",
            "        case, are not supported. Symmetry is not checked; only the lower triangular\n",
            "        portion is used.\n",
            "        \n",
            "        The Wishart distribution is often denoted\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            W_p(\\nu, \\Sigma)\n",
            "        \n",
            "        where :math:`\\nu` is the degrees of freedom and :math:`\\Sigma` is the\n",
            "        :math:`p \\times p` scale matrix.\n",
            "        \n",
            "        The probability density function for `wishart` has support over positive\n",
            "        definite matrices :math:`S`; if :math:`S \\sim W_p(\\nu, \\Sigma)`, then\n",
            "        its PDF is given by:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(S) = \\frac{|S|^{\\frac{\\nu - p - 1}{2}}}{2^{ \\frac{\\nu p}{2} }\n",
            "                   |\\Sigma|^\\frac{\\nu}{2} \\Gamma_p \\left ( \\frac{\\nu}{2} \\right )}\n",
            "                   \\exp\\left( -tr(\\Sigma^{-1} S) / 2 \\right)\n",
            "        \n",
            "        If :math:`S \\sim W_p(\\nu, \\Sigma)` (Wishart) then\n",
            "        :math:`S^{-1} \\sim W_p^{-1}(\\nu, \\Sigma^{-1})` (inverse Wishart).\n",
            "        \n",
            "        If the scale matrix is 1-dimensional and equal to one, then the Wishart\n",
            "        distribution :math:`W_1(\\nu, 1)` collapses to the :math:`\\chi^2(\\nu)`\n",
            "        distribution.\n",
            "        \n",
            "        The algorithm [2]_ implemented by the `rvs` method may\n",
            "        produce numerically singular matrices with :math:`p - 1 < \\nu < p`; the\n",
            "        user may wish to check for this condition and generate replacement samples\n",
            "        as necessary.\n",
            "        \n",
            "        \n",
            "        .. versionadded:: 0.16.0\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n",
            "               Wiley, 1983.\n",
            "        .. [2] W.B. Smith and R.R. Hocking, \"Algorithm AS 53: Wishart Variate\n",
            "               Generator\", Applied Statistics, vol. 21, pp. 341-345, 1972.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> from scipy.stats import wishart, chi2\n",
            "        >>> x = np.linspace(1e-5, 8, 100)\n",
            "        >>> w = wishart.pdf(x, df=3, scale=1); w[:5]\n",
            "        array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n",
            "        >>> c = chi2.pdf(x, 3); c[:5]\n",
            "        array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n",
            "        >>> plt.plot(x, w)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        The input quantiles can be any shape of array, as long as the last\n",
            "        axis labels the components.\n",
            "        \n",
            "        Alternatively, the object may be called (as a function) to fix the degrees\n",
            "        of freedom and scale parameters, returning a \"frozen\" Wishart random\n",
            "        variable:\n",
            "        \n",
            "        >>> rv = wishart(df=1, scale=1)\n",
            "        >>> # Frozen object with the same methods but holding the given\n",
            "        >>> # degrees of freedom and scale fixed.\n",
            "    \n",
            "    wrapcauchy = <scipy.stats._continuous_distns.wrapcauchy_gen object>\n",
            "        A wrapped Cauchy continuous random variable.\n",
            "        \n",
            "        As an instance of the `rv_continuous` class, `wrapcauchy` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pdf(x, c, loc=0, scale=1)\n",
            "            Probability density function.\n",
            "        logpdf(x, c, loc=0, scale=1)\n",
            "            Log of the probability density function.\n",
            "        cdf(x, c, loc=0, scale=1)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(x, c, loc=0, scale=1)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(x, c, loc=0, scale=1)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(x, c, loc=0, scale=1)\n",
            "            Log of the survival function.\n",
            "        ppf(q, c, loc=0, scale=1)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, c, loc=0, scale=1)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        moment(order, c, loc=0, scale=1)\n",
            "            Non-central moment of the specified order.\n",
            "        stats(c, loc=0, scale=1, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(c, loc=0, scale=1)\n",
            "            (Differential) entropy of the RV.\n",
            "        fit(data)\n",
            "            Parameter estimates for generic data.\n",
            "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
            "            keyword arguments.\n",
            "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(c, loc=0, scale=1)\n",
            "            Median of the distribution.\n",
            "        mean(c, loc=0, scale=1)\n",
            "            Mean of the distribution.\n",
            "        var(c, loc=0, scale=1)\n",
            "            Variance of the distribution.\n",
            "        std(c, loc=0, scale=1)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, c, loc=0, scale=1)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability density function for `wrapcauchy` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(x, c) = \\frac{1-c^2}{2\\pi (1+c^2 - 2c \\cos(x))}\n",
            "        \n",
            "        for :math:`0 \\le x \\le 2\\pi`, :math:`0 < c < 1`.\n",
            "        \n",
            "        `wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.\n",
            "        \n",
            "        The probability density above is defined in the \"standardized\" form. To shift\n",
            "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
            "        Specifically, ``wrapcauchy.pdf(x, c, loc, scale)`` is identically\n",
            "        equivalent to ``wrapcauchy.pdf(y, c) / scale`` with\n",
            "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
            "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
            "        some distributions are available in separate classes.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import wrapcauchy\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> c = 0.0311\n",
            "        >>> mean, var, skew, kurt = wrapcauchy.stats(c, moments='mvsk')\n",
            "        \n",
            "        Display the probability density function (``pdf``):\n",
            "        \n",
            "        >>> x = np.linspace(wrapcauchy.ppf(0.01, c),\n",
            "        ...                 wrapcauchy.ppf(0.99, c), 100)\n",
            "        >>> ax.plot(x, wrapcauchy.pdf(x, c),\n",
            "        ...        'r-', lw=5, alpha=0.6, label='wrapcauchy pdf')\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
            "        RV object holding the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pdf``:\n",
            "        \n",
            "        >>> rv = wrapcauchy(c)\n",
            "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> vals = wrapcauchy.ppf([0.001, 0.5, 0.999], c)\n",
            "        >>> np.allclose([0.001, 0.5, 0.999], wrapcauchy.cdf(vals, c))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = wrapcauchy.rvs(c, size=1000)\n",
            "        \n",
            "        And compare the histogram:\n",
            "        \n",
            "        >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n",
            "        >>> ax.set_xlim([x[0], x[-1]])\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "    \n",
            "    yulesimon = <scipy.stats._discrete_distns.yulesimon_gen object>\n",
            "        A Yule-Simon discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `yulesimon` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(alpha, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, alpha, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, alpha, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, alpha, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, alpha, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, alpha, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, alpha, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, alpha, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, alpha, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(alpha, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(alpha, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(alpha,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(alpha, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(alpha, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(alpha, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(alpha, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, alpha, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        \n",
            "        The probability mass function for the `yulesimon` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k) =  \\alpha B(k, \\alpha+1)\n",
            "        \n",
            "        for :math:`k=1,2,3,...`, where :math:`\\alpha>0`.\n",
            "        Here :math:`B` refers to the `scipy.special.beta` function.\n",
            "        \n",
            "        The sampling of random variates is based on pg 553, Section 6.3 of [1]_.\n",
            "        Our notation maps to the referenced logic via :math:`\\alpha=a-1`.\n",
            "        \n",
            "        For details see the wikipedia entry [2]_.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] Devroye, Luc. \"Non-uniform Random Variate Generation\",\n",
            "             (1986) Springer, New York.\n",
            "        \n",
            "        .. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``yulesimon.pmf(k, alpha, loc)`` is identically\n",
            "        equivalent to ``yulesimon.pmf(k - loc, alpha)``.\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import yulesimon\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> alpha = 11\n",
            "        >>> mean, var, skew, kurt = yulesimon.stats(alpha, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(yulesimon.ppf(0.01, alpha),\n",
            "        ...               yulesimon.ppf(0.99, alpha))\n",
            "        >>> ax.plot(x, yulesimon.pmf(x, alpha), 'bo', ms=8, label='yulesimon pmf')\n",
            "        >>> ax.vlines(x, 0, yulesimon.pmf(x, alpha), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = yulesimon(alpha)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = yulesimon.cdf(x, alpha)\n",
            "        >>> np.allclose(x, yulesimon.ppf(prob, alpha))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = yulesimon.rvs(alpha, size=1000)\n",
            "    \n",
            "    zipf = <scipy.stats._discrete_distns.zipf_gen object>\n",
            "        A Zipf (Zeta) discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `zipf` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, a, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, a, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, a, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, a, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, a, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, a, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(a, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(a, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(a, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(a, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        zipfian\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `zipf` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k, a) = \\frac{1}{\\zeta(a) k^a}\n",
            "        \n",
            "        for :math:`k \\ge 1`, :math:`a > 1`.\n",
            "        \n",
            "        `zipf` takes :math:`a > 1` as shape parameter. :math:`\\zeta` is the\n",
            "        Riemann zeta function (`scipy.special.zeta`)\n",
            "        \n",
            "        The Zipf distribution is also known as the zeta distribution, which is\n",
            "        a special case of the Zipfian distribution (`zipfian`).\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``zipf.pmf(k, a, loc)`` is identically\n",
            "        equivalent to ``zipf.pmf(k - loc, a)``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Zeta Distribution\", Wikipedia,\n",
            "               https://en.wikipedia.org/wiki/Zeta_distribution\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import zipf\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a = 6.6\n",
            "        >>> mean, var, skew, kurt = zipf.stats(a, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(zipf.ppf(0.01, a),\n",
            "        ...               zipf.ppf(0.99, a))\n",
            "        >>> ax.plot(x, zipf.pmf(x, a), 'bo', ms=8, label='zipf pmf')\n",
            "        >>> ax.vlines(x, 0, zipf.pmf(x, a), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = zipf(a)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = zipf.cdf(x, a)\n",
            "        >>> np.allclose(x, zipf.ppf(prob, a))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = zipf.rvs(a, size=1000)\n",
            "        \n",
            "        Confirm that `zipf` is the large `n` limit of `zipfian`.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import zipf, zipfian\n",
            "        >>> k = np.arange(11)\n",
            "        >>> np.allclose(zipf.pmf(k, a), zipfian.pmf(k, a, n=10000000))\n",
            "        True\n",
            "    \n",
            "    zipfian = <scipy.stats._discrete_distns.zipfian_gen object>\n",
            "        A Zipfian discrete random variable.\n",
            "        \n",
            "        As an instance of the `rv_discrete` class, `zipfian` object inherits from it\n",
            "        a collection of generic methods (see below for the full list),\n",
            "        and completes them with details specific for this particular distribution.\n",
            "        \n",
            "        Methods\n",
            "        -------\n",
            "        rvs(a, n, loc=0, size=1, random_state=None)\n",
            "            Random variates.\n",
            "        pmf(k, a, n, loc=0)\n",
            "            Probability mass function.\n",
            "        logpmf(k, a, n, loc=0)\n",
            "            Log of the probability mass function.\n",
            "        cdf(k, a, n, loc=0)\n",
            "            Cumulative distribution function.\n",
            "        logcdf(k, a, n, loc=0)\n",
            "            Log of the cumulative distribution function.\n",
            "        sf(k, a, n, loc=0)\n",
            "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
            "        logsf(k, a, n, loc=0)\n",
            "            Log of the survival function.\n",
            "        ppf(q, a, n, loc=0)\n",
            "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
            "        isf(q, a, n, loc=0)\n",
            "            Inverse survival function (inverse of ``sf``).\n",
            "        stats(a, n, loc=0, moments='mv')\n",
            "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
            "        entropy(a, n, loc=0)\n",
            "            (Differential) entropy of the RV.\n",
            "        expect(func, args=(a, n), loc=0, lb=None, ub=None, conditional=False)\n",
            "            Expected value of a function (of one argument) with respect to the distribution.\n",
            "        median(a, n, loc=0)\n",
            "            Median of the distribution.\n",
            "        mean(a, n, loc=0)\n",
            "            Mean of the distribution.\n",
            "        var(a, n, loc=0)\n",
            "            Variance of the distribution.\n",
            "        std(a, n, loc=0)\n",
            "            Standard deviation of the distribution.\n",
            "        interval(confidence, a, n, loc=0)\n",
            "            Confidence interval with equal areas around the median.\n",
            "        \n",
            "        See Also\n",
            "        --------\n",
            "        zipf\n",
            "        \n",
            "        Notes\n",
            "        -----\n",
            "        The probability mass function for `zipfian` is:\n",
            "        \n",
            "        .. math::\n",
            "        \n",
            "            f(k, a, n) = \\frac{1}{H_{n,a} k^a}\n",
            "        \n",
            "        for :math:`k \\in \\{1, 2, \\dots, n-1, n\\}`, :math:`a \\ge 0`,\n",
            "        :math:`n \\in \\{1, 2, 3, \\dots\\}`.\n",
            "        \n",
            "        `zipfian` takes :math:`a` and :math:`n` as shape parameters.\n",
            "        :math:`H_{n,a}` is the :math:`n`:sup:`th` generalized harmonic\n",
            "        number of order :math:`a`.\n",
            "        \n",
            "        The Zipfian distribution reduces to the Zipf (zeta) distribution as\n",
            "        :math:`n \\rightarrow \\infty`.\n",
            "        \n",
            "        The probability mass function above is defined in the \"standardized\" form.\n",
            "        To shift distribution use the ``loc`` parameter.\n",
            "        Specifically, ``zipfian.pmf(k, a, n, loc)`` is identically\n",
            "        equivalent to ``zipfian.pmf(k - loc, a, n)``.\n",
            "        \n",
            "        References\n",
            "        ----------\n",
            "        .. [1] \"Zipf's Law\", Wikipedia, https://en.wikipedia.org/wiki/Zipf's_law\n",
            "        .. [2] Larry Leemis, \"Zipf Distribution\", Univariate Distribution\n",
            "               Relationships. http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Zipf.pdf\n",
            "        \n",
            "        Examples\n",
            "        --------\n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import zipfian\n",
            "        >>> import matplotlib.pyplot as plt\n",
            "        >>> fig, ax = plt.subplots(1, 1)\n",
            "        \n",
            "        Calculate the first four moments:\n",
            "        \n",
            "        >>> a, n = 1.25, 10\n",
            "        >>> mean, var, skew, kurt = zipfian.stats(a, n, moments='mvsk')\n",
            "        \n",
            "        Display the probability mass function (``pmf``):\n",
            "        \n",
            "        >>> x = np.arange(zipfian.ppf(0.01, a, n),\n",
            "        ...               zipfian.ppf(0.99, a, n))\n",
            "        >>> ax.plot(x, zipfian.pmf(x, a, n), 'bo', ms=8, label='zipfian pmf')\n",
            "        >>> ax.vlines(x, 0, zipfian.pmf(x, a, n), colors='b', lw=5, alpha=0.5)\n",
            "        \n",
            "        Alternatively, the distribution object can be called (as a function)\n",
            "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
            "        the given parameters fixed.\n",
            "        \n",
            "        Freeze the distribution and display the frozen ``pmf``:\n",
            "        \n",
            "        >>> rv = zipfian(a, n)\n",
            "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
            "        ...         label='frozen pmf')\n",
            "        >>> ax.legend(loc='best', frameon=False)\n",
            "        >>> plt.show()\n",
            "        \n",
            "        Check accuracy of ``cdf`` and ``ppf``:\n",
            "        \n",
            "        >>> prob = zipfian.cdf(x, a, n)\n",
            "        >>> np.allclose(x, zipfian.ppf(prob, a, n))\n",
            "        True\n",
            "        \n",
            "        Generate random numbers:\n",
            "        \n",
            "        >>> r = zipfian.rvs(a, n, size=1000)\n",
            "        \n",
            "        Confirm that `zipfian` reduces to `zipf` for large `n`, `a > 1`.\n",
            "        \n",
            "        >>> import numpy as np\n",
            "        >>> from scipy.stats import zipf, zipfian\n",
            "        >>> k = np.arange(11)\n",
            "        >>> np.allclose(zipfian.pmf(k, a=3.5, n=10000000), zipf.pmf(k, a=3.5))\n",
            "        True\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.11/dist-packages/scipy/stats/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import constants"
      ],
      "metadata": {
        "id": "Xne5pXRotyD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constants.minute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_9k0M9Qt9A1",
        "outputId": "dde83f7c-43b8-4be0-c0ae-da7ec828bbf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constants.pi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPkGhXoyuLKz",
        "outputId": "0e09e4be-fe52-40db-eb97-5872944f7b28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.141592653589793"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constants.Julian_year"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pfUbzatuOKq",
        "outputId": "23740e71-1966-4c33-9a2f-47b32b564061"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31557600.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}